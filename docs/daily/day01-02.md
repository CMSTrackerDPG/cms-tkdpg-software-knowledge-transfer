# Week 01 - 2021.12.06-10.

based on [Introduction to parallel programming and CUDA by Felice Pantaleo](https://indico.cern.ch/event/863657/)

## Performance measurement

1. Memory throughput
2. Processing throughput

1. Memory throughput is also referred to as memory bandwidth.

To calculate the memory bandwidth we need to understand two concepts related to it.

**Memory interface width:** It is the physical bit-width of the memory bus. Every clock-cycle data is transferred along a memory bus to and from the on-card memory. The memory interface width is the physical count of the bits of how many can fit down the bus per clock cycle.

**Memory clock rate:** (or memory clock cycle) Measured in the unit of *Hz* it is the rate of memory bus clock. For example a memory bus operating on a 1GHz rate is capable of issuing memory tranfers with the bus 10^9 times per second.

Talking about clock rate it is important to take a detour to *SDR*, *DDR* and *QDR*.

![SDR,QDR,DDR image](img/d02_img01.png)

A simple analogy is of course public transport, where memory interface width would be the number of seats on a bus and the memory clock rate is the frequency of buses in unit time (hour let's say).
Multiplying these two numbers we can calculate how many people can be transported along one bus line in a certain time period.

## SPMD

Single Program Multiple Data

## Language extensions

[From CUDA Toolkit Documentation: Language Extensions](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#c-language-extensions):
### `__global__` : 
The __global__ execution space specifier declares a function as being a kernel. Such a function is:

* Executed on the device,
* Callable from the host,
* Callable from the device for devices of compute capability 3.2 or higher (see CUDA Dynamic Parallelism for more details).
A __global__ function must have void return type, and cannot be a member of a class.

Any call to a __global__ function must specify its execution configuration as described in [Execution](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration).

A call to a __global__ function is asynchronous, meaning it returns before the device has completed its execution.

### `__device__` : 

The __device__ execution space specifier declares a function that is:

* Executed on the device,
* Callable from the device only.

The __global__ and __device__ execution space specifiers cannot be used together.

### `__host__` :

The __host__ execution space specifier declares a function that is:

* Executed on the host,
* Callable from the host only.

It is equivalent to declare a function with only the __host__ execution space specifier or to declare it without any of the __host__, __device__, or __global__ execution space specifier; in either case the function is compiled for the host only.

The __global__ and __host__ execution space specifiers cannot be used together.

The __device__ and __host__ execution space specifiers can be used together however, in which case the function is compiled for both the host and the device.

## Exercise 01

Give an example of `global`, `device` and `host-device` functions in `CMSSW`. Can you find an example where `host` and `device` code diverge? How is this achieved?

## Execution Configuration

[From CUDA Toolkit Documentation: Execution Configuration](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#execution-configuration)

Any call to a __global__ function must specify the execution configuration for that call. The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams).

The execution configuration is specified by inserting an expression of the form `<<< Dg, Db, Ns, S >>>` between the function name and the parenthesized argument list, where:

* `Dg` is of type dim3 (see dim3) and specifies the dimension and size of the grid, such that `Dg.x * Dg.y * Dg.z` equals the number of blocks being launched;

* `Db` is of type dim3 (see dim3) and specifies the dimension and size of each block, such that `Db.x * Db.y * Db.z` equals the number of threads per block;

* `Ns` is of type `size_t` and specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory; this dynamically allocated memory is used by any of the variables declared as an external array as mentioned in __shared__; Ns is an optional argument which defaults to 0;

* `S` is of type `cudaStream_t` and specifies the associated stream; S is an optional argument which defaults to 0.


Used resources:

[https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)

[https://towardsdatascience.com/the-ai-illustrated-guide-why-are-gpus-so-powerful-99f4ae85a5c3](https://towardsdatascience.com/the-ai-illustrated-guide-why-are-gpus-so-powerful-99f4ae85a5c3)

[https://streamhpc.com/blog/2016-07-19/performance-can-be-measured-as-throughput-latency-or-processor-utilisation/](https://streamhpc.com/blog/2016-07-19/performance-can-be-measured-as-throughput-latency-or-processor-utilisation/)

[https://www.gamersnexus.net/dictionary/3-memory-interface](https://www.gamersnexus.net/dictionary/3-memory-interface)
