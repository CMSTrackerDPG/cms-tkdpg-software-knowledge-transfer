{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CMS Tracker DPG Knowledge Transfer Material","text":"<p>This is an attempt to aggregate information gathered while working on software used by the CMS TkDPG group. It contains information regarding CMSSW code, both for CPU and GPU implementations, along with information on projects like CertHelper and MLPlayground.</p> <p>This documentation does not aim to replace existing pages or other existing documentation, but to  cherry-pick useful information and have it one place.</p> <p>Use the sidebar to navigate through the available sections.</p> <p>Note</p> <p>Feel free to contribute to this documentation (via pull requests  on github),  or just by pressing the edit button (), on the top right of each page. There will definitely be missing insights, details or whole pieces of information in  this documentation. </p> <p>Project started by @czangela.</p>"},{"location":"basic-concepts/","title":"Basic concepts","text":"<p>Before diving deeper into the software itself, some terms and concepts must be covered so that the reader does not get lost in the abundance of terminology and jargon that pervades anything CERN-related.</p> <p>Your companion glossary can always be found here.</p> <p>Warning</p> <p>There might be lots of inaccuracies in this document. Don't judge me!</p>"},{"location":"basic-concepts/#general-cern-jargon","title":"General CERN Jargon","text":""},{"location":"basic-concepts/#phase-0-phase-1-phase-2","title":"Phase 0, Phase 1, Phase 2, ...","text":"<p>Phases are periods during which the detectors' hardware has more or less the  same capabilities. When large-scale changes and upgrades take place, a new Phase starts.</p> <p>Multiple phases may belong to a single Run (see below).</p>"},{"location":"basic-concepts/#lhc-run-1-run-2","title":"[LHC] Run 1, Run 2, ...","text":"<p>Data-taking periods, which are usually followed by Long Shutdown (LS) periods. During a Run, multiple Phases may be observed. For example, Phase 0 and Phase 1 took place during Run 2. Refers to the long-term schedule of the LHC, and it's in the order of months.</p> <p>Runs' schedule can be found here.</p>"},{"location":"basic-concepts/#fill","title":"Fill","text":"<p>Todo</p> <p>TODO</p>"},{"location":"basic-concepts/#cms-run","title":"[CMS] Run","text":"<p>Todo</p> <p>TODO</p> <p>Data-taking periods (in the order of minutes), during which CMS is recording data.</p>"},{"location":"basic-concepts/#hardware","title":"Hardware","text":""},{"location":"basic-concepts/#adc","title":"ADC","text":"<p>Analog to Digital Conversion/Converter. The process of digitizing an analog signal.</p>"},{"location":"basic-concepts/#pixel-detector","title":"Pixel Detector","text":"<p>Todo</p> <p>TODO</p> <p>Comprised of many Pixels,  the Pixel Detector is part of the whole CMS detector.</p> <p>In the reconstruction code, the Pixel Detector is represented as a 2D matrix of pixels (<code>PixelDigi</code>s), with X and Y coordinates, each one having an ADC value.</p>"},{"location":"basic-concepts/#pixel-local-reconstruction","title":"Pixel-local Reconstruction","text":"<p>See here.</p>"},{"location":"basic-concepts/#pixel-cluster","title":"Pixel Cluster","text":"<p>Each time particles pass through the Pixel Detector and interact with it, several  adjascent Pixels are activated at the same time from each particle. Those adjascent Pixels form Clusters. In other words, a Cluster  is the trace that the particle leaves on the Pixel Detector.</p> <p> </p> A cluster of pixels activated by a particle resembles a stain (Pixel Cluster) on a shirt (Pixel Detector)"},{"location":"basic-concepts/#rechit","title":"RecHit","text":"<p>RecHits are the result of \"charge-weighting\" a cluster.  In layman's terms, it's the \"actual\" location in the pixel detector where the particle passed through.</p>"},{"location":"basic-concepts/#track","title":"Track","text":"<p>Tracks are collections of RecHits on different layers, which RecHits are assumed to compose a part of the trajectory of a single particle through the Pixel Detector.</p>"},{"location":"basic-concepts/#vertex","title":"Vertex","text":"<p>Once tracks are created, they must be assigned to collisions which generated each track. Since many collisions happen at the same time within the area where the beams collide, vertices are, actually, tracks which have been assigned to different collisions.</p>"},{"location":"basic-concepts/#read-out-chip","title":"Read-Out Chip","text":"<p>Pixel Read-Out Chip (ROC) with 2 x 26 x 80 pixels. A chip which measures and converts analog signal to digital.</p> <p>Specification here.</p> <p></p> <p>Image taken from here.</p> <p></p>"},{"location":"basic-concepts/#module","title":"Module","text":"<p>A structure of 2 x 8 Read-Out Chips (ROCs), totalling 2 x 8 x (2 x 26 x 80) = 66560 pixels.</p> <p>Multiple modules are used in the Pixel Detector.</p> <p>In the exploded view below, the ROCs can be seen in the second layer from the top.</p> <p></p> <p>Info</p> <p>In software, each module is identified by a unique number. This number, called DetId, is used for all detector parts.</p>"},{"location":"basic-concepts/#software","title":"Software","text":""},{"location":"basic-concepts/#cmssw","title":"CMSSW","text":"<p>A huge repository of software related to CMS data analysis, reconstruction, also containing various other tools.</p> <p>Intimidating for beginners, rumors have it that even people that have been developing with it for ages don't want to touch it, not even with a 10-foot pole.</p> <p>Resembles a mythic monster, that keeps growing with the passage of time, thriving on the keyboard strokes of innocent physicists.</p> <p> </p>    It's not that ugly, CMSSW was just having a bad hair day when this   pic was taken    Hydra image source <p>@velinov</p>"},{"location":"basic-concepts/#cmsrun-cmsdriver-runthematrix-etc","title":"<code>cmsRun</code>, <code>cmsDriver</code>, <code>runTheMatrix</code> etc.","text":"<p>Proceed to the Tools section for more information.</p>"},{"location":"basic-concepts/#configuration-files","title":"Configuration files","text":"<p>Most probably referring to  <code>cmsRun</code> configuration files</p>"},{"location":"basic-concepts/#pixel-clusterizing","title":"Pixel Clusterizing","text":"<p>Once data from each pixel has been recorded, we need to detect Clusters of adjascent/neigbouring pixels which have an ADC value above a certain threshold. This process of detecting Clusters is called Clusterizing.</p> <p>This is implemented in both CPU and GPU.</p>"},{"location":"basic-concepts/#soaaos","title":"SoA/AoS","text":"<p>Alternative ways to store class variables in memory for parallel computing.</p> <p>In usual class declarations, one might have a class attribute, e.g. <code>int num_books</code>.  When an instance of this class is loaded into memory, <code>num_books</code> is most probably close to other class attributes. Say we have thousands of such class instances and want to run parallel code on them using only the <code>num_books</code> attribute of each instance. This way, the computer will have to load to memory lots of data which will stay unused. </p> <p>This will degrade performance at low-levels, e.g. cache lines will be filled with data unrelated to the actual calculations, meaning lots of wasted cycles.</p> <p>Such an approach is called Array of Structures (AoS), and descrbes using an array of structures/classes for storing data.</p> <p>To alleviate the drawbacks of AoS, instead of thousands of class instances stored as an array, the Structure of Arrays (SoA) approach can be used. This approach suggests using only one class instance for all the data one wants to store.</p> <p>This way, instead of having an <code>int num_books</code> attribute in the class, the class now contains an <code>int* nums_books</code> array, which stores the data of all instances.</p> <p>The data are now stored consecutively in RAM when loaded, meaning less memory overhead and better parallel code performance.</p> <p>A video explanation may be found here</p>"},{"location":"basic-concepts/#detid","title":"<code>DetId</code>","text":"<p>A unique, 32-bit number, used to identify all CMS detector parts in CMSSW (mainly code that runs on CPUs).</p> <p>Defined here.</p> <p>More info here.</p>"},{"location":"basic-concepts/#run-reconstruction","title":"Run Reconstruction","text":"<p>The result of reconstructing tracks and extracting information from raw CMS Run data.</p> <p>Those reconstructions are created offline (i.e. after the CMS Run has taken place), and are split into four types with regard to their accuracy, detail and time passed since the raw data were taken:</p> <ul> <li>Stream Express (~20 minutes after data taking)</li> <li>Prompt Reconstruction (~48 hours after data taking)</li> <li>ReReco (??? years after data taking)</li> <li>ReReco Ultra Legacy (??? years after data taking)</li> </ul>"},{"location":"basic-concepts/#run-registry","title":"Run Registry","text":"<p>Also RunRegistry or RR, this is a web application and API which serves information for Online and Offline CMS run reconstructions.</p> <p>Link to Run Registry.</p>"},{"location":"basic-concepts/#run-registry-dataset","title":"[Run Registry] Dataset","text":"<p>Todo</p> <p>TODO: No idea what this represents</p> <p>A string identifying the reconstruction that took place, e.g.:</p> <ul> <li><code>/Express/Cosmics22/DQM</code></li> <li><code>/Express/Commissioning2022/DQM</code></li> <li><code>/PromptReco/Collisions2022/DQM</code></li> </ul>"},{"location":"basic-concepts/#dbod","title":"DBoD","text":"<p>DataBase on Demand, a service provided by CERN to request database instances for personal or project use.</p> <p>Website</p> <p>Note</p> <p>Not to be confused with BDoD</p>"},{"location":"basic-concepts/#dry","title":"DRY","text":"<p> Do not Repeat Yourself. Copy/Pasting code is BAD </p> <p></p>"},{"location":"basic-concepts/#openshift","title":"OpenShift","text":"<p>A RedHat platform similar to Kubernetes, used by CERN under the name \"PaaS\" (Platform as a Service).</p>"},{"location":"basic-concepts/#s2i-source-to-image","title":"s2i (Source to Image)","text":"<p>A build process for creating docker images directly from the source code, depending on the source's requirements.</p> <p>Developed by RedHat, it is one of the available methods for deploying software on the OpenShift platform.</p> <p>A general readme on s2i is available here.</p> <p>PaaS-specific instructions available here.</p>"},{"location":"basic-concepts/#python-s2i","title":"Python s2i","text":"<p>For Django projects, a python s2i configuration is used. The source code for this script can be found here.</p> <p>In general, the python-specific tasks run when creating the container (e.g. <code>pip install -r requirements.txt</code>) are run in the <code>assemble</code> script, found here. This scripts also checks if the source code is a Django project.</p> <p>The <code>run</code> script, found here, checks for <code>gunicorn</code>, <code>django</code> and other set environmental variables (e.g. <code>APP_SCRIPT</code>).</p> <p>These scripts can be augmented by creating custom <code>assemble</code> and <code>run</code> scripts, such as this one, which was created for CertHelper in order to add custom build-time secret access tokens for CERN's gitlab.</p>"},{"location":"basic-concepts/#pod-openshift","title":"Pod (OpenShift)","text":"<p>The instance of an application deployed on OpenShift, launched inside a containerized environment.</p> <p>See the official documentation for more information.</p>"},{"location":"certhelper/connectivity/","title":"CertHelper's connectivity","text":"<p>During its operation, CertHelper will try to connect to various remote resources. An overview can be seen in the sketch below. </p> <pre><code>flowchart\n    db[(DBoD)] &lt;-- TCP --&gt; ch[CertHelper]\n    vocms[vocms066] &lt;-- SSH --&gt; ch\n    ebutz[ebutz] &lt;-- HTTP --&gt; ch\n    rr[RunRegistry] &lt;-- HTTP --&gt; ch\n    oms[OMS API] &lt;-- HTTP --&gt; ch\n    redis[Redis] &lt;-- TCP --&gt; ch\n</code></pre>"},{"location":"certhelper/connectivity/#storage","title":"Storage","text":"<ul> <li>PostgreSQL database (hosted by CERN's DBoD service)(required)</li> <li>Redis server, as a backing store for Websockets (Deployed on Openshift).</li> </ul>"},{"location":"certhelper/connectivity/#apis","title":"APIs","text":"<ul> <li>OMS API, for getting run and fill information (in <code>omsapi</code> app and <code>oms/utils.py</code> as of writing)</li> <li>CMS Run Registry, for getting reconstruction and dataset information (in <code>oms/utils.py</code> as of writing)</li> <li>ebutz's personal tools, for getting extra information for Reference Runs (in <code>oms/models.py</code> as of writing)</li> </ul>"},{"location":"certhelper/connectivity/#tools","title":"Tools","text":"<ul> <li>Any remotescript configuration (such as the Trackermaps generation script on <code>vocms066.lxplus.cern.ch</code>) accesses available remote hosts as configured (see <code>remotescripts</code>).</li> </ul>"},{"location":"certhelper/docs/","title":"Available Documentation","text":""},{"location":"certhelper/docs/#user-documentation","title":"User documentation","text":"<p>On readthedocs. Source code for the documentation is in the github repository under the <code>docs/</code> directory and is automatically deployed on each push to <code>master</code> or <code>develop</code>.</p>"},{"location":"certhelper/docs/#dev-documentation","title":"Dev documentation","text":"<p>You're looking at it!</p>"},{"location":"certhelper/faq/","title":"FAQ","text":""},{"location":"certhelper/faq/#errors","title":"Errors","text":""},{"location":"certhelper/faq/#i-keep-getting-403-client-error-forbidden-for-url-when-trying-to-certify-a-new-reconstruction","title":"I keep getting <code>403 Client Error: Forbidden for url</code> when trying to certify a new reconstruction.","text":"<p>If an error like the following appears:</p> <p></p> <p>It may be related to this issue. Regenerating and replacing the grid credentials used by Certhelper may solve the issue.</p> <p>To do that:</p> <ol> <li>You need access to the <code>/eos/user/t/tkdqmdoc/private</code> directory, where the <code>usercert.pem</code> and <code>userkey.pem</code> files reside. Contact TkDPG for access.</li> <li>Regenerate a Grid Certificate.</li> <li>Convert the certificate to a PEM keypair (<code>usercert.pem</code>, <code>userkey.pem</code>).</li> <li>Replace the <code>usercert.pem</code>, <code>userkey.pem</code> files in <code>/eos/user/t/tkdqmdoc/private</code>.</li> <li>No restart of the app should be required.</li> </ol>"},{"location":"certhelper/introduction/","title":"Introduction","text":"<p>Todo</p> <p>TODO</p> <p>A Django project, focused on providing tools for Tracker Offline Shifters and Shift Leaders.</p> <p>It's deployed on CERN's PaaS platform.</p>"},{"location":"certhelper/introduction/#code","title":"Code","text":"<ul> <li>GitHub repo</li> </ul>"},{"location":"certhelper/introduction/#models-structure","title":"Models structure","text":"<p>The relationship of the models defined in CertHelper's apps can be seen below:</p> <p></p>"},{"location":"certhelper/apps/addrefrun/overview/","title":"addrefrun app","text":"<p>An application which contains functionality and templates related to listing and managing Reference Run Reconstructions.</p>"},{"location":"certhelper/apps/addrefrun/views/update_refruns_info/","title":"update_refruns_info","text":"<p>A view which triggers an update of the information of each <code>OmsRun</code> which is related to any <code>RunReconstruction</code> which is a reference one (<code>is_reference=True</code>).</p> <p>This was added due to the fact that some <code>OmsRun</code> information (namely <code>APV_mode</code>) is only available through a specific external web app (ebutz.web.cern.ch). Initially,  the app was queried on <code>OmsRun</code> instance creation (by overriding the <code>save</code> method), but this proved to be too slow, especially when running tests.</p> <p>Therefore, this functionality is only triggered on demand, via a button on the <code>/reference/</code> URL of CertHelper.</p>"},{"location":"certhelper/apps/certifier/overview/","title":"certifier app","text":"<p>Arguably Certhelper's most important app. Contains tools and logic to certify run reconstructions.</p>"},{"location":"certhelper/apps/certifier/query/","title":"TrackerCertificationQuerySet","text":""},{"location":"certhelper/apps/certifier/views/addbadreason/","title":"addBadReason","text":"<p>A simple form served at <code>/certify/addbadreasonform/</code> so that it can be loaded using jQuery (using <code>load()</code>) on the <code>/certify/</code> page.</p> <p>The form to submit a new bad reason is dynamically loaded via jQuery from the <code>/certify/addbadreasonform/</code> URL and stored in the <code>div</code> with <code>id=\"addBadReason\"</code>. When the button to submit a new bad reason is pressed, a <code>POST XMLHttpRequest</code> is made to <code>/certify/badreasons/</code> with the <code>name</code> and <code>description</code> of the new bad reason. A <code>json</code> response is returned.</p>"},{"location":"certhelper/apps/certifier/views/badreason/","title":"badReason","text":"<p>Allows the user to get existing <code>Bad Reasons</code> and add new ones. Responds in <code>json</code> format.</p> <p>This view is triggered by clicking on the <code>+</code> sign next to the <code>Bad Reasons</code> dropdown menu  in the <code>/certify/</code> page (Adds new bad reason by sending a <code>POST XMLHttpRequest</code>).</p> <p>A simple <code>GET</code> request to the page will just return the existing <code>Bad Reasons</code>.</p>"},{"location":"certhelper/apps/certifier/views/certify/","title":"CertifyView","text":"<p>OOF</p> <p>Possibly the most intricate part of CertHelper,  which also requires a lot of refactoring. Brace yourselves.</p>"},{"location":"certhelper/apps/certifier/views/certify/#overview","title":"Overview","text":"<p>This is a multi-purpose<sup>1</sup> class-based view which handles the following:</p> <ul> <li>Renders a form to certify a combination of run number &amp; reconstruction type</li> <li>Allows the user to submit a complete certification form.</li> </ul> <p>Its final purpose is to guide the user in order to create <code>TrackerCertification</code> objects. As a side-effect, <code>OmsRun</code>, <code>OmsFill</code>, <code>Dataset</code> and <code>RunReconstruction</code> objects are also created.</p> <p>The user can land on this page from:</p> <ul> <li> <p>The <code>/openruns/</code> page:</p> <ul> <li> <p>By selecting a run number and (optionally) a reconstruction type on the top form (<code>GET</code> request).</p> </li> <li> <p>By clicking a colored button in the results listed after searching for openruns (bottom form, <code>GET</code> request).</p> </li> </ul> </li> <li> <p>The <code>/certify/</code> page:</p> <ul> <li>By submitting the complete certification form (<code>POST</code> request)</li> </ul> </li> </ul>"},{"location":"certhelper/apps/certifier/views/certify/#inputs","title":"Inputs","text":"<ul> <li>Run number (<code>int</code>, from URL, required)</li> <li>Reconstruction type (<code>str</code>, from URL, optional)</li> <li>Dataset name (<code>str</code>, from <code>GET</code> parameters, optional, only used when clicking on colored boxes in <code>/openruns/</code>)</li> </ul>"},{"location":"certhelper/apps/certifier/views/certify/#behavior","title":"Behavior","text":""},{"location":"certhelper/apps/certifier/views/certify/#on-class-creation","title":"On class creation","text":"<p>This is common behavior for both <code>GET</code> and <code>POST</code> and is contained in the <code>dispatch</code> method (the part of the view run before checking for <code>GET</code> or <code>POST</code>), which is overridden.</p> <ul> <li>Make sure that a run number and a reconstruction type are specified.</li> <li>Make sure that current user is allowed to certify specific reconstruction.</li> <li>If certification exists and user is the owner, redirect to update it, else continue below.</li> <li>Make sure an <code>OmsRun</code> and <code>OmsFill</code> object exists for specific run number.</li> <li>Run <code>GET</code> or <code>POST</code>-specific logic (below).</li> </ul> <p>This procedure could raise:</p> <ul> <li><code>RunReconstructionAllDatasetsCertified</code> if no uncertified datasets are found for this run number in RunRegistry. This redirects back to the <code>/openruns/</code> page, so that the user can choose another run number.</li> <li><code>ConnectionError</code> if RunRegistry is not accessible, and <code>ParseError</code> if there's some CERN SSO outage (see issue #136). This is pretty much equivalent with <code>ConnectionError</code>. In this case, the user is not allowed to proceed, since there is not enough information. A reconstruction type should be also supplied.</li> <li><code>RunRegistryReconstructionNotFound</code>, <code>RunRegistryNoAvailableDatasets</code> if no info was found on RunRegistry for this specific reconstruction or dataset.</li> <li><code>OmsApiRunNumberNotFound</code>, <code>OmsApiFillNumberNotFound</code> if no info was found on OMS API for this run or fill number.</li> </ul> <p>Ok this may be stupid</p> <p>...as this behavior is not probably needed for both <code>GET</code> and <code>POST</code>. TO BE REFACTORED AT SOME POINT</p>"},{"location":"certhelper/apps/certifier/views/certify/#get","title":"<code>GET</code>","text":"<ul> <li>Create a form for the user to certify the reconstruction. This form also contains information about whether there was complete information from RunRegistry and OMS API at the time (<code>external_info_complete</code>).</li> </ul> <pre><code>flowchart LR\nget(GET request) --&gt; get_rr[Get info from RR]\nget_rr --&gt; info_rr{Info available?}\ninfo_rr -- \"Yes\" --&gt; get_oms[Get info from OMS API]\nget_oms --&gt; info_oms{Info available?}\ninfo_oms -- \"Yes\" --&gt; ext_info_true[External Info Complete]\ninfo_oms -- \"No\" --&gt; ext_info_false[External Info Incomplete]\ninfo_rr -- \"No\" --&gt; ext_info_false\next_info_true --&gt; render[Render form]\next_info_false --&gt; render</code></pre> <p>Info</p> <p><code>external_info_complete</code> is updated and set as an attribute to the rendered form so that this information is available upon <code>POST</code>ing the form back and a <code>TrackerCertification</code> instance is created, where the <code>external_info_complete</code> is stored. </p>"},{"location":"certhelper/apps/certifier/views/certify/#post","title":"<code>POST</code>","text":"<ul> <li>Get or create a <code>RunReconstruction</code> object given the run number and the reconstruction type.</li> <li>If the dataset is specified (e.g. <code>/Express/Commissioning2022/DQM</code>), <code>get_or_create</code> a <code>Dataset</code> object.</li> <li>Parse the <code>POST</code>ed form.</li> <li>Check whether a <code>TrackerCertification</code> object exists for this combination of parameters, else create it.</li> <li> <p>If <code>external_info_complete</code> is <code>False</code> in the <code>POST</code> data (see  below):</p> <ul> <li> <p>Check if <code>OmsFill</code> exists with the info supplied, and create or update it with the info in the form. If object already exists, update it: <pre><code>omsfill_form = OmsFillForm(\n                request.POST,\n                instance=OmsFill.objects.get(\n                    fill_number=omsfill_form.data[\"fill_number\"]\n                ),\n            )\n</code></pre></p> </li> <li> <p>Since the <code>OmsRun</code> object has already been created before, even without RR/OMS information, update the appropriate <code>OmsRun</code> instance with the info supplied manually by the user with the form: <pre><code>omsrun_form = OmsRunForm(request.POST, instance=self.run)\n</code></pre> Where <code>self.run</code> is the <code>OmsRun</code>instance created before.</p> </li> </ul> </li> </ul> TRIGGER WARNING; NOT DRY <p>This last step of the procedure has been COPIED-PASTED to the <code>UpdateRunView</code> in the <code>listruns</code> app until a cleaner solution has been found.</p>"},{"location":"certhelper/apps/certifier/views/certify/#special-cases","title":"Special cases","text":""},{"location":"certhelper/apps/certifier/views/certify/#only-a-run-number-is-supplied","title":"Only a run number is supplied","text":"<p>This case is valid if the user navigates to <code>/openruns/</code> and only specifies a run number before pressing <code>Certify</code>:</p> <p></p> <p>The procedure is as follows:</p> <ul> <li>Query the RunRegistry using the supplied run number to get the next available uncertified dataset (e.g. <code>/Express/Commissioning2022/DQM</code>). This is done using the <code>rr_retrieve_next_uncertified_dataset</code> function.</li> <li>Then, to specify the reconstruction type, using the dataset name acquired in the previous step, the <code>get_reco_from_dataset</code> function is run (which simply searches for specific keywords in the dataset string, e.g. in the previous example, the reconstruction type would be <code>express</code>).</li> </ul>"},{"location":"certhelper/apps/certifier/views/certify/#a-combination-of-run_number-and-reconstruction-type-is-specified","title":"A combination of run_number and reconstruction type is specified","text":"<p>Steps specific to this case:</p> <ul> <li>The dataset is retrieved from RunRegistry using the run number and the reconstruction type specified (<code>rr_retrieve_dataset_by_reco</code>).</li> </ul> <p>The same exceptions raised above apply.</p>"},{"location":"certhelper/apps/certifier/views/certify/#a-dataset-is-specified-but-not-a-reconstruction-type","title":"A dataset is specified but not a reconstruction type","text":"<p>This case applies when the user clicks any of the dataset buttons on the <code>/openruns/</code> page, in the table generated when searching for open runs.</p>"},{"location":"certhelper/apps/certifier/views/certify/#requested-omsrun-andor-runregistry-information-is-not-available","title":"Requested OmsRun and/or RunRegistry information is not available","text":"<p>This can be caused either by:</p> <ul> <li>CertHelper not having access to RunRegistry or OMS API or</li> <li>Run number/run reconstruction was not found in RunRegistry or OMS API.</li> </ul> <p>Input fields are presented to the user instead of a display, so that they can edit the missing OmsRun and OmsFill information manually. This is achieved by providing two more forms to the <code>certify.html</code> template:</p> <ul> <li><code>OmsRunForm</code> (<code>omsrun_form</code>)</li> <li><code>OmsFillForm</code> (<code>omsfill_form</code>)</li> </ul> <p>Those fields are activated only if the <code>external_info_complete</code> attribute of the certification <code>form</code> is set to <code>False</code><sup>2</sup>. For example, for the <code>run_type</code> field of <code>OmsRun</code>:</p> <pre><code>{% if not form.external_info_complete.value %}\n    {% render_field omsrun_form.run_type class+=\"form-control form-select\" title=omsrun_form.run_type.help_text %}\n{% else %}\n    {{ run.run_type|capfirst }}\n{% endif %}\n</code></pre> <p>This was done for every field that needed to be editable by the user, no smarter way was found to do that.</p> <p>On <code>Submit</code> button press, a <code>POST</code> request is made (reminder that this also runs the <code>dispatch</code> method).</p>"},{"location":"certhelper/apps/certifier/views/certify/#certification-already-exists","title":"Certification already exists","text":"<ul> <li>Check if the user trying to certify this reconstruction is the same as the one who initially created it OR has shift leader rights</li> <li>Redirect to <code>UpdateRunView</code> of <code>listruns</code>.</li> </ul> <ol> <li> <p>And messy too  \u21a9</p> </li> <li> <p>Once again, this is done in the <code>dispatch</code> method\u00a0\u21a9</p> </li> </ol>"},{"location":"certhelper/apps/dqmhelper/overview/","title":"dqmhelper app","text":"<p>This is the Django project's main app, which was created automatically at the start of the project. Contains:</p> <ul> <li>Base templates to be used by all other apps,</li> <li><code>settings.py</code> for the project,</li> <li>The main <code>urls.py</code> file.</li> </ul>"},{"location":"certhelper/apps/home/overview/","title":"home app","text":"<p>A very simple app with the only responsibility of containing the home page where all the clickable images to other pages are shown.</p>"},{"location":"certhelper/apps/listruns/overview/","title":"listruns app","text":"<p>This is an app which handles:</p> <ul> <li>Listing the runs which have already been certified (<code>listruns</code>) and</li> <li>Updating information on existing <code>TrackerCertification</code> instances (<code>UpdateRunView</code>).</li> </ul> <p>Warning</p> <p>No idea why this is not integrated into the <code>certifier</code> app. Could be a good idea to merge those at some point.</p>"},{"location":"certhelper/apps/listruns/views/updaterun/","title":"UpdateRunView","text":"<p>Class-based view, inheriting from <code>UpdateView</code>, handling a form to udpate information on existing <code>TrackerCertification</code>s.</p>"},{"location":"certhelper/apps/listruns/views/updaterun/#methods","title":"Methods","text":""},{"location":"certhelper/apps/listruns/views/updaterun/#get","title":"<code>get</code>","text":"<p>The existing <code>get</code> method is overridden in order to add a warning message once the <code>certify.html</code> template is rendered to inform the user that they are updating an existing certification (if they are updating their own) or are updating another user's certification (if the user is a shift leader).</p>"},{"location":"certhelper/apps/listruns/views/updaterun/#post","title":"<code>post</code>","text":"<p>The default <code>post</code> method is overridden to account for the functionality existing in the <code>certify</code> view, which handles editing <code>OmsFill</code> and <code>OmsRun</code> information too.</p> <p>The <code>external_info_complete</code> flag is loaded from the <code>TrackerCertification</code> instance which is being updated.</p> <p>Warning</p> <p>This part of the code is copied from the <code>CertifyView</code>,  meaning that it kind of violates the DRY principle. </p> <p>Once again, this part of the code has no business being alone in a separate app, but should be refactored into the <code>certify</code> app at some point. </p>"},{"location":"certhelper/apps/oms/models/","title":"OMS Models","text":"<p>Information on the models described in the <code>oms/models.py</code> file.</p>"},{"location":"certhelper/apps/oms/models/#omsrun","title":"<code>OmsRun</code>","text":"<p>Contains information on CMS Runs. Not to be confused with LHC Runs.</p>"},{"location":"certhelper/apps/oms/models/#fields","title":"Fields","text":""},{"location":"certhelper/apps/oms/models/#apv_mode","title":"<code>apv_mode</code>","text":"<p>Warning</p> <p>No documentation was found for what APV mode is</p> <p>A field with possible values <code>DECO</code> or <code>PEAK</code>. Displayed as a column in the list of reference runs.</p> <p>This information is acquired as a separate HTTP request to ebutz.web.cern.ch. Updated when model instance's <code>update_apv_mode</code> method is called.</p>"},{"location":"certhelper/apps/oms/models/#methods","title":"Methods","text":""},{"location":"certhelper/apps/oms/models/#save","title":"<code>save</code>","text":"<p>The default <code>save</code> method is overridden so that specific flags of the model instance are updated:</p> <ul> <li><code>run_type</code></li> </ul>"},{"location":"certhelper/apps/oms/models/#update_apv_mode","title":"<code>update_apv_mode</code>","text":"<p>Private method to fetch APV mode information from ebutz.web.cern.ch. Updates the instance's <code>apv_mode</code> field and saves the model instance.</p>"},{"location":"certhelper/apps/oms/overview/","title":"oms app","text":"<p>This is an app which contains tools and models for fetching and storing information from:</p> <ul> <li>OMS (namely CMS Run and Fill information) and</li> <li>RunRegistry.</li> </ul> <p>Also uses tools available from the omsapi app.</p>"},{"location":"certhelper/apps/omsapi/overview/","title":"omsapi app","text":""},{"location":"certhelper/apps/remotescripts/configuration/","title":"Configuration","text":"<p>Most of the settings were adapted after following the  channels tutorial.</p> <p>Files that play a role in this configuration:</p> <ul> <li><code>dqmhelper/asgi.py</code>: The <code>application</code> variable is edited to include the <code>\"websocket\"</code> key.</li> <li><code>remotescripts/routing.py</code>: The equivalent of <code>urls.py</code> for channels. Routes a URL pattern to a <code>Consumer</code> (equivalent to a view).</li> </ul>"},{"location":"certhelper/apps/remotescripts/overview/","title":"remotescripts app","text":"<p>Expanding on the <code>trackermaps</code> app, this app tries to generalize remote script execution from certhelper, mainly to facilitate adding and customizing execution of tools which must be run on other computers (such as the <code>vocms066</code>).</p> <p>A script configuration is composed of:</p> <ul> <li>The script itself,</li> <li>Input Arguments (either positional or keyword) and</li> <li>File products generated by the script</li> </ul> <p><pre><code>flowchart LR\n    arg1[Positional Argument 1] --&gt; script[Script Configuration] \n    arg2[Positional Argument 2] --&gt; script\n    argdot[Positional Argument ...] --&gt; script  \n    kwarg1[Keyword Argument 1] --&gt; script\n    kwarg2[Keyword Argument 2] --&gt; script   \n    kwargdot[Keyword Argument ...] --&gt; script       \n    script --&gt; fp1[File Product 1]\n    script --&gt; fp2[File Product 2]  \n    script --&gt; fpdot[File Product ...]  </code></pre> An overview of the dataflow for executing a script can be seen below:</p> <pre><code>flowchart LR\n    User --&gt; View[View w/ Automatically generated form]\n    View -- Using script configuration --&gt; thread[Thread]\n    thread -- Execute script via SSH --&gt; remote[Remote Machine]\n    remote -- stdout + file products --&gt; thread\n    thread -- WebSocket --&gt; fe[\"FrontEnd (stdout + file products)\"]\n</code></pre> <p>WebSockets are used for real-time updating of the front-end with the scripts' console output, status updates and images created. This is implemented by using Django Channels, channels_redis and a Redis server which acts as the backing store.</p>"},{"location":"certhelper/apps/remotescripts/consumers/script_output/","title":"ScriptOutputConsumer","text":"<p>A <code>WebsocketConsumer</code> child class, which represents the code run when a client connects to the <code>ws/remotescripts/(?P&lt;script_id&gt;\\d+)/$</code> URL.</p>"},{"location":"certhelper/apps/remotescripts/consumers/script_output/#attributes","title":"Attributes","text":""},{"location":"certhelper/apps/remotescripts/consumers/script_output/#group_name","title":"<code>group_name</code>","text":"<p>This represents the name of the <code>channel_layer</code> that will be created and it's always formed based on the <code>script_id</code> part of the WS URL (i.e. <code>f\"output_{self.script_id}\"</code>).</p> <p>This, in effect, means that for each <code>remotescript</code> configuration created, a separate <code>channel_layer</code> will be created (e.g. <code>output_1</code> for remotescript with id = 1).</p>"},{"location":"certhelper/apps/remotescripts/consumers/script_output/#methods","title":"Methods","text":""},{"location":"certhelper/apps/remotescripts/consumers/script_output/#connect","title":"<code>connect</code>","text":"<p>This is run in the backend when a front-end WS client connects.</p> <p>The <code>script_id</code> is stored (using the kwargs in the <code>url_route</code>) so  that the appropriate <code>group_name</code> can be created.</p>"},{"location":"certhelper/apps/remotescripts/consumers/script_output/#script_output","title":"<code>script_output</code>","text":"<p>When this method is called, an <code>event</code> dictionary is passed to it. This contains all the information that the caller wants to push to the Websocket client connected to a specific channel group.</p> <p>A <code>message</code> key is expected to be found in the <code>event</code> dict, which is the content which is dumped to JSON and pushed to the front-end  WS client.</p> <p>This method is run indirectly from the <code>ScriptExecutionBaseView</code> (<code>views.py</code>) via the <code>channel_layer.group_send</code> function call. This is done by specifying the <code>type</code> parameter to be <code>script.output</code> (see channels' documentation).</p>"},{"location":"certhelper/apps/remotescripts/views/script_execution/","title":"ScriptExecution","text":"<p>Class-based views (inheriting from <code>DetailView</code>) which render a form automatically based on the configuration created based on a specific <code>RemoteScriptConfiguration</code> or <code>BashScriptConfiguration</code> (the latter has not been implemented yet).</p>"},{"location":"certhelper/apps/remotescripts/views/script_execution/#scriptexecutionbaseview","title":"<code>ScriptExecutionBaseView</code>","text":"<p>Base class-based view to be used by <code>RemoteScriptConfiguration</code>.</p>"},{"location":"certhelper/apps/remotescripts/views/script_execution/#remotescriptview","title":"<code>RemoteScriptView</code>","text":"<p>Inheriting from <code>ScriptExecutionBaseView</code>, it is a view intended to render a form for executing <code>RemoteScriptConfiguration</code> instances. </p>"},{"location":"certhelper/apps/remotescripts/views/script_execution/#methods","title":"Methods","text":""},{"location":"certhelper/apps/remotescripts/views/script_execution/#get","title":"<code>get</code>","text":"<p>Displays the dynamically-created form which is based on the <code>RemoteScriptConfiguration</code> instance.</p>"},{"location":"certhelper/apps/remotescripts/views/script_execution/#post","title":"<code>post</code>","text":"<p>This is triggered on submitting the rendered form via <code>XMLHttpRequest</code>.</p> <p>It prepares:</p> <ul> <li>The <code>channel_name</code> that the specific script should put its output to (see also consumers).</li> <li>The <code>args</code> and <code>kwargs</code> that were submitted with the form and parses their values so that they can be used to execute the script</li> <li>The <code>kwargs</code> are also augmented by callbacks, lambda functions that will be executed while the script is running (mainly to push specific messages on special parts of the script execution).</li> <li>A <code>Thread</code> to execute the actual script, so that the view does not get blocked while the script is running.</li> </ul> <p>Note</p> <p>The script execution takes place in the <code>execute</code> method defined in the <code>RemoteScriptConfiguration</code> model in <code>models.py</code></p>"},{"location":"certhelper/apps/shiftleader/obsolete/","title":"Obsolete code","text":""},{"location":"certhelper/apps/shiftleader/obsolete/#list-of-central-certification-runs","title":"List of central certification runs","text":"<p>On the <code>Weekly Certification</code> tab (<code>shiftleader/templates/shiftleader/weekly-cert.html</code>), a form field with id <code>id-cc-input</code> triggers a <code>keyup</code> event (<code>shiftleader/static/shiftleader/js/shiftleader.js</code>) which makes an AJAX request to an non-existent <code>ajax/validate-cc-list/</code> URL with a <code>text</code> parameter which is assigned the contents of the form field.</p> <p>As of writing, no documentation was found regarding this AJAX call so this functionality was removed altogether.</p>"},{"location":"certhelper/apps/shiftleader/overview/","title":"shiftleader app","text":""},{"location":"certhelper/apps/shiftleader/views/shiftleader_report/","title":"ShiftLeader Report","text":""},{"location":"certhelper/apps/shiftleader/views/shiftleader_report_presentation/","title":"ShiftLeader Report Presentation","text":"<p>The <code>ShiftLeaderReportPresentationView</code> calls the <code>ShiftLeaderReportPresentation</code> class (in <code>shiftleader/utiltities/shiftleader_report_presentation.py</code>) which  creates an Open Document Format Presentation using <code>odfpy</code>. </p> <p>Similarly to the <code>ShiftLeaderView</code>, the backbone of this view is also the <code>TrackerCertificationQuerySet</code>.</p>"},{"location":"certhelper/apps/shiftleader/views/shiftleader_report_presentation/#notes-tips","title":"Notes &amp; Tips","text":"<p>Warning</p> <p>The OpenDocumentFormat can be very confusing at first. You are advised to follow some simple examples found in the  <code>odfpy</code> repository.  Based on those, these prototypes  were created to test different setups and combinations.</p> <p><code>Style</code>s suck</p> <p>File format</p> <p>OpenDocumentFormat files are actually archives, containing two very important files:</p> <ul> <li><code>styles.xml</code></li> <li><code>content.xml</code></li> </ul> <p>Styles</p> <p>Some styles must have specific names, while <code>automaticstyles</code>  do not have this requirement. </p> <p>Automatic styles location</p> <p><code>automaticstyles</code> are stored in <code>content.xml</code></p> <p>Making an ODF file programmatically</p> <p>You are generally advised to make changes to an existing <code>.odp</code>/<code>.odt</code> file and then open them as archives to check the <code>contents.xml</code> and <code>styles.xml</code> files to see how those are structured, instead of trying to understand the vast specification from scratch.</p>"},{"location":"certhelper/apps/shiftleader/views/shiftleader_report_presentation/#links","title":"Links","text":"<ul> <li>OASIS Open Document Format 1.2 Specification</li> <li><code>odfpy</code> (Contains examples and reference, albeit not so good) </li> </ul>"},{"location":"certhelper/apps/summary/models/","title":"summary app models","text":""},{"location":"certhelper/apps/summary/models/#summaryinfo","title":"<code>SummaryInfo</code>","text":"<p>Contains extra information provided by the shifter on Summary generation:</p> <ul> <li>Links to Prompt Feedback prompts</li> <li>Special comments</li> </ul> <p>Each <code>SummaryInfo</code> instance is identified by a unique combination of <code>TrackerCertification</code> instances: When a shifter generates a report, in  the backend a <code>TrackerCertification</code> queryset is created which filters the certifications done by the specific user, for a specific day.</p> <p>The primary keys of this exact queryset (i.e. the <code>runreconstruction</code> field, which is actually the primary key of <code>RunReconstruction</code> instances) is made into a list, which is stored in the <code>certifications</code> fields of the <code>SummaryInfo</code> instances.</p> <p>This means that a list of <code>RunReconstruction</code> primary keys is the primary key of each  <code>SummaryInfo</code>.</p> <p>This was chosen instead of using a <code>ManyToManyField</code>, because each summary has to be unique for each combination of <code>TrackerCertification</code>s, and <code>ManyToManyField</code> cannot have <code>unique=True</code>.</p> <p>Warning</p> <p>There are two implications regarding this choice:</p> <ul> <li>The underlying database must always be PostgreSQL since it's the only one supporting <code>ArrayFields</code></li> <li>If a <code>TrackerCertification</code> is deleted, the corresopnding summary will not be automatically deleted, meaning that there may be summaries that refer to non-existent certifications in the long run.</li> </ul> <p>This information is used when the Shiftleader report or presentation are rendered (in the day-by-day certification).</p>"},{"location":"certhelper/apps/summary/overview/","title":"summary app","text":"<p>Application which handles Daily Report creation.</p>"},{"location":"certhelper/apps/summary/views/SummaryView/","title":"<code>SummaryView</code>","text":"<p>A class-based view which handles both <code>GET</code> and <code>POST</code> requests.</p>"},{"location":"certhelper/apps/summary/views/SummaryView/#get","title":"<code>GET</code>","text":"<p>When the <code>summary/</code> page is requested with <code>GET</code>, the filters supplied in the URL are checked. If no filters are supplied,  the current day is used to filter <code>TrackerCertification</code> instances.</p> <p>Then, only the requesting user's certifications are kept. </p> <p>Using the queryset that has been created (<code>runs</code>), a unique <code>SummaryInfo</code> instance is created using <code>get_or_create</code>.</p> <p>Then, a <code>form</code> is created, which, if the specific <code>SummaryInfo</code> instance did not exist, is empty. If the instance already existed, the form is pre-populated with the data from the <code>SummaryInfo</code> instance.</p> <p>Finally, the HTML template is rendered.</p>"},{"location":"certhelper/apps/summary/views/SummaryView/#post","title":"<code>POST</code>","text":"<p>In the generated summary, there are two fields (supplied by the <code>SummaryExtraInfoForm</code>) which are manually entered by the shifter (<code>links_prompt_feedback</code> and <code>special_comment</code>). Once the shifter submits either of those fields, a <code>POST</code> request is made by the front-end (via <code>ajax</code>) which updates the two fields in the <code>SummaryInfo</code> instance.</p> <p>In order to do that, the <code>certs_list</code> list is passed in the context of the  template, which is included in the <code>ajax</code> request, so that the <code>post</code> method of the view can identify which <code>SummaryInfo</code> instance to update. </p> <p>Info</p> <p>This should maybe be done with an <code>UpdateView</code> in the future.  As the feature was implemented hastily, I just refactored the old code subclassing a simple <code>View</code>.</p>"},{"location":"certhelper/deploying/deployments/","title":"Deployments","text":"<ul> <li>Production instance \u2014   Main instance, used by Shifters and Shift Leaders. Based on the <code>master</code> branch.</li> <li>Training instance \u2014   Instance used for training new Shifters, operating on a different Database. Based on the <code>training</code> branch.</li> <li>Developer instance \u2014   Instance used to test features before deploying to production instance. Based on the <code>develop</code> branch.</li> </ul>"},{"location":"certhelper/deploying/guide/","title":"Deployment Guide","text":"<p>The following steps will guide you through the deployment procedure of the app on OpenShift. An overview of the steps is:</p> <ol> <li>Create a new PaaS project</li> <li>Setup the repository that will be used</li> <li>Setup Environmental Variables</li> <li>Setup a Database</li> <li>Mount EOS Storage</li> <li>Add Redis Server</li> <li>Add nginx Server (not working for now)</li> <li>Single Sign-On</li> <li>Deploy</li> <li>Expose the app</li> </ol> <p>The procedure can be done completely via the web UI provided by PaaS. However, the <code>oc</code> command line utility can prove very useful. See below for instructions on how to install it.</p>"},{"location":"certhelper/deploying/guide/#prerequisites","title":"Prerequisites","text":""},{"location":"certhelper/deploying/guide/#oc-command-line-utility","title":"<code>oc</code> command line utility","text":"<p>Note</p> <p>Optional</p> <p>Download and install the <code>oc</code> command line utility, preferably on your lxplus account.</p>"},{"location":"certhelper/deploying/guide/#s2i-directory-inside-the-root-of-your-repository","title":"<code>.s2i</code> directory inside the root of your repository","text":"<p>We will be using the Software To Image (s2i) approach to deploy on PaaS, namely the Python flavor. This means that a Docker image is created from our repository on each deployment.</p> <p>There should be a <code>.s2i</code> directory inside your repository, with the <code>environment</code> file in it. Inside the <code>.s2i</code> directory, make sure there is a <code>bin</code> directory with an <code>assemble</code> file in it.</p>"},{"location":"certhelper/deploying/guide/#environment-contents","title":"<code>environment</code> contents","text":"<p>These are environmental variables used by Openshift when creating the Docker image. The value of <code>APP_SCRIPT</code> will be the entrypoint of the created image.</p> <pre><code>DISABLE_COLLECTSTATIC=true\nAPP_SCRIPT=openshift-start-up-script.sh\n</code></pre>"},{"location":"certhelper/deploying/guide/#assemble-contents","title":"<code>assemble</code> contents","text":"<p>This will override the <code>assemble</code> stage of the s2i procedure, so that we can do configuration as needed. Most important configuration change is CERN gitlab authentication which is required to <code>pip install</code> from private repositories.</p> <pre><code>#!/bin/bash\necho \"Before assembling\"\ngit config --global url.\"https://$CERN_GITLAB_USER:$CERN_GITLAB_TOKEN@gitlab.cern.ch\".insteadOf https://gitlab.cern.ch\n/usr/libexec/s2i/assemble\nrc=$?\n\nif [ $rc -eq 0 ]; then\n  echo \"After successful assembling\"\nelse\n  echo \"After failed assembling\"\nfi\n\nexit $rc\n</code></pre> <p>Note</p> <p>See Setup Environmental Variables for the required environmental variables used in the script above.</p>"},{"location":"certhelper/deploying/guide/#requesting-a-website","title":"Requesting a website","text":"<p>Create a new PaaS project by clicking here. Then, fill out the fields as shown below:</p> <p></p> <p>When creating a website, different site types can be chosen. In order to use the OpenShift software, the <code>PaaS Web Application</code> option has to be selected.</p>"},{"location":"certhelper/deploying/guide/#setup-procedure","title":"Setup Procedure","text":"<p>Once the website is successfully requested the application should be available in OpenShift. Following steps need to be done in order to configure the web application with the GitHub repository:</p> <ol> <li>Go to PaaS.</li> <li> <p>Select the project you created</p> <p></p> </li> <li> <p>Click on \"Add\" on the left</p> <p></p> </li> <li> <p>choose <code>Git Repository</code></p> <p></p> </li> <li> <p>Paste the repository URL in the field provided.</p> </li> <li>Under <code>Advanced Git options</code>, you may select a specific branch, if needed. E.g. for the  training certhelper instance, the <code>training</code> branch must be selected.</li> <li> <p>okd will automatically detect that this is a Python application and will select the latest version of Python.</p> <p>Info</p> <p>As of writing, we select <code>3.8-ubi8</code>.</p> </li> <li> <p>Under General, change the Application name and Name appropriately. </p> </li> <li> <p>Under Resources, select Deployment</p> <p></p> </li> <li> <p>[Optional] Add GitHub credentials at \"Source Secret\" if the repository is     private</p> </li> <li> <p>Make sure that Create a route to the Application is ticked.</p> </li> <li>Under Show advanced Routing options:     a. Paste the Hostname you want (will be automatically registered),     b. Make sure Secure Route is ticked,     c. Under TLS termination, select <code>Edge</code>,     d. Under Insecure Traffic, select <code>Redirect</code>.</li> <li> <p>Click on Create. The application has been configured!</p> <p>Note</p> <p>Under Topology, you will see your project trying to run for the first time. This will fail, since most environmental variables are missing. Click on the main app:</p> <p></p> <p>You should be getting the following error:</p> <p></p> </li> </ol>"},{"location":"certhelper/deploying/guide/#setup-environmental-variables","title":"Setup Environmental Variables","text":"<ol> <li> <p>Under <code>Builds --&gt; Your project name --&gt; Environment</code> use the <code>Add more</code> and <code>Add from ConfigMap or Secret</code> buttons to add the variables:</p> <ol> <li> <p>Accounts/Secrets environment variables (added using <code>Add Value from Config Map or Secret</code> button):</p> <ul> <li>Database credentials:</li> </ul> <pre><code>DJANGO_SECRET_KEY          &lt;your-secret&gt;\nDJANGO_DATABASE_USER       &lt;your-username&gt;\nDJANGO_DATABASE_PASSWORD   &lt;your-password&gt;\n</code></pre> <ul> <li>Email notifications:</li> </ul> <pre><code>DJANGO_EMAIL_HOST_USER     &lt;your-email-username&gt;\nDJANGO_EMAIL_HOST_PASSWORD &lt;your-email-password&gt;\n</code></pre> <ul> <li>Tracker Maps credentials:</li> </ul> <pre><code>DJANGO_SECRET_ACC           &lt;account-username&gt;\nDJANGO_SECRET_PASS          &lt;account-password&gt;\n</code></pre> </li> <li> <p>Remaining Variables (added using <code>Add Value</code> button):</p> <ul> <li>Needed for OpenShift to be able to access the site:</li> </ul> <pre><code>DJANGO_ALLOWED_HOSTS       &lt;Host website you registered in step 12.a&gt;\nDJANGO_DEBUG               False\n</code></pre> <ul> <li>this will be used for the database credentials:</li> </ul> <pre><code>DJANGO_DATABASE_ENGINE     django.db.backends.postgresql_psycopg2\nDJANGO_DATABASE_NAME       &lt;your-database-name&gt;\nDJANGO_DATABASE_HOST       &lt;your-database-host&gt;\nDJANGO_DATABASE_PORT       6611\n</code></pre> <ul> <li>this will be used for the email notifications:</li> </ul> <pre><code>DJANGO_EMAIL_HOST          smtp.cern.ch\nDJANGO_EMAIL_PORT          587\nDJANGO_EMAIL_USE_TLS       True\nDJANGO_SERVER_EMAIL        &lt;tkdqmdoctor-email-address&gt;\n</code></pre> <ul> <li>this will be used for the cernrequest and Runregistry API:</li> </ul> <pre><code>CERN_CERTIFICATE_PATH       &lt;path&gt;\n</code></pre> <ul> <li>this will be used to access the Redis server (secret is created automatically by the redis yaml):</li> </ul> <pre><code>REDIS_HOST                  &lt;redis-[server number]&gt;\nREDIS_PASSWORD              &lt;password&gt;\n</code></pre> </li> <li> <p>Other:</p> </li> </ol> <pre><code>CSRF_TRUSTED_ORIGINS        https://[the hostname you resistered in step 12a]\n</code></pre> <ol> <li>To access CERN's gitlab private repositories via <code>pip</code>:</li> </ol> <pre><code>CERN_GITLAB_USER           &lt;CERN gitlab username&gt;\nCERN_GITLAB_TOKEN          &lt;CERN gitlab access token with read_repository permissions&gt;\n</code></pre> </li> <li> <p>Save the variables and rebuild the project:     </p> <p>You should now be able to visit the app on the URL you specified.</p> </li> </ol> <p>Note</p> <p>The procedure above should only be followed once. Once the app is fully configured, you should not have to alter anything, unless a change occurs (e.g. Database host/password).</p>"},{"location":"certhelper/deploying/guide/#setup-a-database","title":"Setup a Database","text":"<p>The database was requested from the CERN DB on demand service. To request one,  follow the instructions here.</p> <p>A PostgreSQL database is used.</p> <p>After the database has been requested it can be used straight away. Django takes care of creating the necessary tables and only requires the credentials.</p>"},{"location":"certhelper/deploying/guide/#actions-to-take-once-the-database-is-ready","title":"Actions to take once the database is ready","text":""},{"location":"certhelper/deploying/guide/#change-default-password","title":"Change default password","text":"<p>Given the username that was sent to you via the DBoD Service, (possibly <code>admin</code>) connect to the database:</p> <pre><code>psql -h dbod-birdup.cern.ch -U admin -p 6601\n</code></pre> <p>And run:</p> <pre><code>ALTER ROLE admin WITH PASSWORD 'new_password';\n</code></pre>"},{"location":"certhelper/deploying/guide/#ssl-configuration","title":"SSL Configuration","text":"<p>Error</p> <pre><code>django.db.utils.OperationalError: connection to server at \n\"&lt;host&gt;\" (&lt;ip&gt;), port 6601 failed: FATAL:  no pg_hba.conf entry for host \"&lt;ip&gt;\"\n</code></pre> <p>Follow the instructions here to edit the required configuration files using the file editor.</p>"},{"location":"certhelper/deploying/guide/#create-the-database","title":"Create the database","text":"<p>Assuming that the database name you are going to use is <code>certhelperdb</code> (i.e. <code>DJANGO_DATABASE_NAME</code> is <code>certhelperdb</code>), you will need to  create it manually first.</p> <p>To do so, you will have to first connect to it using <code>psql</code><sup>1</sup>:</p> <pre><code>psql -h &lt;Database hostname&gt; -p &lt;Database port&gt; -U &lt;Database user&gt;\n</code></pre> <p>Then, in the SQL prompt, run the following to create the database:</p> <pre><code>CREATE DATABASE certhelperdb;\n</code></pre> <p>and enter your password once prompted.</p>"},{"location":"certhelper/deploying/guide/#mount-eos-storage","title":"Mount EOS Storage","text":""},{"location":"certhelper/deploying/guide/#via-the-ui","title":"Via the UI","text":"<p>Info</p> <p>Recommended</p> <p>The project has 1 TB of storage associated in the EOS. To mount it to OpenShift follow these instructions.</p> <p>Detailed instructions can be found on the PaaS docs.</p>"},{"location":"certhelper/deploying/guide/#using-oc","title":"Using <code>oc</code>","text":"<p>Warning</p> <p>Might be deprecated</p>"},{"location":"certhelper/deploying/guide/#create-secret","title":"Create Secret","text":"<p>Replace with your password.</p> <pre><code>oc create secret generic eos-credentials --type=eos.cern.ch/credentials --from-literal=keytab-user=tkdqmdoc --from-literal=keytab-pwd=&lt;the-password&gt;\n</code></pre>"},{"location":"certhelper/deploying/guide/#do-eos-stuff","title":"Do EOS stuff","text":"<p>Run these commands and replace with the name of your build.</p> <pre><code>oc set volume dc/&lt;your-build-name&gt; --add --name=eos --type=persistentVolumeClaim --mount-path=/eos --claim-name=eos-volume --claim-class=eos --claim-size=1\n\noc patch dc/&lt;your-build-name&gt; -p \"$(curl --silent https://gitlab.cern.ch/paas-tools/eosclient-openshift/raw/master/eosclient-container-patch.json)\"\n\noc set probe dc/&lt;your-build-name&gt; --liveness --initial-delay-seconds=30 -- stat /eos/project/t/tkdqmdoc\n\noc set probe dc/&lt;your-build-name&gt; --readiness -- stat /eos/project/t/tkdqmdoc\n</code></pre> <p>if it gets stuck or you encouter some errors on openshift like:</p> <p><code>Readiness probe failed: stat: cannot stat '/eos/project/t/tkdqmdoc': No such file or directory</code></p> <p>then rerun all 4 commands again:</p> <p><pre><code>oc set volume dc/&lt;your-build-name&gt; --add --name=eos --type=persistentVolumeClaim --mount-path=/eos --claim-name=eos-volume --claim-class=eos --claim-size=1\n\noc patch dc/&lt;your-build-name&gt; -p \"$(curl --silent https://gitlab.cern.ch/paas-tools/eosclient-openshift/raw/master/eosclient-container-patch.json)\"\n\noc set probe dc/&lt;your-build-name&gt; --liveness --initial-delay-seconds=30 -- stat /eos/project/t/tkdqmdoc\n\noc set probe dc/&lt;your-build-name&gt; --readiness -- stat /eos/project/t/tkdqmdoc\n</code></pre> Then start the built and it should work.</p> <p>Tip</p> <p>For deleting the volume run the following command first</p> <pre><code>kubectl patch pvc PVC_NAME -p '{\"metadata\":{\"finalizers\": []}}' --type=merge\n</code></pre>"},{"location":"certhelper/deploying/guide/#add-shared-volume","title":"Add shared volume","text":"<p>Add a shared volume to allow the use of unix socket between nginx and daphne</p> <pre><code>oc set volume dc/&lt;your-build-name&gt; --add --name=&lt;volume-name&gt; --type=persistentVolumeClaim --mount-path=&lt;path&gt; --claim-name=&lt;volume-name&gt; --claim-class=cephfs-no-backup --claim-size=1\n</code></pre>"},{"location":"certhelper/deploying/guide/#add-redis-server","title":"Add Redis Server","text":"<p>A redis server will used by the <code>channels-redis</code> module as a backing store. </p> <p>Navigate to <code>Topology</code> and right-click next to the pod of the project. Then, click <code>Add to Project --&gt; From Catalog</code>.</p> <p></p> <p>Then, search for and select <code>Redis</code>, and then <code>Instantiate Template</code>. </p> <p></p> <p>Choose <code>5-el8</code> as the <code>Version of Redis Image</code>. <code>6-el8</code> was tested but did not work as of writing (2022/07). Leave all other settings to their default values. Take note of the <code>Database Service Name</code>, which will serve as the hostname that Django will have to connect to.</p> <p>Click on <code>Create</code>. This will automatically place a new pod on the topology, which is effectively a separate system running a redis server.</p> <p>Verify that by navigating to <code>Secrets</code>, a new <code>redis</code> secret which has been created.</p> <p>Now, navigate to <code>Developer --&gt; Builds --&gt; &lt;Your Project&gt; --&gt; Environment</code> and add two new values:</p> <ul> <li>Click on <code>Add more</code> and name the new key <code>REDIS_HOST</code>. Its value must be equal to the hostname you noted earlier.</li> <li>Click on <code>Add from ConfigMap or Secret</code> and name the new key <code>REDIS_PASSWORD</code>. Its value must be the <code>redis --&gt; database-password</code> secret.</li> </ul> <p>Rebuild the main project and, by connecting to Tracker Maps, you should not be getting any errors in the Django logs.</p>"},{"location":"certhelper/deploying/guide/#add-nginx-server-not-working-for-now","title":"Add nginx Server (not working for now)","text":"<p>Warning</p> <p>Not tested/Deprecated</p> <ol> <li>Go to the webconsole</li> <li>choose \"Nginx HTTP server and a reverse proxy (nginx)\"</li> <li>click <code>Next</code></li> <li>select your project in <code>Add to Project</code></li> <li>choose a name</li> <li>add this git repository</li> <li>click <code>Create</code></li> <li>add the shared volume</li> </ol> <pre><code>oc set volume dc/&lt;your-chosen-name&gt; --add --name=&lt;volume-name&gt; --type=persistentVolumeClaim --mount-path=&lt;path&gt; --claim-name=&lt;volume-name&gt; --claim-class=cephfs-no-backup --claim-size=1\n</code></pre> <ol> <li>go to <code>Application --&gt; Routes</code></li> <li>replace the dev-certhelper route with an one for nginx-server</li> </ol>"},{"location":"certhelper/deploying/guide/#single-sign-on","title":"Single Sign-On","text":""},{"location":"certhelper/deploying/guide/#cern-setup","title":"CERN Setup","text":"<p>OIDC is an authorization service which can be used to authenticate CERN users. The advantage of using such an authorization service is that users of the certification helper do not have register manually, but can already use their existing CERN accounts.</p> <ul> <li>Visit the application portal.</li> <li>Add a new application registration.</li> <li>Click on <code>SSO Registration</code> and generate an <code>OpenID Connect</code> provider.<ul> <li>For <code>redirect_uri</code>, use <code>https://certhelper.web.cern.ch/accounts/cern/login/callback/</code> for the production website and <code>https://dev-certhelper.web.cern.ch/accounts/cern/login/callback/</code> for the development site.</li> </ul> </li> <li>Note the <code>Client ID</code> and <code>Client Secrets</code>.</li> </ul> <p>Note</p> <p>Each instance of certhelper (production, development, training) requires a different SSO registration key, so you cannot reuse an existing <code>client_id</code> and <code>secret</code> for a new instance.</p>"},{"location":"certhelper/deploying/guide/#integration","title":"Integration","text":"<p>The single sign-on integration is very easy when using the <code>django-allauth</code> python package, which has built-in OIDC support.</p> <p>Follow the installation procedure here.</p> <p>Certhelper uses the <code>openid_connect</code> provider. There used to be CERN integration with <code>django-allauth</code>, but this has been deprecated after CERN's migration to a new SSO solution, and updating was proved to be reduntant. In its place, the <code>openid_connect</code> provider works fine.</p> <p>Notes on the installation procedure</p> <ul> <li>When adding a <code>Site</code>, use the complete URL of the app (e.g. <code>https://certhelper.web.cern.ch/</code>).</li> <li>Just in case, visit <code>/admin/socialaccount/socialapp/1/change/</code> to add the Client ID and secret there too.</li> <li>Verify the <code>SITE_ID</code> value by checking the database itself. E.g. it might ge <code>1</code> or <code>2</code></li> <li>Add <code>ACCOUNT_EMAIL_VERIFICATION = \"none\"</code> in <code>settings.py</code> to disable sending  a verification email on first signup.</li> </ul>"},{"location":"certhelper/deploying/guide/#deploying-a-new-build","title":"Deploying a new build","text":""},{"location":"certhelper/deploying/guide/#production-site-certhelper","title":"Production Site (<code>certhelper</code>)","text":"<p>If you want to rebuild the production website (<code>master</code> branch) you can do so manually by triggering a build on PaaS.</p> <p>This can be done by visiting paas.cern.ch, selecting the <code>certhelper</code> project and then visiting <code>Build --&gt; builds</code>. This page should already contain a build of the Certification Helper project that is automatically pulled from GitHub. By clicking on this build and then pressing the <code>build</code> button the whole deployment process should be started. In the meantime, the logs of the build process can be viewed by clicking on <code>View Log</code>.</p>"},{"location":"certhelper/deploying/guide/#deployment-schedule","title":"Deployment Schedule","text":"<p>The main <code>certhelper</code> instance should only be deployed on shift changes, to prevent confusion and/or any inconvenience. Contact a Shift Leader to verify the schedule.</p> <p>The <code>training-certhelper</code> instance should also be updated whenever no training is taking place.</p> <p>The <code>dev-certhelper</code> instance can be updated at will.</p> <p>To automate deployment, use OpenShift's <code>CronJobs</code> to create pods based on the <code>curlimages/curl</code> image:</p> <ul> <li>Navigate to the project's BuildConfig, find the <code>Generic</code> webhook shown at the bottom of the page and click <code>Copy URL with Secret</code>.</li> <li>Navigate to <code>Administrator --&gt; Workloads --&gt; CronJobs</code> (link) and create a new <code>CronJob</code>.</li> <li>Update <code>name</code> under <code>metadata</code> to something meaningful (e.g.: <code>scheduled-deployment</code>)</li> <li>Update <code>schedule</code> under <code>spec</code> to the desired crontab (e.g.: <code>'0 0 * * 1,5'</code>, time is in UTC)</li> <li>Use <code>curlimages/curl</code> as <code>image</code></li> <li>Under <code>args</code> paste: <pre><code>args:\n    - curl\n    - '-X'\n    - POST\n    - '-k'\n    - &gt;-\n        &lt;the Generic Webhook you copied earlier&gt;\n</code></pre></li> </ul> <p>A new pod will be created under the crontab schedule you configured, triggering a new build.</p> <p>Warning</p> <p>It is generally not a good idea to have the CronJob running at all times, mainly due to the fact that PyPI libraries may be updated at any time; automatic deployment with the latest available libraries is going to be a risk, as they new versions may well contain bugs. </p> <p>A safe approach is to enable the CronJob by setting <code>suspend: false</code> (see here) only once you have pushed changes to the certifier repository. Then, once the changes are deployed, set <code>suspend: true</code> again.</p>"},{"location":"certhelper/deploying/guide/#exposing-the-app","title":"Exposing the app","text":"<p>See the PaaS docs on how to make the app visible from outside the CERN GPN.</p> <ol> <li> <p>You will either have to do that through LXPLUS, or your computer must be inside CERN. You can always use an SSH tunnel for that.\u00a0\u21a9</p> </li> </ol>"},{"location":"certhelper/deploying/overview/","title":"Deployment overview","text":"<p>Deployed on PaaS, using s2i.</p> <p>Three different instances are deployed, based on three different git brances. They can be found in Deployments.</p>"},{"location":"certhelper/developing/branches/","title":"CertHelper Branches","text":"<p>Todo</p> <p>TODO</p>"},{"location":"certhelper/developing/branches/#overview","title":"Overview","text":"<p>There are three main branches in the repository, each with its own importance:</p> <ul> <li><code>master</code> branch, which contains code which is    thoroughly tested and production-ready, meaning   as few bugs as possible (if not zero). CertHelper's production instance   builds from this branch.</li> <li><code>training</code> branch, which should be synchronized with the <code>master</code> branch   at all times. It contains minor changes which make it useful for training   new Shifters. CertHelper's training instance builds from this branch.</li> <li><code>develop</code> branch, where all development takes place. CertHelper's development   instance builds from this branch.</li> </ul>"},{"location":"certhelper/developing/branches/#developing","title":"Developing","text":"<p>The following is a suggested strategy for working with CertHelper branches.</p>"},{"location":"certhelper/developing/branches/#working-on-a-feature-or-bug","title":"Working on a feature or bug","text":"<ul> <li>Create an issue on github</li> <li>Create a new branch with the number of the issue as a name, starting from <code>develop</code>, i.e.:   <pre><code>git checkout develop\ngit pull origin develop\ngit checkout -b \"#71\"\n</code></pre>   and do whatever changes you need to do.</li> <li>Push the branch to github:   <pre><code>git push origin \"#71\"\n</code></pre></li> <li>Go to github and create a Pull Request to merge the <code>#71</code> branch to the <code>develop</code> one (important! Don't choose the <code>master</code> as target!)</li> <li>Merge your Pull Request to <code>develop</code>.</li> </ul> <p>You can start new builds on the <code>certhelper-dev</code> deployment at will, which will pull all latest changes from the <code>develop</code> branch.</p>"},{"location":"certhelper/developing/branches/#merging-develop-to-master","title":"Merging <code>develop</code> to <code>master</code>","text":"<p>Once you have tested your changes on the development deployment, you will need to update the main CertHelper instance.</p> <ul> <li>Make sure you have updated the <code>CERTHELPER_VERSION</code> string in <code>dqmhelper/settings.py</code>, following the Semantic Versioning guide.</li> <li>Merge the <code>develop</code> branch to <code>master</code>:   <pre><code>git checkout master\ngit pull origin master\ngit merge --no-ff develop\ngit push master\n</code></pre></li> <li>Put a tag on the <code>master</code> branch branch, with the same name as the <code>CERTHELPER_VERSION</code> string, e.g.:   <pre><code>git checkout master\ngit tag 1.10.0\ngit push --tags\n</code></pre></li> <li>Merge the <code>master</code> branch to <code>training</code>:   <pre><code>git checkout training\ngit pull origin training\ngit merge master\ngit push training\n</code></pre></li> </ul> <p>Warning</p> <p>The <code>master</code> and <code>training</code> branches must always be in sync.</p>"},{"location":"certhelper/developing/local-development/","title":"Local Development","text":"<p>To run CertHelper locally, you will need:</p> <ul> <li>A Postgres instance:<ul> <li>You can use a local instance, but you will have no data inside or have to populate it yourself, or</li> <li>You can use the DBoD Development Database instance (hosted on <code>dbod-devcertdb.cern.ch</code>, database <code>developdb</code>). The latter requires you to either be within the CERN network or an SSH tunnel to CERN (see: <code>sshuttle</code>).</li> </ul> </li> <li>A Redis instance, with the default settings, hosted locally, on port <code>6379</code>. Instructions to download it here.</li> <li>An <code>.env</code> file with the following variables inside:<ul> <li><code>DJANGO_DATABASE_ENGINE=django.db.backends.postgresql</code></li> <li><code>DJANGO_DEBUG=True</code></li> <li><code>DJANGO_DATABASE_HOST</code>: change it to the appropriate PostgreSQL host.</li> <li><code>DJANGO_DATABASE_NAME=developdb</code> if using the instance on DBoD, else change it to whatever is the name of the database you created locally.</li> <li><code>DJANGO_DATABASE_USER</code>: the username for logging into the database.</li> <li><code>DJANGO_DATABASE_PASSWORD</code>: the password for logging into the database.</li> <li><code>DJANGO_DATABASE_PORT</code>: the port to connect to the database.</li> <li><code>DJANGO_SECRET_KEY</code>: a random string, can be anything you want.</li> <li><code>CERN_SSO_REGISTRATION_CLIENT_ID</code>: This is the client id for the SSO registration of the application to login users through the CERN SSO. Can be a random string if you don't intend to use CERN SSO login locally. If you need to find the actual values, go to the Application Portal and login with user <code>tkdqmdoc</code>.</li> <li><code>CERN_SSO_REGISTRATION_CLIENT_SECRET</code>: The client secret of the aformentioned SSO registration. Can also be a random string for local development.</li> <li><code>OMS_CLIENT_ID</code>: SSO registation of certhelper in order to connect to the OMS API. This specidic SSO registration can be found from the Build Env vars on PaaS. This registration has also been whitelisted by the OMS team to access the API. </li> <li><code>OMS_CLIENT_SECRET</code>: Secret of aforementioned client id.</li> <li><code>SSO_CLIENT_ID</code>: Can be the same with <code>OMS_CLIENT_ID</code></li> <li><code>SSO_CLIENT_SECRET</code>: Can be the same with <code>OMS_CLIENT_SECRET</code>.</li> <li><code>SITE_ID</code>: You must first add a <code>Sites</code> entry from the admin page. Then, you# can see the id by running <code>python manage.py shell</code> and then:   <pre><code>from django.contrib.sites.models import Site\ns=Site.objects.all()\ns[0]\ns[1]\n</code></pre>   It will probably be <code>1</code>.</li> </ul> </li> </ul> <p>Then, follow the generic instructions on Django local setup, found here.</p>"},{"location":"cmssw/available_machines/","title":"Available machines","text":"<p>As an overview, you have three choices in order to run GPU-enabled code:</p> <ul> <li>The very heavily-used <code>lxplus-gpu</code> machines ([NOT RECOMMENDED]).</li> <li>The Point5's Online Machines (RECOMMENDED).</li> <li>Special nodes.</li> </ul>"},{"location":"cmssw/available_machines/#lxplus-gpu","title":"lxplus-gpu","text":"<p>The lxplus service offers <code>lxplus-gpu.cern.ch</code> for shared GPU instances - with limited isolation and performance.</p> <p>One can connect similary as would do to the <code>lxplus.cern.ch</code> host domain.</p> <pre><code>ssh &lt;username&gt;@lxplus-gpu.cern.ch [-X]\n</code></pre> If connecting directly from your computer <p>You might need to initialize kerberos and execute bash again from within the machine:</p> <pre><code>kinit\nexec bash\n</code></pre>"},{"location":"cmssw/available_machines/#cms-point-5-machines","title":"CMS Point 5 Machines","text":"Info <p>This section is taken from the  CMS TWiki TriggerDevelopmentWithGPUs  page.</p> <p>There are 10 machines available for general development and validation of the online reconstruction on GPUs:</p> <ul> <li><code>gpu-c2a02-35-01.cms</code></li> <li><code>gpu-c2a02-35-02.cms</code></li> <li><code>gpu-c2a02-37-01.cms</code></li> <li><code>gpu-c2a02-37-02.cms</code> (currently without a GPU)</li> <li><code>gpu-c2a02-37-03.cms</code></li> <li><code>gpu-c2a02-37-04.cms</code></li> <li><code>gpu-c2a02-39-01.cms</code></li> <li><code>gpu-c2a02-39-02.cms</code> (Preferred)</li> <li><code>gpu-c2a02-39-03.cms</code></li> <li><code>gpu-c2a02-39-04.cms</code></li> </ul> <p>These are dedicated machines for the development of the online reconstruction.</p> <p>To access them, you will first need a CMS online account. See below for instructions.</p>"},{"location":"cmssw/available_machines/#request-a-cms-online-account","title":"Request a CMS Online account","text":"<p>To request access, please subscribe to the cms-hlt-gpu e-group and send an email to andrea.bocci@cern.ch, indicating:</p> <ul> <li>whether you already have an online account;</li> <li>your online or lxplus username;</li> <li>your full name and email.</li> </ul>"},{"location":"cmssw/available_machines/#how-to-connect","title":"How to connect","text":"<p>Requirements:</p> <ul> <li>Have a CMS online account and</li> <li>Be in the <code>gpudev</code> group.</li> </ul> <p>To connect directly from your computer:</p> <ul> <li>Create a proxy:   <pre><code>ssh -f -N -D18080 &lt;username&gt;@cmsusr.cern.ch\n</code></pre></li> <li>Connect via SSH:   <pre><code>ssh -o ProxyCommand='nc --proxy localhost:18080 --proxy-type socks5 %h %p' &lt;username&gt;@gpu-c2a02-39-02.cms\n</code></pre>   or    <pre><code>ssh -o ProxyCommand='nc -x localhost:18080 -X 5 %h %p' &lt;username&gt;@gpu-c2a02-39-02.cms\n</code></pre></li> </ul> <p>Note</p> <p>More detailed instructions here</p>"},{"location":"cmssw/available_machines/#special-configuration-required","title":"Special configuration required","text":"<ul> <li>To make commands like <code>cmsenv</code> and <code>cmsrel</code> available, run    <pre><code>source /cvmfs/cms.cern.ch/cmsset_default.sh\n</code></pre>   first.</li> <li>To allow connecting to GitHub via HTTP:<ul> <li>Configure the SOCKS proxy</li> <li>Open the proxy: <pre><code>ssh -f -N cmsusr.cms\n</code></pre></li> <li>Configure <code>git</code>: <pre><code>git config --global --replace-all http.proxy socks5://localhost:18080\n</code></pre></li> </ul> </li> <li>Set the correct <code>SCRAM_ARCH</code> for these machines: <pre><code>export SCRAM_ARCH=el8_amd64_gcc10\n</code></pre></li> </ul>"},{"location":"cmssw/available_machines/#notes","title":"Notes","text":"<ul> <li>These machines lie in a different subnet than the one that the LXPLUS machines   belong to.</li> <li>A side-effect of the previous point is that those machines do not   have access to the Grid.</li> <li>The <code>/nfshome0/&lt;username&gt;</code> directory is shared and available from all the machines above,   but has limited space.</li> <li>The <code>/data/user/&lt;username&gt;</code> directory is not shared across the devices,   but has larger capacity.</li> <li>CMS Cluster Users Guide</li> <li>Use <code>curl</code> with the <code>--socks5</code> flag like so: <code>curl --socks5 socks5://localhost:18080 &lt;url&gt;</code></li> </ul>"},{"location":"cmssw/available_machines/#useful-commands","title":"Useful commands","text":""},{"location":"cmssw/available_machines/#transfering-files-tofrom-p5-machines","title":"Transfering files to/from P5 machines","text":"<p>From your own computer:</p> <pre><code>scp -r -o ProxyCommand='nc -x localhost:18080 -X 5 %h %p' &lt;username&gt;@gpu-c2a02-39-01.cms:/remote/path /local/path\n</code></pre> <p>This prevents the <code>nc: invalid option -- '-'</code> error.</p>"},{"location":"cmssw/available_machines/#special-gpu-nodes","title":"Special GPU nodes","text":"Info <p>This section is more or less taken from the Patatrack website systems subpage.</p>"},{"location":"cmssw/available_machines/#cmg-gpu1080","title":"cmg-gpu1080","text":""},{"location":"cmssw/available_machines/#system-information","title":"System information","text":"<p>Topology of the machine</p>"},{"location":"cmssw/available_machines/#getting-access-to-the-machine","title":"Getting access to the machine","text":"<p>In order to get access to the machine you should send a request to subscribe to the CERN e-group:  cms-gpu-devel.</p> <p>You should also send an email to Felice Pantaleo motivating the reason for the requested access.</p>"},{"location":"cmssw/available_machines/#usage-policy","title":"Usage Policy","text":"<p>Normally, no more than 1 GPU per users should be used. To limit visible devices use</p> <pre><code>export CUDA_VISIBLE_DEVICES=&lt;list of numbers&gt;\n</code></pre> <p>Where <code>&lt;list of numbers&gt;</code> can be e.g. <code>0</code>, <code>0,4</code>, <code>1,2,3</code>. Use <code>nvidia-smi</code> to check available resources.</p>"},{"location":"cmssw/available_machines/#usage-for-ml-studies","title":"Usage for ML studies","text":"<p>If you need to use the machine for training DNNs you could accidentally occupy all the GPUs, making them unavailable for other users.</p> <p>For this reason you're kindly asked to use</p> <p><code>import setGPU</code></p> <p>before any import that will use a GPU (e.g. tensorflow). This will assign to you the least loaded GPU on the system.</p> <p>It is strictly forbidden to use GPUs from within your jupyter notebook. Please export your notebook to a python program and execute it. The access to the machine will be revoked when failing to comply to this rule.</p>"},{"location":"cmssw/build/","title":"Building the code","text":"<p>Info</p> <p>See also scram.</p>"},{"location":"cmssw/build/#normal-build","title":"Normal build","text":"<pre><code>scram b -j `nproc`\n</code></pre>"},{"location":"cmssw/build/#building-for-debugging","title":"Building for debugging","text":"<p>Whether you prefer debugging with extra verbose messages or by using <code>gdb</code>, those options will help you add extra parameters to <code>scram</code>:</p> <ul> <li>For enabling <code>LogDebug</code> messages, run <code>export USER_CXXFLAGS=\"-DEDM_ML_DEBUG\"</code> before running the <code>scram</code> command.</li> <li>For also defining the <code>GPU_DEBUG</code> flag globally (for GPU code), run <code>export USER_CXXFLAGS=\"-DGPU_DEBUG -DEDM_ML_DEBUG\"</code> before running the <code>scram</code> command.</li> <li>For running the code with <code>gdb</code>, you will need to run <code>export USER_CXXFLAGS=\"-O0 -g\"</code> before building.</li> <li>If your debug build is not working, you might need to clean your development area: <pre><code>scram b clean\n</code></pre></li> </ul>"},{"location":"cmssw/build/#rebuilding","title":"Rebuilding","text":"<p>If you built CMSSW, changed a file and rebuilt it, some cached object files may still be there. Usually, this should not be a problem, and you should only need to re-run <code>scram b</code> after changing any file.</p> <p>If you want to be 100% certain that no cached files are used, run <code>scram b clean</code> before re-running <code>scram b</code>.</p>"},{"location":"cmssw/build/#multiple-jobs","title":"Multiple jobs","text":"<p>Note that running <code>scram b</code> with multiple jobs launches multiple threads on multiple source files. The compilation order will not be predictable, and the complilation messages will not be predictable.</p>"},{"location":"cmssw/debugging/","title":"Debugging CMSSW","text":"<p>First, build CMSSW with the <code>-O0 -g</code> flags (see Building).</p> <p>Then, assuming you already have a configuration file to run via <code>cmsRun</code>, follow the instructions here.</p>"},{"location":"cmssw/debugging/#quick-reference","title":"Quick reference","text":""},{"location":"cmssw/debugging/#set-breakpoints","title":"Set Breakpoints","text":""},{"location":"cmssw/debugging/#by-file-line","title":"By file line","text":"<pre><code>break '/absolute/path/to/file.c':LINE\n</code></pre> <p>E.g.:</p> <pre><code>break '/data/user/dpapagia/cmssw/CMSSW_13_0_X_2023-01-09-1100/src/RecoPixelVertexing/PixelTriplets/plugins/CAHitNtupletGeneratorOnGPU.cc':293\n</code></pre> <p>You should see a response like this:</p> <pre><code>Make breakpoint pending on future shared library load? (y or [n]) y\nBreakpoint 3 ('/data/user/dpapagia/cmssw/CMSSW_13_0_X_2023-01-09-1100/src/RecoPixelVertexing/PixelTriplets/plugins/CAHitNtupletGeneratorOnGPU.cc':293) pending.\n</code></pre>"},{"location":"cmssw/debugging/#by-function-name","title":"By function name","text":"<p>E.g.:</p> <pre><code>break TrackingRecHitSoADevice&lt;pixelTopology::Phase1&gt;::TrackingRecHitSoADevice\n</code></pre>"},{"location":"cmssw/debugging/#print-the-type-of-a-variable","title":"Print the type of a variable","text":"<pre><code>ptype &lt;name of variable in scope&gt;\n</code></pre>"},{"location":"cmssw/debugging/#print-contents-of-variable","title":"Print contents of variable","text":"<p>After hitting a breakpoint, you can do something like:</p> <pre><code>p &lt;name of variable in scope&gt;\n</code></pre> <p>You will see a response like this:</p> <pre><code>$4 = {&lt;cms::cuda::PortableHostCollection&lt;TrackingRecHitSoA&lt;pixelTopology::Phase1&gt;::TrackingRecHitSoALayout&lt;128, false&gt; &gt;&gt; = {buffer_ = {_M_t = {&lt;std::__uniq_ptr_impl&lt;std::byte, cms::cuda::host::impl::HostDeleter&gt;&gt; = {\n          _M_t = {&lt;std::_Tuple_impl&lt;0, std::byte*, cms::cuda::host::impl::HostDeleter&gt;&gt; = {&lt;std::_Tuple_impl&lt;1, cms::cuda::host::impl::HostDeleter&gt;&gt; = {&lt;std::_Head_base&lt;1, cms::cuda::host::impl::HostDeleter, false&gt;&gt; = {_M_head_impl = {\n                    type_ = cms::cuda::host::impl::MemoryType::kDefault}}, &lt;No data fields&gt;}, &lt;std::_Head_base&lt;0, std::byte*, false&gt;&gt; = {_M_head_impl = 0x7ffef978bc80}, &lt;No data fields&gt;}, &lt;No data fields&gt;}}, &lt;No data fields&gt;}}, layout_ = {static alignment = 128, \n      static alignmentEnforcement = false, static conditionalAlignment = 0, mem_ = 0x7ffef978bc80, elements_ = 8, scalar_ = 1, byteSize_ = 23808, xLocal_ = 0x7ffef978bc80, yLocal_ = 0x7ffef978bd00, xerrLocal_ = 0x7ffef978bd80, yerrLocal_ = 0x7ffef978be00, \n      xGlobal_ = 0x7ffef978be80, yGlobal_ = 0x7ffef978bf00, zGlobal_ = 0x7ffef978bf80, rGlobal_ = 0x7ffef978c000, iphi_ = 0x7ffef978c080, chargeAndStatus_ = 0x7ffef978c100, clusterSizeX_ = 0x7ffef978c180, clusterSizeY_ = 0x7ffef978c200, detectorIndex_ = 0x7ffef978c280, \n      nHits_ = 0x7ffef978c300, offsetBPIX2_ = 0x7ffef978c380, phiBinnerStorage_ = 0x7ffef978c400, hitsModuleStart_ = 0x7ffef978c480, hitsLayerStart_ = 0x7ffef978e200, cpeParams_ = 0x7ffef978e280, averageGeometry_ = 0x7ffef978e300, phiBinner_ = 0x7ffef978f100}, \n    view_ = {&lt;TrackingRecHitSoA&lt;pixelTopology::Phase1&gt;::TrackingRecHitSoALayout&lt;128, false&gt;::ConstViewTemplateFreeParams&lt;128, false, true, false&gt;&gt; = {static alignment = 128, static alignmentEnforcement = false, static conditionalAlignment = 0, static restrictQualify = true, \n        elements_ = 8, xLocalParameters_ = {static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978bc80}, yLocalParameters_ = {static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978bd00}, xerrLocalParameters_ = {\n          static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978bd80}, yerrLocalParameters_ = {static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978be00}, xGlobalParameters_ = {static columnType = cms::soa::SoAColumnType::column, \n          addr_ = 0x7ffef978be80}, yGlobalParameters_ = {static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978bf00}, zGlobalParameters_ = {static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978bf80}, rGlobalParameters_ = {\n          static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978c000}, iphiParameters_ = {static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978c080}, chargeAndStatusParameters_ = {static columnType = cms::soa::SoAColumnType::column, \n          addr_ = 0x7ffef978c100}, clusterSizeXParameters_ = {static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978c180}, clusterSizeYParameters_ = {static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978c200}, detectorIndexParameters_ = {\n          static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978c280}, nHitsParameters_ = {static columnType = cms::soa::SoAColumnType::scalar, addr_ = 0x7ffef978c300}, offsetBPIX2Parameters_ = {static columnType = cms::soa::SoAColumnType::scalar, \n          addr_ = 0x7ffef978c380}, phiBinnerStorageParameters_ = {static columnType = cms::soa::SoAColumnType::column, addr_ = 0x7ffef978c400}, hitsModuleStartParameters_ = {static columnType = cms::soa::SoAColumnType::scalar, addr_ = 0x7ffef978c480}, hitsLayerStartParameters_ = {\n          static columnType = cms::soa::SoAColumnType::scalar, addr_ = 0x7ffef978e200}, cpeParamsParameters_ = {static columnType = cms::soa::SoAColumnType::scalar, addr_ = 0x7ffef978e280}, averageGeometryParameters_ = {static columnType = cms::soa::SoAColumnType::scalar, \n          addr_ = 0x7ffef978e300}, phiBinnerParameters_ = {static columnType = cms::soa::SoAColumnType::scalar, addr_ = 0x7ffef978f100}}, static alignment = 128, static alignmentEnforcement = false, static conditionalAlignment = 0, static restrictQualify = true}}, \n  nHits_ = 3476513472, cpeParams_ = 0x7fffe3d4e2e0 &lt;cms::cuda::getEventCache()::cache&gt;, offsetBPIX2_ = 3822379744, phiBinnerStorage_ = 0x7fffffff0090}\n</code></pre>"},{"location":"cmssw/faq/","title":"CMSSW FAQ","text":""},{"location":"cmssw/faq/#cmssw-tools","title":"CMSSW Tools","text":""},{"location":"cmssw/faq/#fatal-unable-to-find-remote-helper-for-https-errors","title":"<code>fatal: unable to find remote helper for 'https'</code> errors","text":"<p>If you keep getting <code>fatal: unable to find remote helper for 'https'</code> errors when running <code>git</code> commands, it's most probably due to an unsuccessful <code>cmsenv</code> execution (e.g. running <code>cmsenv</code> in an more than two weeks old <code>CMSSW_*_*_X</code>version).  Just log out and login again.</p>"},{"location":"cmssw/faq/#debugging-build-tools-such-as-edmwriteconfigs","title":"Debugging build tools (such as <code>edmWriteConfigs</code>)","text":"<p>Tools used during <code>scram b</code>, such as <code>edmWriteConfigs</code>, can also be edited by the user and can, therefore, be debugged. </p> <p>If the source code of the tool has not been checked out by the user, a precompiled one will be used. If the user has checked out the package containing the tool and builds CMSSW, the newly compiled tool will be used instead.</p> <p>For example, if <code>edmWriteConfigs</code> hangs during <code>scram b</code> (resulting in <code>scram b</code> never finishing, with the message <code>@@@@ Running edmWriteConfigs for RecoPixelVertexingPixelTripletsPlugins</code> being the last one printed), you could:</p> <ul> <li><code>git cms-addpkg FWCore/ParameterSet</code></li> <li>Edit the <code>FWCore/ParameterSet/bin/edmWriteConfigs.cpp</code> file to print stuff you want to check.</li> <li><code>scram b</code> as usual.</li> </ul>"},{"location":"cmssw/introduction/","title":"Introduction to CMSSW","text":"<p>CMSSW is a monster which contains a multitude of algorithms and tools, all in one repository.</p> <p>Looking around for information on its structure and use you can very easily get lost, frustrated, mad even, as most information is either too cryptic, outdated or both. This document will try to focus on the few source files that are of interest to the TkDPG group.</p> <p>A usual split of the codebase is based on the hardware the sowftware is targeted:</p> <ul> <li>CPU code targets CPU-only execution.</li> <li>GPU code targets execution on NVIDIA GPU-capable systems.</li> <li>Alpaka code (under development) targets all Alpaka-supported back-ends.</li> </ul>"},{"location":"cmssw/introduction/#usage","title":"Usage","text":"<p>The usual way to work with CMSSW is the following (from within one of the available machines of your choice):</p> <ul> <li>Connect to an LXPLUS (CPU only) or P5 (with GPUs) machine.</li> <li>List the available CMSSW versions.</li> <li>Clone one of the available versions of CMSSW.  See Releases for more information on the different releases.</li> <li>Checkout packages that you want to make changes to.</li> <li>Make changes to any file you want.</li> <li>Build CMSSW again.</li> <li>Execute Workflows in order to do validation.</li> <li>Measure your code's throughput.</li> <li>Open a PR to cmssw.</li> </ul> <p>To do so, it is recommended that you work with CMSSW on an appropriately configured machine, where many  useful tools for managing, code-checking, code-formatting, building are available.</p> <p>See also: Setup.</p>"},{"location":"cmssw/introduction/#versions","title":"Versions","text":"<p>CMSSW is regularly released and built, both from stable and and unstable version. More information can be found in the Releases section.</p>"},{"location":"cmssw/introduction/#cpu-code","title":"CPU code","text":"<p>For Pixel Track Reconstruction, the main functionality of interest is located in the following CMSSW subdirectories:</p> <ul> <li><code>DataFormats/SiPixelDigi/</code></li> <li><code>DataFormats/SiPixelCluster/</code></li> <li><code>RecoLocalTracker/SiPixelClusterizer/</code></li> </ul> <p>To execute the CPU reconstruction code, you can do so by running it on any LXPLUS machine, usually through Workflows.</p> <p>Information about the CPU code is found in the CPU section.</p>"},{"location":"cmssw/introduction/#gpu-code","title":"GPU code","text":"<p>To speedup CPU code execution, a GPU version of (more or less) the same code has been implemented in CUDA. </p> <p>Note</p> <p>If you're unfamiliar with GPU programming with CUDA, you are encouraged to follow Angela's tutorial. Apart from a general primer to CUDA code, it also provides insights on the GPU code  used in CMSSW.</p> <p>Note</p> <p>Throughout this document, keep in mind that there is no one-to-one mapping  of the CPU functions to CUDA kernels, not even structure-wise.</p> <p>For Pixel Tracks, the main functionality of interest is located in the following CMSSW subdirectories:</p> <ul> <li><code>CUDADataFormats/SiPixelDigi/</code></li> <li><code>CUDADataFormats/SiPixelCluster/</code></li> <li><code>CUDADataFormats/TrackingRecHit/</code></li> <li><code>CUDADataFormats/Track/</code></li> <li><code>CUDADataFormats/Vertex/</code></li> <li><code>RecoLocalTracker/SiPixelClusterizer/</code></li> <li><code>RecoLocalTracker/SiPixelClusterizer/</code></li> <li><code>RecoLocalTracker/SiPixelRecHits/</code></li> <li><code>RecoTracker/TkSeedGenerator/</code></li> <li><code>RecoPixelVertexing/PixelTriplets</code></li> <li><code>RecoPixelVertexing/PixelTrackFitting/</code></li> <li><code>RecoPixelVertexing/PixelVertexFinding/</code></li> </ul> <p>To run and execute code on GPUs, you must first connect to the appropriate LXPLUS machine. See the here  for instructions.</p> <p>Information about the GPU code is found in the GPU section.</p>"},{"location":"cmssw/introduction/#alpaka-code","title":"Alpaka code","text":"<p>Todo</p> <p>Add links, info</p> <p>Warning</p> <p>Possibly inaccurate links/description</p> <p>An ongoing effort by the Patatrack team aims to port reconstruction code to the   Alpaka framework<sup>1</sup><sup>2</sup>, a framework which aims to abstract away from the hardware that a piece of code runs on (CPU/GPU), so that a single source file can run on any of the supported hardware.</p> <p>Alpaka resembles CUDA code, and has (more or less) the same logic with frameworks like OpenCL.</p> <ol> <li> <p>Alpaka's readthedocs \u21a9</p> </li> <li> <p>Alapaka group's homepage \u21a9</p> </li> </ol>"},{"location":"cmssw/proposing-changes/","title":"Creating a PR to CMSSW","text":"<p>Based on http://cms-sw.github.io/tutorial.html</p>"},{"location":"cmssw/proposing-changes/#conflict-after-pr","title":"Conflict after PR","text":"<p>To resolve a conflict that appeared after proposing the changes in a PR, one should prefer rebase to merge as it keeps the commit history clean.</p> <p>For a rebase, do:</p> <pre><code>git checkout &lt;development_branch_name&gt;\ngit fetch -a &lt;remote name&gt;\ngit rebase -i &lt;CMSSW_current_release&gt;\n</code></pre> <p>It might be, that the tags for branches are only considered for your default remote, eg. <code>my-cmssw</code>, but the current release of CMSSW is not included in that. To resolve this, you can also try specifying the remote for the official cmssw (which is not a fork):</p> <pre><code>git rebase -i &lt;official cmssw remote&gt;/&lt;CMSSW_current_release&gt;\n</code></pre> <p>See aso this quick recipe which may take care of conflicts, and can help you squash your commits.</p>"},{"location":"cmssw/proposing-changes/#build-release","title":"Build release","text":"<p>See Building for more info.</p>"},{"location":"cmssw/proposing-changes/#before-making-a-pr","title":"Before making a PR","text":"<pre><code>scram b code-format # run clang-format\nscram b code-checks # run clang-tidy\n</code></pre>"},{"location":"cmssw/releases/","title":"Releases","text":"<p>CMSSW has multiple releases, information about which one can find here. </p> <p>Builds can be categorized either as Stable/Pre-releases or Integration Builds (IB).</p>"},{"location":"cmssw/releases/#stable-and-pre-releases","title":"Stable and pre-releases","text":"<p>Find out what's new in a particular release: https://cmssdt.cern.ch/SDT/ReleaseNotes/index.html.</p> <p>For example, for release <code>CMSSW_12_3_0_pre2</code>:</p> <p>Quote</p> <p>Changes since CMSSW_12_3_0_pre1:</p> <p>compare to previous</p> <p>36515 from @cms-tsg-storm: Revert EI removal in TSG tests <code>hlt</code> created: 2021-12-16 07:48:39 merged: 2021-12-16 09:48:40</p> <p>36506 from @bsunanda: Run3-TB64 Make some simple changes to the classes in SimG4CMS/HcalTestBeam <code>simulation</code> created: 2021-12-15 15:29:55 merged: 2021-12-16 07:05:18</p> <p>36505 from @kpedro88: restrict number of events for 0T PU workflows <code>pdmv</code> <code>upgrade</code> created: 2021-12-15 10:28:32 merged: 2021-12-15 14:50:12</p> <p>etc.</p> <p>https://cmssdt.cern.ch/SDT/ReleaseNotes/CMSSW_12/CMSSW_12_3_0_pre2.html</p>"},{"location":"cmssw/releases/#integration-builds","title":"Integration builds","text":"<p>Integration builds are automatically built on a schedule.</p> <p>Browse available integration builds, profiling and other test results: https://cmssdt.cern.ch/SDT/html/cmssdt-ib/.</p> <p>Important features include the ability to filter <code>IBs</code> based on</p> <ul> <li>Flavor</li> <li>OS</li> <li>CPU architecture</li> <li>Compiler</li> </ul> <p></p> <p>Look at latest commits, PRs added for a particluar build</p> <p></p>"},{"location":"cmssw/setup/","title":"Setup and Usage","text":""},{"location":"cmssw/setup/#cloning-a-cmssw-release-on-your-machine","title":"Cloning a CMSSW release on your machine","text":""},{"location":"cmssw/setup/#search-for-available-releases","title":"Search for available releases","text":"<p>Search for available releases:</p> <pre><code>scram list CMSSW\n</code></pre> <p>This should result in some output like:</p> <pre><code>Listing installed projects available for platform &gt;&gt; slc7_amd64_gcc10 &lt;&lt;\n\n--------------------------------------------------------------------------------\n| Project Name  | Project Version          | Project Location                  |\n--------------------------------------------------------------------------------\n\n  CMSSW           CMSSW_12_0_0_pre3          \n                                         --&gt; /cvmfs/cms.cern.ch/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_0_0_pre3\n  CMSSW           CMSSW_12_0_0_pre4          \n                                         --&gt; /cvmfs/cms.cern.ch/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_0_0_pre4\n  CMSSW           CMSSW_12_0_0_pre5          \n                                         --&gt; /cvmfs/cms.cern.ch/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_0_0_pre5\n...\n  CMSSW           CMSSW_12_3_DBG_X_2022-02-10-2300  \n                                         --&gt; /cvmfs/cms-ib.cern.ch/week1/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_3_DBG_X_2022-02-10-2300\n  CMSSW           CMSSW_12_3_SKYLAKEAVX512_X_2022-02-10-2300  \n                                         --&gt; /cvmfs/cms-ib.cern.ch/week1/slc7_amd64_gcc10/cms/cmssw/CMSSW_12_3_SKYLAKEAVX512_X_2022-02-10-2300\n  CMSSW           CMSSW_12_3_X_2022-02-11-1100  \n                                         --&gt; /cvmfs/cms-ib.cern.ch/week1/slc7_amd64_gcc10/cms/cmssw-patch/CMSSW_12_3_X_2022-02-11-1100\n</code></pre>"},{"location":"cmssw/setup/#create-a-cmssw-area","title":"Create a CMSSW area","text":"<p>Once you have decided on a CMSSW release, set up the work area:</p> <pre><code>cmsrel CMSSW_12_3_X_2022-02-11-1100\ncd CMSSW_12_3_X_2022-02-11-1100/src\ncmsenv\n</code></pre> <p>These commands clone a specific release of CMSSW, then activate the CMSSW environment, where the paths of most of the CMSSW tools are made available.</p>"},{"location":"cmssw/setup/#initialize-git-area","title":"Initialize git area","text":"<pre><code>git cms-init --upstream-only\n</code></pre>"},{"location":"cmssw/setup/#checkout-a-few-packages-using-git-cms-addpkg","title":"Checkout a few packages using <code>git cms-addpkg</code>","text":"<p>Some useful packages for developing the pixel local reconstruction in CUDA:</p> <pre><code>CUDADataFormats/SiPixelCluster/\nCUDADataFormats/SiPixelDigi/\nRecoLocalTracker/SiPixelClusterizer/\n</code></pre> <p>You can add these packages with cms-addpkg:</p> <p>(You need to be in the <code>CMSSW_12_3_X_2022-02-11-1100/src</code> repository)</p> <pre><code>git cms-addpkg CUDADataFormats/SiPixelCluster/\ngit cms-addpkg CUDADataFormats/SiPixelDigi/\ngit cms-addpkg DataFormats/SiPixelCluster/\ngit cms-addpkg DataFormats/SiPixelDigi/\ngit cms-addpkg RecoLocalTracker/SiPixelClusterizer/\n</code></pre> <p>See the checked out packages in <code>.git/info/sparse-checkout</code></p> Output of <code>cat .git/info/sparse-checkout</code> <pre><code>/.clang-format\n/.clang-tidy\n/.gitignore\n/CUDADataFormats/SiPixelCluster/\n/CUDADataFormats/SiPixelDigi/\n/RecoLocalTracker/SiPixelClusterizer/\n</code></pre> <p>Only build the desired packages, add/remove unwanted ones: http://cms-sw.github.io/git-cms-addpkg.html</p> <p>Fetch updates from remote <pre><code>git checkout master\ngit fetch [official-cmssw]\ngit merge official-cmssw/master master\n</code></pre></p> <p>Where <code>official-cmssw</code> is the remote name configured for the CMSSW offline software repository.</p> <p>You can list your remote repositories with (for even more verbose output, you can stack the <code>-v</code>s, e.g. <code>-vvv</code>):</p> <pre><code>git remote -v\n</code></pre>"},{"location":"cmssw/setup/#running-checks-on-the-code","title":"Running checks on the code","text":"<pre><code>scram b code-checks\n</code></pre>"},{"location":"cmssw/setup/#formatting-the-code","title":"Formatting the code","text":"<pre><code>scram b code-format\n</code></pre>"},{"location":"cmssw/throughput/","title":"Throughput calculation","text":"<p>TODO</p>"},{"location":"cmssw/tools/","title":"Tools","text":"<p>It all starts with <code>cmsRun</code> which is the main entry point to all CMSSW code. Configuring it can be a pain so, for this reason, <code>cmsDriver</code> was built. Still, configuring <code>cmsDriver</code> can also be tedious for pipelines that require multiple steps of data fetching, configuring, simulating etc., so <code>runTheMatrix.py</code> was developed.</p> <p>Those tools are available on LXPLUS machines, after creating a work area and activating a CMSSW environment by <code>cmsenv</code>.</p>"},{"location":"cmssw/tools/#scram","title":"<code>scram</code>","text":"<p>A software build and maintenance tool written in Python. It calls <code>gmake</code> under the hood.</p> <p>It can <code>clean</code>, build, check and format the code.</p> <p>Documentation on <code>scram</code> can be found here.</p>"},{"location":"cmssw/tools/#extra-parameters","title":"Extra Parameters","text":"<ul> <li>Scram also accepts extra flags which are passed directly to <code>gmake</code> such as <code>--dry-run</code>.</li> <li>The <code>USER_CXXFLAGS</code> is an environmental variable which is used to pass arguments to <code>g++</code>.</li> </ul> <p>See Building for more information and options.</p>"},{"location":"cmssw/tools/#cmsrun","title":"<code>cmsRun</code>","text":"<p>Todo</p> <p>TODO</p> <p>The main entrypoint to execute CMSSW code. Probably never run by hand, it requires complicated configuration files  to execute which are generated by other tools, described below.</p>"},{"location":"cmssw/tools/#cmsdriverpy","title":"<code>cmsDriver.py</code>","text":"<p>Todo</p> <p>TODO</p> <p>A Python tool to create <code>cmsRun</code> configuration files, which are in the form of Python3 code.</p>"},{"location":"cmssw/tools/#resources","title":"Resources","text":"<ol> <li>Source code</li> <li>Twiki page</li> </ol>"},{"location":"cmssw/tools/#runthematrixpy","title":"<code>runTheMatrix.py</code>","text":"<p>A wrapper script which is responsible of executing multi-step analysis/simulation recipes by configuring <code>cmsDriver</code>.</p> <p>It is examined in more detail in the Workflows section</p>"},{"location":"cmssw/validation/","title":"Validation","text":"<p>This section will describe the basic steps one needs to do in order to validate that their code, apart from syntactically correct, also produces correct results.</p> <p>This usually boils down to:</p> <ul> <li>Executing specific workflows. The specific workflow to execute depends on the code you changed. See Workflow numbers below.</li> <li>Comparing the results from an execution of a unchanged CMSSW version.</li> </ul>"},{"location":"cmssw/validation/#procedure","title":"Procedure","text":"<ul> <li>Checkout to an unchanged CMSSW version you want to compare results against.</li> <li>Build it.</li> <li>Execute the appropriate workflow.</li> <li>Create the validation plots: <pre><code>harvestTrackValidationPlots.py step3_inDQM*.root -o file_base.root\n</code></pre> The <code>file_base.root</code> which will be produced is going to be used later.</li> <li>Execute the appropriate workflow after building your code.</li> <li>Checkout to the code where you've made your changes.</li> <li><code>scram b clean</code> and re-build the code.</li> <li>Execute the same appropriate workflow.</li> <li>Create the validation plots: <pre><code>harvestTrackValidationPlots.py step3_inDQM*.root -o file_changes.root\n</code></pre></li> <li>Make the validation plots: <pre><code>makeTrackValidationPlots.py file_base.root file_changes.root\n</code></pre></li> </ul>"},{"location":"cmssw/validation/#workflow-numbers-for-validation","title":"Workflow numbers for validation","text":"<p>Workflows to execute for validation.</p>"},{"location":"cmssw/validation/#pixeltrack","title":"PixelTrack","text":"<ul> <li>11634.502</li> <li>11834.502</li> <li>11604.0</li> <li>11609.0</li> </ul>"},{"location":"cmssw/pixel-local/overview/","title":"Overview","text":"<p>\"Pixel-local reconstruction\" refers to the chain of CMSSW modules which are reponsible for converting raw CMS detector Pixel data to Vertices.</p> <pre><code>graph LR\n  A[Raw data] --&gt; B[Digis];\n  B --&gt; C[Clusters]\n  C --&gt; D[RecHits]\n  D --&gt; E[Tracks]\n  E --&gt; F[Vertices]</code></pre>"},{"location":"cmssw/pixel-local/pixel/","title":"CPU/GPU data flow","text":"<pre><code>flowchart TD\n classDef Legacy fill:#ffffff,stroke:#fc1702,stroke-width:4px,color:#fc1702;\n classDef SoA fill:#0000ff,stroke:none,color:#ffffff;\n linkStyle default stroke-width:4px,fill:none,stroke:orange;\n\n subgraph &lt;strong&gt;GPU workflow&lt;/strong&gt;\n  rd_g[Raw Data] --&gt; d[Digis]\n  d --&gt; c[Clusters]\n  c --&gt; h_soa[\"Hits (SoA)\"]\n  h_soa --&gt; db[Doublets]\n  db --&gt; nt[n-tuplets]\n  nt --&gt; pt[Pixel Tracks]\n  pt --&gt; pv[Pixel Vertices]\n end \n\n class rd_g Legacy;\n class d,c,h_soa,db,nt,pt,pv SoA;\n\n subgraph &lt;strong&gt;Products&lt;/strong&gt;\n  direction TB\n  rd_p[Raw Data] --&gt; d_psoa[\"Digis (Partial SoA)\"]\n  d_psoa --&gt; h_psoa[\"Hits (Partial SoA)\"]\n  h_psoa --&gt; dc_l[\"Digis &amp; Clusters (Legacy)\"] --&gt; h_l[\"Hits (Legacy)\"]\n  h_l --&gt; pt_soa_prod[\"Pixel Tracks (SoA)\"]\n  pt_soa_prod --&gt; pv_soa_prod[\"Pixel Vertices (SoA)\"]\n  pv_soa_prod --&gt; pt_l[\"Pixel Tracks (Legacy)\"]\n  pt_l --&gt; pv_l[\"Pixel Vertices (Legacy)\"]\n end\n\n linkStyle 7,8,9,10,11,12,13,14 stroke-width:0px,fill:none,stroke:none; \n\n rd_p --&gt; rd_g\n d --&gt; d_psoa\n h_soa --&gt; h_psoa\n pt --&gt; pt_soa_prod\n pv --&gt; pv_soa_prod\n linkStyle 15,16,17,18,19 stroke-width:4px,fill:none,stroke:green;\n\n class rd_p,dc_l,h_l,pt_l,pv_l Legacy;\n class d_psoa,h_psoa,pt_soa_prod,pv_soa_prod SoA;\n\n subgraph &lt;strong&gt;CPU workflow&lt;/strong&gt;\n  rd_c[Raw Data] --&gt; d_l[\"Digis (Legacy)\"]\n  d_l --&gt; c_l[\"Clusters (Legacy)\"]\n  c_l --&gt; h_soa_c[\"Hits (SoA)\"]\n  h_soa_c --&gt; db_c[Doublets]\n  db_c --&gt; nt_c[n-tuplets]\n  nt_c --&gt; pt_soa[\"Pixel Tracks (SoA)\"]\n  pt_soa --&gt; pv_soa[\"Pixel Vertices (SoA)\"]\n end\n\n class rd_c,d_l,c_l Legacy;\n class h_soa_c,db_c,nt_c,pt_soa,pv_soa SoA;\n\n h_soa_c --&gt; h_l\n pt_soa --&gt; pt_l\n pv_soa --&gt; pv_l\n linkStyle 27,28,29 stroke-width:4px,fill:none,stroke:red; </code></pre>"},{"location":"cmssw/pixel-local/cpu/","title":"CPU code overview","text":"<p>Todo</p> <p>TODO</p> <p>Reconstruction modules targeted to run on CPUs.</p> <p>Functionality covered in this documentation:</p> <ul> <li>Raw to Digi conversion (TODO)</li> <li>Clusterization</li> </ul>"},{"location":"cmssw/pixel-local/cpu/DetSetVector-overview/","title":"DetSetVector","text":"<p>File on github.</p> <p>Defined in the <code>edm</code> namespace, this is a template which creates a vector which groups data per module it belongs to. It works as an iterator, with each \"row\" being a vector of type <code>T</code>. Each \"row\" is, therefore, associated with a module.</p> <p>An example is a <code>DetSetVector</code> of <code>PixelDigi</code>s (see here).</p> <p>Warning</p> <p>Not to be confused with an <code>edmNew::DetSetVector</code>, a.k.a <code>DetSetVectorNew</code>.</p>"},{"location":"cmssw/pixel-local/cpu/PixelDigi-overview/","title":"PixelDigi overview","text":"<p>A <code>PixelDigi</code> represents an actual pixel of the Pixel Detector in software. It's a class that encapsulates:</p> <ul> <li>The pixel's coordinates on the detector (row, column)</li> <li>The pixel's ADC value (i.e. the actual sensor value)</li> <li>The flag ???? TODO</li> </ul> <p>and contains all this information in a packed format.</p> <p>It is located in <code>DataFormats/SiPixelDigi/interface</code>.</p> <p>File on github.</p>"},{"location":"cmssw/pixel-local/cpu/PixelDigi-overview/#uml-diagram","title":"UML Diagram","text":"<pre><code>    classDiagram\n        class PixelDigi{\n            +PackedDigiType theData\n            +row(): int\n            +col(): int\n            +adc(): unsigned short\n            +flag(): int\n            +packedData(): PackedDigiType\n        }</code></pre>"},{"location":"cmssw/pixel-local/cpu/PixelDigi-overview/#class-attributes","title":"Class attributes","text":""},{"location":"cmssw/pixel-local/cpu/PixelDigi-overview/#thedata","title":"<code>theData</code>","text":"<p>A single attribute of <code>PackedDigiType</code> which aims to contain all the required information in the <code>PixelDigi</code> instance:</p> <ul> <li><code>row</code></li> <li><code>col</code></li> <li><code>adc</code> value</li> <li><code>flag</code></li> </ul>"},{"location":"cmssw/pixel-local/cpu/PixelDigi-overview/#class-functions","title":"Class functions","text":""},{"location":"cmssw/pixel-local/cpu/PixelDigi-overview/#row-accessor","title":"<code>row</code> (accessor)","text":"<p>The row that the Digi belongs to in the Pixel Detector. Extracted from the <code>theData</code>.</p>"},{"location":"cmssw/pixel-local/cpu/PixelDigi-overview/#column-accessor","title":"<code>column</code> (accessor)","text":"<p>The col that the Digi belongs to in the Pixel Detector. Extracted from the <code>theData</code>.</p>"},{"location":"cmssw/pixel-local/cpu/PixelDigi-overview/#flag-accessor","title":"<code>flag</code> (accessor)","text":"<p>Extracts <code>flag</code> from the <code>theData</code>.</p>"},{"location":"cmssw/pixel-local/cpu/PixelDigi-overview/#adc-accessor","title":"<code>adc</code> (accessor)","text":"<p>Extracts <code>adc</code> from the <code>theData</code>.</p>"},{"location":"cmssw/pixel-local/cpu/PixelDigi-overview/#packeddata-accessor","title":"<code>packedData</code> (accessor)","text":"<p>Returns the <code>theData</code> attribute.</p>"},{"location":"cmssw/pixel-local/cpu/PixelThresholdClusterizer-copytobuffer/","title":"copy_to_buffer","text":"<p>A class function that does more that its name suggests.</p>"},{"location":"cmssw/pixel-local/cpu/PixelThresholdClusterizer-copytobuffer/#algorithm","title":"Algorithm","text":"<ul> <li>Given a linked list of <code>DigiIterator</code>s:</li> <li>For each <code>DigiIterator</code> it calibrates the ADC value to electron counts.</li> <li>For each <code>DigiIterator</code>:<ul> <li>Checks if its <code>adc</code> value is above <code>thePixelThreshold</code> and, if it is:<ul> <li>Updates the class attribute <code>theBuffer</code> (the actual Pixel matrix)  with the pixels ADC value.</li> <li>If the <code>adc</code> value is also above <code>theSeedThreshold</code>, the position  (row &amp; col) of the <code>DigiIterator</code> is stored in <code>theSeeds</code> array.</li> <li>If this specific Digi has been recorded more than once (i.e. the row/col combination has already been encountered in a previous <code>DigiIterator</code>),  the Digi is removed from <code>theSeeds</code> array and the ADC value of the corresponding  place (coordinates) is set to 0.</li> </ul> </li> </ul> </li> </ul>"},{"location":"cmssw/pixel-local/cpu/PixelThresholdClusterizer-overview/","title":"PixelThresholdClusterizer overview","text":"<p>Located in <code>RecoLocalTracker/SiPixelClusterizer/plugins/</code> in CMSSW, these files (<code>PixelThresholdClusterizer.h</code> and <code>.cc</code>) provide the declaration of the  <code>PixelThresholdClusterizer</code> class.</p> <p>It inherits from <code>PixelClusterizerBase</code> (declared in the same directory, in <code>PixelClusterizerBase.h</code>).</p> <p>The class' main purpose is to create Clusters, given <code>PixelDigi</code>s.</p> <p>Other functionality has been integrated in it, such as:</p> <ul> <li>Duplicate Digi removal</li> </ul>"},{"location":"cmssw/pixel-local/cpu/PixelThresholdClusterizer-overview/#uml-diagram","title":"UML diagram","text":"<pre><code>classDiagram\n\nclass PixelClusterizerBase\n\nclass PixelThresholdClusterizer{\n    +SiPixelArrayBuffer theBuffer\n    +vector~PixelPos~ theSeeds\n    +vector~bool~ theFakePixels\n    +vector~SiPixelCluster~ theClusters\n}\n\nPixelClusterizerBase &lt;|-- PixelThresholdClusterizer\n\nclass SiPixelArrayBuffer{\n    +int nrows\n    +int ncols\n    +set_adc()\n    +add_adc()\n    +setSize()\n}\n\ndirection LR\nPixelThresholdClusterizer -- \"1\" SiPixelArrayBuffer : theBuffer\n</code></pre>"},{"location":"cmssw/pixel-local/cpu/PixelThresholdClusterizer-overview/#class-attributes","title":"Class attributes","text":""},{"location":"cmssw/pixel-local/cpu/PixelThresholdClusterizer-overview/#thebuffer","title":"<code>theBuffer</code>","text":"<p>A 2D matrix of <code>SiPixelArrayBuffer</code> type. Its purpose is to represent the Pixel Detector and store ADC values for each Pixel.</p>"},{"location":"cmssw/pixel-local/cpu/PixelThresholdClusterizer-overview/#thepixeloccurence","title":"<code>thePixelOccurence</code>","text":"<p>A simple vector of type <code>uint8_t</code> to store the number of occurences that each Digi is encountered. Used to detect duplicates due to noise. </p>"},{"location":"cmssw/pixel-local/cpu/PixelThresholdClusterizer-overview/#thepixelthreshold","title":"<code>thePixelThreshold</code>","text":"<p>A constant ADC threshold above which a pixel is regarded as active. </p>"},{"location":"cmssw/pixel-local/cpu/PixelThresholdClusterizer-overview/#theseedthreshold","title":"<code>theSeedThreshold</code>","text":"<p>A constant ADC threshold above which an active pixel is also considered as a Seed, i.e. a starting point for the Clusterization algorithm.</p>"},{"location":"cmssw/pixel-local/cpu/SiPixelArrayBuffer-overview/","title":"SiPixelArrayBuffer overview","text":"<p>A class representing a 2D matrix holding information on the Pixels of  the Pixel Detector.</p> <p>The <code>theBuffer</code> attribute of <code>PixelThresholdClusterizer</code> is an instance of <code>SiPixelArrayBuffer</code>.</p> <p>It is located in <code>RecoLocalTracker/SiPixelClusterizer/plugins</code>.</p>"},{"location":"cmssw/pixel-local/cpu/SiPixelArrayBuffer-overview/#uml-diagram","title":"UML diagram","text":"<pre><code>classDiagram\n\nclass SiPixelArrayBuffer{\n    +int nrows\n    +int ncols\n    +set_adc()\n    +add_adc()\n    +setSize()\n}</code></pre>"},{"location":"cmssw/pixel-local/cpu/SiPixelCluster-overview/","title":"SiPixelCluster overview","text":"<p>Located in <code>DataFormats/SiPixelCluster/interface</code>, the <code>SiPixelCluster.h</code> contains the declaration of the <code>SiPixelCluster</code> class. </p> <p>From the source file's header:</p> <p>Quote</p> <p>Class to contain and store all the topological information of pixel clusters: charge, global size, size and the barycenter in x and y local directions. It builds a vector of SiPixel (which is an inner class) and a container of channels.</p> <p>In it, three helper subclasses are contained:</p> <ul> <li><code>Pixel</code></li> <li><code>PixelPos</code></li> <li><code>Shift</code></li> </ul>"},{"location":"cmssw/pixel-local/cpu/SiPixelCluster-overview/#uml-diagram","title":"UML diagram","text":"<pre><code>classDiagram\n    class SiPixelCluster{\n        +Pixel\n        +PixelPos\n        +Shift\n    }\n\n    class PixelPos{\n        -int row_\n        -int col_\n        +dx(): int\n        +dy(): int\n    }\n\n    class Shift{\n    -int dx_\n    -int dy_\n    +dx(): int\n    +dy(): int\n    }\n\n    class Pixel{\n        +uint16_t x \n        +uint16_t y\n        +uint16_t adc\n    }   \n\n    SiPixelCluster -- PixelPos : Nested Declaration\n    SiPixelCluster -- Pixel : Nested Declaration    \n    SiPixelCluster -- Shift : Nested Declaration        \n</code></pre>"},{"location":"cmssw/pixel-local/gpu/","title":"GPU code overview","text":"<p>CMSSW modules meant to be run on NVIDIA GPUs.</p> <p>Functionality covered in this documentation:</p> <ul> <li>Raw to Digi conversion</li> <li>Clusterization</li> </ul>"},{"location":"cmssw/pixel-local/gpu/#module-dataflow","title":"Module Dataflow","text":"<pre><code>flowchart TD\n classDef Module fill:#c5c5FF,stroke:#0000ff,stroke-width:4px,color:#0000ff;\n classDef CUDAProduct fill:#ffffff,stroke:#008000,stroke-width:2px,color:#008000; \n SiPixelRawToClusterCUDA --&gt; SiPixelDigisCUDA([SiPixelDigisCUDA])\n SiPixelRawToClusterCUDA --&gt; SiPixelClustersCUDA([SiPixelClustersCUDA])\n\n SiPixelClustersCUDA --&gt; SiPixelRecHitCUDA\n SiPixelDigisCUDA --&gt; SiPixelRecHitCUDA \n SiPixelRecHitCUDA --&gt; TrackingRecHit2DGPU([TrackingRecHit2DGPU])\n\n TrackingRecHit2DGPU --&gt; CAHitNtupletCUDA\n CAHitNtupletCUDA --&gt; PixelTrackHeterogeneous([PixelTrackHeterogeneous])\n PixelTrackHeterogeneous --&gt; PixelVertexProducerCUDA\n PixelVertexProducerCUDA --&gt; ZVertexHeterogeneous([ZVertexHeterogeneous])\n\n subgraph Legend\n  CUDAProduct([CUDA Product])\n  CMSSWModule([CMSSW Module])\n end\n\n class CMSSWModule,SiPixelRawToClusterCUDA,SiPixelRecHitCUDA,CAHitNtupletCUDA,PixelVertexProducerCUDA Module;\n class CUDAProduct,SiPixelClustersCUDA,SiPixelDigisCUDA,TrackingRecHit2DGPU,PixelTrackHeterogeneous,ZVertexHeterogeneous CUDAProduct;  </code></pre>"},{"location":"cmssw/pixel-local/gpu/#data-structure","title":"Data Structure","text":"<p>The SoA approach is used to store pixel data used by the CUDA code.</p> <p>In short, data from each module is concatenated into multiple 1D arrays.</p> <p>For example, suppose we have two modules (module0 and module1). Each one will contain 16 x 80 x 52 = 66560 pixels. Each pixel has its module coordinates (<code>x</code> and <code>y</code>) and the module index it belongs to (<code>moduleInd</code>, for this example <code>0</code> and <code>1</code>).</p> <p>A simplistic visualization of the example above can be seen below; each module is composed of 2 x 8 ROCs, each pixel having unique coordinates relative to the module.</p> <p></p> <p>The SoA approach to store the pixel data would look like the image below; data from all modules are concatenated one after another in 1D arrays:</p> <p></p> <p>Warning</p> <p>The data is not stored in a per-module sorted manner, meaning that <code>module1</code> data could precede <code>module0</code>'s data. Data from each module is, however, stored consecutively, meaning that data from one module is not split up into several blocks.</p> <p>An actual example of such arrays can be seen in the SiPixelDigisCUDASOAView.</p> <p>Todo</p> <p>Where is this data structure created? Unpacking?</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelClustersCUDA/","title":"SiPixelClustersCUDA.h","text":"<p>File on github</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDA/","title":"SiPixelDigisCUDA","text":"<p>Class used to contain pixel data (coordinates, ADC values) using the SoA approach, intended to be used by CUDA code.</p> <p>The actual data is stored in an instance of SiPixelDigisCUDASOAView (<code>m_view</code> attribute), accessed via the <code>view()</code> method.</p> <p>Header on github.</p> <p>Source on github.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDA/#uml-diagram","title":"UML diagram","text":"<pre><code>classDiagram\nclass SiPixelDigisCUDA{\n    -SiPixelDigisCUDASOAView m_view\n    -cms::cuda::device::unique_ptr&lt;uint16_t[]&gt; m_store\n    +view() SiPixelDigisCUDASOAView\n}\n\nclass SiPixelDigisCUDASOAView\n\nSiPixelDigisCUDA -- \"1\" SiPixelDigisCUDASOAView : m_view</code></pre>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDA/#attributes","title":"Attributes","text":""},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDA/#m_store","title":"<code>m_store</code>","text":"<p>Todo</p> <p>This attribute is a pointer to GPU memory. What does it store? </p> <p>Attribute of type <code>cms::cuda::device::unique_ptr&lt;StoreType[]&gt;</code> (from HeterogeneousCore) , where <code>StoreType</code> is <code>uint16_t</code>. This is a pointer to an array stored on GPU memory.</p> <p>It is initialized by default in the <code>SiPixelDigisCUDA</code> constructor, and is then passed on as a parameter to initialize <code>m_view</code>. See the Constructor below for more information.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDA/#m_view","title":"<code>m_view</code>","text":"<p>An instance of SiPixelDigisCUDASOAView. Stores all the Pixel Digi data as SoA.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDA/#methods","title":"Methods","text":""},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDA/#constructor","title":"Constructor","text":"<p>Gets a number of <code>maxFedWords</code> (number of pixels??) and a CUDA stream as inputs.</p> <p>Initializes the <code>m_store</code> and <code>m_view</code> attributes.</p> <p><code>m_store</code> is initialized using the <code>cms::cuda::make_device_unique</code> function (found in HeterogeneousCore), by giving it the <code>maxFedWords</code> multiplied by the <code>kMAX</code> constant found in the <code>StorageLocation</code> enumerator as input.</p> <p>Todo</p> <p>What is the meaning of the number produced by the multiplication above?</p> <p>i.e. <code>kMAX * maxFedWords</code></p> <p>Probably the memory size allocated on the device for storing the <code>SiPixelDigisCUDASOAView</code> instance?</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDA/#view","title":"<code>view</code>","text":"<p>Accesses the data stored in the <code>m_view</code> attribute, which are stored with the SoA approach.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/","title":"SiPixelDigisCUDASOAView","text":"<p>Class which stores Pixel digi data in an SoA approach. Used indirectly by SiPixelDigisCUDA instances, such as the <code>digis_d</code> variable in the SiPixelRawToClusterGPUKernel class.</p> <p>The data is stored on the GPU.</p> <p>File on github.</p> <p>Key attributes of this class:</p> <ul> <li><code>xx_</code>: x-coordinates of pixels (array)</li> <li><code>yy_</code>: y-coordinates of pixels (array)</li> <li><code>adc_</code>: ADC values for each pixel (array)</li> <li><code>moduleInd_</code>: GPU-specific module identifiers that each pixel belongs to (GPU only)</li> <li><code>clus_</code>: The cluster identifier that each pixel is assigned to.</li> <li><code>pdigi_</code>: Packed digi format. Contains coordinates and ADC values in one variable. See also PixelDigi for the CPU counterpart.</li> <li><code>rawIdArr_</code>: Unique identifier used to identify modules in the whole of CMS (TODO: Double-check that this is the same as  <code>DetId</code>).</li> </ul>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#uml-diagram","title":"UML Diagram","text":"<pre><code>classDiagram\n    class SiPixelDigisCUDASOAView{\n        -uint16_t* xx_\n        -uint16_t* yy_      \n        -uint16_t* adc_     \n        -uint16_t* moduleInd_       \n        -int32_t* clus_             \n        -uint32_t* pdigi_\n        -uint32_t* rawIdArr_        \n\n        +xx() uint16_t*\n        +yy() uint16_t*     \n        +adc() uint16_t*        \n        +moduleInd() uint16_t*      \n        +clus() int32_t*        \n        +pdigi() uint32_t*              \n        +rawIdArr() uint32_t*   \n        -roundFor128ByteAlignment(int size) static int\n        -getColumnAddress(column, store, size)\n}</code></pre> <p>Todo</p> <p>Explain how this works</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#contents","title":"Contents","text":""},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#enums","title":"Enums","text":""},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#storagelocation","title":"<code>StorageLocation</code>","text":"<p>TODO</p> <p>Indices used for accessing specific parts of SoA data in GPU memory (?), and to determine the maximum size of the memory  required to be allocated (<code>kMAX</code>).</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#methods","title":"Methods","text":""},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#getcolumnaddress","title":"<code>getColumnAddress</code>","text":"<p>A template method for returning the memory address of column <code>column</code> (typename <code>LocationType</code>)</p> <p>Used by the <code>SiPixelDigisCUDASOAView</code> class constructor to assign a device pointer to the SoA variables (<code>xx_</code>, <code>yy_</code>..).</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#roundfor128bytealignment","title":"<code>roundFor128ByteAlignment</code>","text":"<p>A method for calculating the number of <code>uint16_t</code> elements that must be requested in order to always have 128-byte memory transactions (Could have something to do with this).</p> <p>For example, if we want to transfer only 1 <code>uint16_t</code> variable (usually has a size of 2 bytes), one would have to round this up to the nearest 128-byte transaction, meaning that we should request 64 <code>uint16_t</code> variables (because <code>64 * sizeof(uint16_t) = 128</code>) to be efficient. Similary, if we needed 64 <code>uint16_t</code>, we would still need one 128-byte transaction. However, for 65 <code>uint16_t</code>, we would need 2 x 128-byte transfers.</p> <p>It's used mainly for pointer arithmetic, for calculating the offset that the pointer will have to move to find the start of a specific SoA variable, e.g. the <code>adc_</code> array.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#constructor","title":"Constructor","text":"<p>Todo</p> <p>TODO</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#memory-structure","title":"Memory Structure","text":"<p>As mentioned, efficient GPU global memory transactions must be 32-, 64- or 128-byte aligned, therefore all memory storage and accesses are best done in chuncks of the aforementioned sizes.</p> <p>For this code, 128-byte chunks have been chosen by the developers.</p> <p>When a <code>SiPixelDigisCUDASOAView</code> instance is created, pointers to the start of each SoA variable (<code>clus_</code>, <code>adc_</code> etc) are created. Those are calculated using the start of the memory requested (i.e. the pointer stored in <code>m_store</code>), the constant assigned to each variable in <code>StorageLocation</code> and the number of pixels available (<code>maxFedWords</code>).</p> <p>Note</p> <p>The values in <code>StorageLocation</code> (0, 2, 4, 6, 7, 8, 9) are not random, but are directly related to the data type of the variables.  Note that 32-bit variables (<code>clus_</code>, <code>pdigi_</code> and <code>rawIdArr_</code>) are given  numbers that have double the size of 16-bit variables (<code>adc_</code>, <code>xx_</code>, <code>yy_</code>, <code>moduleInd_</code>). For example, note that the values of <code>StorageLocation::kCLUS</code> and <code>StorageLocation::kPDIGI</code> differ by 2, while the values of <code>StorageLocation::kADC</code> and <code>StorageLocation::kXX</code> differ by 1.</p> <p>Firstly, 32-bit variables are stored in order, i.e.:</p> <ul> <li><code>clus_</code> (<code>int32_t</code>)</li> <li><code>pdigi_</code> (<code>uint32_t</code>)</li> <li><code>rawIdArr_</code> (<code>uint32_t</code>)</li> </ul> <p>Then, 16-bit variables are stored, i.e.:</p> <ul> <li><code>adc_</code> (<code>uint16_t</code>)</li> <li><code>xx_</code> (<code>uint16_t</code>)</li> <li><code>yy_</code> (<code>uint16_t</code>)</li> <li><code>moduleInd_</code> (<code>uint16_t</code>)</li> </ul> <p>As an example, let's assume that we want to store information for 8 pixels (<code>numPixels</code> = <code>maxFedWords</code> = <code>8</code>) on the GPU, using the <code>SiPixelDigisCUDASOAView</code> which makes use of the SoA approach. Upon a <code>digis_d</code> instance creation, an <code>m_store</code> pointer to the GPU memory is created, where the data will be stored. Let's assume that it points to GPU memory location <code>0</code>.</p> <p></p> The GPU memory, initially empty"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#clus_","title":"<code>clus_</code>","text":"<p>To find out where the first variable will be stored (i.e. the first constant in <code>StorageLocation</code>, <code>kCLUS</code>), we check the assignment done in the class' constructor:</p> <pre><code>clus_ = getColumnAddress&lt;int32_t&gt;(StorageLocation::kCLUS, store, maxFedWords);\n</code></pre> <p>The <code>getColumnAddress</code> method is called, which will calculate the memory offset starting from the <code>store</code> pointer as:</p> <pre><code>store + kCLUS * roundFor128ByteAlignment(numPixels) = 0 + 0 * 64 = 0\n</code></pre> <p>Note</p> <p><code>roundFor128ByteAlignment</code> will calculate the minimum number of <code>uint16_t</code> variables that must be requested, given the number of pixels that must be stored. </p> <p>For 8 pixels, <code>roundFor128ByteAlignment</code> will return <code>64</code>, since a single block of 64 * <code>uint16_t</code>s (i.e. 1 x 128 bytes, since a <code>uint16_t</code> is 2 bytes) will suffice to fit 8 pixels.</p> <p>Therefore, since each <code>clus_</code> element is an <code>int32_t</code> variable, we will be storing <code>8 * sizeof(int32_t)</code> = 32 bytes starting from memory address 0 (calculated by <code>getColumnAddress</code> above).</p> <p></p>  The GPU memory, with clus_ stored. The data takes up memory address 0 to 31"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#pdigi_","title":"<code>pdigi_</code>","text":"<p>Moving on, next in <code>StorageLocation</code> is <code>kPDIGI</code>, with value <code>2</code>. Similarly, we calculate the initial memory offset that the data will be stored at by using the <code>getColumnAddress</code> method:</p> <pre><code>store + kPDIGI * roundFor128ByteAlignment(numPixels) = 0 + 2 * 64 = 128\n</code></pre> <p>This number is the offset counted in <code>uint16_t</code>, meaning twice this number in bytes, meaning 256 bytes offset from the start.</p> <p></p>  The GPU memory, with pdigi_ also stored. The data takes up memory address 256 to 256 + 31  <p>Note</p> <p>Memory addresses 32 to 256, i.e. memory between <code>clus_</code> and <code>pdigi_</code>, remain unused.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#rawidarr_","title":"<code>rawIdArr_</code>","text":"<p>Similarly for <code>rawIdArr_</code> (<code>kRAWIDARR = 4</code>):</p> <pre><code>store + kRAWIDARR * roundFor128ByteAlignment(numPixels) = 0 + 4 * 64 = 256\n</code></pre> <p></p>  The GPU memory, with rawIdArr_ also stored. The data takes up memory address 512 to 512 + 31"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#adc_","title":"<code>adc_</code>","text":"<p>Next, <code>adc_</code> is stored (<code>kADC = 6</code>). These values are of size <code>uint16_t</code>, meaning that, for 8 pixels, this array will take up <code>8 * sizeof(uint16_t)</code> = 16 bytes of memory.</p> <p>To get the memory location where the data will be stored at,  we calculate the value returned from <code>getColumnAddress</code>:</p> <pre><code>store + kADC * roundFor128ByteAlignment(numPixels) = 0 + 6 * 64 = 384\n</code></pre> <p>We multiply by two, since this is still an offset counted in <code>uint16_t</code>s and we store <code>adc_</code> in memory address 768.</p> <p></p>  The GPU memory, with adc_ also stored. The data takes up memory address 768 to 768 + 31"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#xx_","title":"<code>xx_</code>","text":"<p><code>xx_</code> (<code>kXX = 7</code>) is also an <code>uint16_t</code> variable, and it is going to be stored in:</p> <pre><code>store + kXX * roundFor128ByteAlignment(numPixels) = 0 + 7 * 64 = 448\n</code></pre> <p>Final offset is 896.</p> <p></p>  The GPU memory, with x_ also stored. The data takes up memory address 896 to 896 + 16"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#final-structure","title":"Final structure","text":"<p>After following the same procedure for <code>yy_</code> and <code>moduleInd_</code>, we find the final memory structure to look like the one below:</p> <p></p>  The GPU memory, with all variables stored. The memory that remains unused is marked with light blue.  <p>Parts of the memory will remain unused, depending on the <code>numPixels</code> but, for large number of pixels, the percentage of wasted memory becomes negligible.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisCUDASOAView/#special-case","title":"Special case","text":"<p>What if <code>numPixels</code> was <code>64</code>? By following the same procedure, the resulting memory usage would look something like this:</p> <p></p>  The GPU memory, with all variables stored, and numPixels = 64. No memory is unused.  <p>Note</p> <p>Any multiple of 64 would result in no wasted memory.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisClustersFromSoA-overview/","title":"SiPixelDigisClustersFromSoA","text":"<p>Link on github.</p> <p>After the CUDA to SoA conversion has executed, the data (which is in <code>SiPixelDigisCUDASOA</code> format) is copied to <code>edm::DetSetVector</code>s (of <code>PixelDigi</code>s and <code>edmNew::DetSetVector</code>s of <code>SiPixelCluster</code>s)</p> <pre><code>flowchart\n    cudasoa[SiPixelDigisCUDASOA] --&gt; detsetpix[edm::DetSetVector of PixelDigi]\n    cudasoa --&gt; detsetclus[edmNew::DetSetVectorNew of SiPixelCluster]   \n</code></pre> <p>See also: <code>edm::DetSetVector</code>.</p> <p>Warning</p> <ul> <li>For clusters (stored in the <code>outputClusters</code> variable), an  <code>edmNew::DetSetVector&lt;SiPixelCluster&gt;</code> is used (a.k.a <code>SiPixelClusterCollectionNew</code>, found in <code>SiPixelCluster.h</code>).</li> <li>For pixel digis (stored in the <code>collection</code> variable), an <code>edm::DetSetVector&lt;PixelDigi&gt;</code>  is used (see <code>PixelDigi</code>s). </li> </ul>"},{"location":"cmssw/pixel-local/gpu/SiPixelDigisSoAFromCUDA-overview/","title":"SiPixelDigisSoAFromCUDA.cc","text":"<p>Link on github.</p> <p>Once the CUDA Raw to Cluster code has executed, the data is copied from the GPU back to the CPU in SoA format using the <code>produce</code> function found in this file.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterCUDA-overview/","title":"SiPixelRawToClusterCUDA.cc","text":"<p>File containing the wrapper class which wraps the <code>SiPixelRawToClusterGPUKernel</code> class.</p> <p>File on github.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterCUDA-overview/#uml-diagram","title":"UML diagram","text":"<pre><code> classDiagram\n      class SiPixelRawToClusterCUDA{\n         -SiPixelRawToClusterGPUKernel gpuAlgo_\n      }\n\n      class SiPixelRawToClusterGPUKernel{\n         -makeClustersAsync()\n         -makePhase2ClustersAsync()\n      }\n\n     SiPixelRawToClusterCUDA -- \"1\" SiPixelRawToClusterGPUKernel : gpuAlgo_</code></pre>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterCUDA-overview/#class-attributes","title":"Class attributes","text":""},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterCUDA-overview/#gpualgo_","title":"<code>gpuAlgo_</code>","text":"<p>An instance of the <code>SiPixelRawToClusterGPUKernel</code> class, which calls the CUDA kernels.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterCUDA-overview/#class-methods","title":"Class methods","text":""},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterCUDA-overview/#acquire","title":"<code>acquire</code>","text":"<p>This method gets the <code>Event</code> and <code>EventSetup</code> data and extracts required data from them, such as <code>gpuMap</code> and <code>words</code>.</p> <p>Then, <code>gpuAlgo_.makeClustersAsync</code> is called with the requied params.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterGPUKernel-overview/","title":"SiPixelRawToClusterGPUKernel","text":"<p>File containing the <code>pixelgpudetails</code> namespace which in turn contains:</p> <ul> <li>A class of the same name  which,   among other things, contains:<ul> <li>The <code>makeClustersAsync</code> method</li> </ul> </li> <li>CUDA kernels:<ul> <li><code>RawToDigi_kernel</code></li> </ul> </li> </ul> <p>Files on github: header and source.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterGPUKernel-overview/#sipixelrawtoclustergpukernel-class","title":"<code>SiPixelRawToClusterGPUKernel</code> Class","text":"<p>An instance of this class is created and called by the <code>SiPixelRawToClusterCUDA</code>) class.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterGPUKernel-overview/#uml-diagram","title":"UML diagram","text":"<p>Warning</p> <p>UML diagram incomplete</p> <pre><code>classDiagram\nclass SiPixelRawToClusterGPUKernel{\n-uint32_t nDigis \n-cms::cuda::host::unique_ptr~uint32_t[]~ nModules_Clusters_h\n-SiPixelDigisCUDA digis_d\n-SiPixelClustersCUDA clusters_d\n+makeClustersAsync(...) void\n+makePhase2ClustersAsync(...) void\n+getResults()\n+getErrors() \n}</code></pre>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterGPUKernel-overview/#attributes","title":"Attributes","text":""},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterGPUKernel-overview/#digis_d","title":"<code>digis_d</code>","text":"<p>An instance of SiPixelDigisCUDA, which stores digi information on the CUDA device (hence the <code>_d</code> in the name).</p> <p>It is initialized using the <code>numDigis</code> and the CUDA <code>stream</code> as parameters.</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterGPUKernel-overview/#clusters_d","title":"<code>clusters_d</code>","text":"<p>An instance of SiPixelClustersCUDA which is used to store clusters found during the <code>findClus</code> kernel execution</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterGPUKernel-overview/#methods","title":"Methods","text":""},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterGPUKernel-overview/#makeclustersasync","title":"<code>makeClustersAsync</code>","text":"<p>A function that implements the following functionality:</p> <ul> <li>Converts Raw Pixel data to Digis (by calling the <code>RawToDigi_kernel</code>)</li> <li>Calibrates the Digis (by calling the <code>calibDigis.md</code> kernel)</li> <li>Counts the number of unique modules present in the SoA data (by calling the <code>countModules</code> kernel).</li> <li>Uses the Digis created in the first step to create Clusters (by calling the <code>findClus</code> kernel)</li> </ul>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterGPUKernel-overview/#arguments","title":"Arguments","text":""},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterGPUKernel-overview/#wordcounter-input","title":"<code>wordCounter</code> [Input]","text":"<p>Specifies the length of the SoA arrays stored in <code>SiPixelDigisCUDA</code>.</p> <p>Comment included in the source file</p> <p>wordCounter is the total no of words in each event to be trasfered (sic) on device</p>"},{"location":"cmssw/pixel-local/gpu/SiPixelRawToClusterGPUKernel-overview/#flowchart","title":"Flowchart","text":"<pre><code>graph TB\n\n    subgraph makeClustersAsync\n        C[RawToDigi_kernel] --&gt; D[calibDigis]\n        D --&gt; E[countModules]\n        E --&gt; F[findClus]\n\n        style C fill:lightgreen\n        style D fill:lightgreen \n        style E fill:lightgreen \n        style F fill:lightgreen \n    end\n\n    subgraph Legend\n        A[CUDA Kernel]\n        style A fill:lightgreen\n        style Legend fill:none\n    end</code></pre>"},{"location":"cmssw/pixel-local/gpu/gpuCalibPixel-calibDigis/","title":"calibDigis","text":"<p>This CUDA kernel resides in  <code>RecoLocalTracker/SiPixelClusterizer/plugins/gpuCalibPixel.h</code></p>"},{"location":"cmssw/pixel-local/gpu/gpuCalibPixel-calibDigis/#introduction","title":"Introduction","text":"<p>Warning</p> <p>TODO</p> <ul> <li>What's this kernel about? </li> <li>What are we expected to find in this page?</li> <li>Code structure overview</li> </ul>"},{"location":"cmssw/pixel-local/gpu/gpuCalibPixel-calibDigis/#code","title":"Code","text":"The whole kernel <pre><code>  template &lt;bool isRun2&gt;\n  __global__ void calibDigis(uint16_t* id,\n                             uint16_t const* __restrict__ x,\n                             uint16_t const* __restrict__ y,\n                             uint16_t* adc,\n                             SiPixelGainForHLTonGPU const* __restrict__ ped,\n                             int numElements,\n                             uint32_t* __restrict__ moduleStart,        // just to zero first\n                             uint32_t* __restrict__ nClustersInModule,  // just to zero them\n                             uint32_t* __restrict__ clusModuleStart     // just to zero first\n  ) {\n    int first = blockDim.x * blockIdx.x + threadIdx.x;\n\n    // zero for next kernels...\n    if (0 == first)\n      clusModuleStart[0] = moduleStart[0] = 0;\n    for (int i = first; i &lt; phase1PixelTopology::numberOfModules; i += gridDim.x * blockDim.x) {\n      nClustersInModule[i] = 0;\n    }\n\n    for (int i = first; i &lt; numElements; i += gridDim.x * blockDim.x) {\n      if (invalidModuleId == id[i])\n        continue;\n\n      bool isDeadColumn = false, isNoisyColumn = false;\n\n      int row = x[i];\n      int col = y[i];\n      auto ret = ped-&gt;getPedAndGain(id[i], col, row, isDeadColumn, isNoisyColumn);\n      float pedestal = ret.first;\n      float gain = ret.second;\n      // float pedestal = 0; float gain = 1.;\n      if (isDeadColumn | isNoisyColumn) {\n        printf(\"bad pixel at %d in %d\\n\", i, id[i]);\n        id[i] = invalidModuleId;\n        adc[i] = 0;\n      } else {\n        float vcal = float(adc[i]) * gain - pedestal * gain;\n        if constexpr (isRun2) {\n          float conversionFactor = id[i] &lt; 96 ? VCaltoElectronGain_L1 : VCaltoElectronGain;\n          float offset = id[i] &lt; 96 ? VCaltoElectronOffset_L1 : VCaltoElectronOffset;\n          vcal = vcal * conversionFactor + offset;\n        }\n        adc[i] = std::max(100, int(vcal));\n      }\n    }\n  }\n</code></pre>"},{"location":"cmssw/pixel-local/gpu/gpuCalibPixel-calibDigis/#1-init","title":"1. Init","text":"<p>This part of the code has nothing to do with calibrating pixels. We're initialising <code>clusModuleStart</code> and <code>moduleStart</code> as well as <code>nClustersInModule</code>. Even if we have effectively zero digis in this event we still need to call this kernel for these initialisations to happen. </p> <pre><code>// zero for next kernels...\nif (0 == first)\n    clusModuleStart[0] = moduleStart[0] = 0;\nfor (int i = first; i &lt; phase1PixelTopology::numberOfModules; i += gridDim.x * blockDim.x) {\n    nClustersInModule[i] = 0;\n}\n</code></pre>"},{"location":"cmssw/pixel-local/gpu/gpuCalibPixel-calibDigis/#2-adc-to-vcal","title":"2. ADC to VCAL","text":"<p>Converting from <code>ADC</code> to <code>VCAL</code>:</p> <pre><code>auto ret = ped-&gt;getPedAndGain(id[i], col, row, isDeadColumn, isNoisyColumn);\nfloat pedestal = ret.first;\nfloat gain = ret.second;\n</code></pre> <p>...</p> <pre><code>float vcal = float(adc[i]) * gain - pedestal * gain;\n</code></pre> <p>Note that to determine the <code>gain</code> and <code>pedestal</code> values the inverse of them is measured. This is done by injecting different <code>VCAL</code> values to the detector and measuring the <code>ADC</code> response.</p> ADC-to-charge calibration<sup>1</sup> <p>In the second step of the ADC-to-charge calibration, a polynomial of first degree is fit to the ADC vs charge measurements. The fit is performed in a restricted VCAL range to minimize the influence of the non-linearities at high charge. The resulting two parameters (gain and pedestal) are displayed in Fig. 2 as obtained in a calibration run from October 2009. The gain is the inverse slope and the pedestal is the offset in Fig. 1. The parameters are very stable and are determined about every four months for control purposes.</p> <p> Figure 1. Example ADC response as a function of injected charge in VCAL units (see text for conversion to electrons). The red line is a first degree polynomial fit to the data in a restricted VCAL range  https://doi.org/10.1016/j.nima.2010.11.188 </p> <p> Figure 2. Distributions of the gain and pedestal constants for each pixel as obtained in a dedicated calibration run in October 2009. https://doi.org/10.1016/j.nima.2010.11.188 </p> <p>Some more recent slides from https://indico.cern.ch/event/914013/#10-gain-calibration-for-run3-p:</p> <p></p>"},{"location":"cmssw/pixel-local/gpu/gpuCalibPixel-calibDigis/#3-vcal-to-electrons-charge","title":"3. VCAL to electrons (charge)","text":"VCAL to charge conversion (up to Run2)<pre><code>if constexpr (isRun2) {\n    float conversionFactor = id[i] &lt; 96 ? VCaltoElectronGain_L1 : VCaltoElectronGain;\n    float offset = id[i] &lt; 96 ? VCaltoElectronOffset_L1 : VCaltoElectronOffset;\n    vcal = vcal * conversionFactor + offset;\n}\n</code></pre> ADC-to-charge calibration description<sup>1</sup> <p>The ADC-to-charge calibration proceeds in two steps. First, in a dedicated standalone calibration run (3\u20136 hour duration, depending on the setup) of the pixel detector, all pixels are subject to charge injection from around 4000 electrons into the saturation regime (&gt; 50000 electrons). ... The charge injection is controlled by a digital-to-analog-converter called VCAL. The relation between VCAL and injected charge Q in electrons, Q = 65.5\u00d7VCAL\u2212414, has been obtained from dedicated x-ray source calibrations with variable x-ray energies (17.44 keV from Mo, 22.10 keV from Ag, and 32.06 keV from Ba, excited from a primary Am source).</p> <p>To us, the linear relationship is relevant here <code>Q = 65.5\u00d7VCAL\u2212414</code>.</p> <p></p> Note the difference between Run2 and afterwards <p>Gain calibration has changed from Run3, for more information read the following resources:</p> <p>PRs:</p> <p> pixel mc gain calibration scheme: new GTs for MC Run3, modified Clusterizer conditions for Run3 #29333</p> <p> Add SiPixelVCal DB object for pixel gain calibration #29829</p> <p>Presentations:</p> <p>https://indico.cern.ch/event/879470/contributions/3796405/attachments/2009273/3356603/pix_off_25_3_gain_calibration_mc.pdf</p> <p>From this presentation:</p> <p>For the phase1 pixels the MC gain constants used to be:</p> <p><code>gains = 3.17 (0.055)</code></p> <p><code>pedestal = 16.3 (5.4)</code></p> <p>same for bpix &amp; fpix.</p> <p>After the inclusion of the vcal calibration they become:</p> <p><code>gains = 149(2.6) for L1 158.6(2.8)</code></p> <p><code>pedestal = 16.7 (5.4) for L1 20.5(5.4)</code></p> <pre><code>from Configuration.Eras.Modifier_run3_common_cff import run3_common\nrun3_common.toModify(siPixelClusters,\n    VCaltoElectronGain = 1, # all gains=1, pedestals=0\n    VCaltoElectronGain_L1 = 1, #\n    VCaltoElectronOffset = 0, #\n    VCaltoElectronOffset_L1 = 0 #\n)\n</code></pre> <p>Practically speaking, we're combining the two linear models, <code>ADC to VCAL</code> and <code>VCAL to #electrons</code> into 1 linear model. So afterwards, our <code>VCAL to electron</code> conversion is only defined for backwards compatibility, it is not used/executed, it doesn't do anything. (<code>slope=1, offset=0</code>)</p>"},{"location":"cmssw/pixel-local/gpu/gpuCalibPixel-calibDigis/#4-min-electron-cut","title":"4. min electron cut","text":"minimum electron value becomes 100<pre><code>adc[i] = std::max(100, int(vcal));\n</code></pre> <p>This is also present in the legacy code.</p>"},{"location":"cmssw/pixel-local/gpu/gpuCalibPixel-calibDigis/#5-conclusion","title":"5. Conclusion","text":"<p>From a high level overview, this kernel calculates the charge (#electrons) for some pixel hit.</p> <p>We receive an <code>ADC</code> value at a specific <code>x</code>, <code>y</code> coordinate in a specific module <code>id[i]</code>, perform the gain calibration <code>ADC-&gt;VCAL and VCAL-&gt;electrons</code> (or <code>ADC-&gt;electrons</code>).</p> <p>Output #electrons in <code>adc</code> array</p> <p>We store the output, <code>#electrons</code> in the same storage that we had for input <code>ADCs</code>, the <code>adc</code> array.</p>"},{"location":"cmssw/pixel-local/gpu/gpuCalibPixel-calibDigis/#citations","title":"Citations","text":"<ol> <li> <p>Urs Langenegger, Offline calibrations and performance of the CMS pixel detector, Nuclear Instruments and Methods in Physics Research Section A:  Accelerators, Spectrometers, Detectors and Associated Equipment,Volume 650, Issue 1, 2011, Pages 25-29, ISSN 0168-9002, https://doi.org/10.1016/j.nima.2010.11.188. (https://www.sciencedirect.com/science/article/pii/S0168900210027385)\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"cmssw/pixel-local/gpu/gpuCalibPixel/","title":"gpuCalibPixel.h","text":"<p>File containing:</p> <ul> <li><code>calibDigis</code> kernel</li> <li><code>calibDigisPhase2</code> kernel</li> </ul> <p>File on github.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/","title":"countModules","text":"<p>Todo</p> <ul> <li>Add more detailed introduction</li> <li>How is the data split? How much works is done per thread? </li> </ul> <p>CUDA kernel which implements the following purposes:</p> <ul> <li>Initialize the <code>clusterId</code> array, to be used in later code (in the <code>findClus</code> kernel),</li> <li>Filter out invalid modules (TODO: what does it mean for a module to be invalid?)</li> <li>Fill the <code>moduleStart</code> array, which is an array of indices which point to the first element of the SoA data which corresponds to each module. Since data is stored in a SoA format, the data of all modules is stored in  a non-consecutive way, as far as modules are concerned, in a single 1D array. The <code>moduleStart</code> indices are therefore required for accessing the data of each module. See the <code>Data Structure</code> section for more information.</li> </ul>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/#code","title":"Code","text":"Kernel code countModules kernel<pre><code>  template &lt;bool isPhase2&gt;\n  __global__ void countModules(uint16_t const* __restrict__ id,\n                               uint32_t* __restrict__ moduleStart,\n                               int32_t* __restrict__ clusterId,\n                               int numElements) {\n    int first = blockDim.x * blockIdx.x + threadIdx.x;\n    constexpr int nMaxModules = isPhase2 ? phase2PixelTopology::numberOfModules : phase1PixelTopology::numberOfModules;\n    assert(nMaxModules &lt; maxNumModules);\n    for (int i = first; i &lt; numElements; i += gridDim.x * blockDim.x) {\n      clusterId[i] = i;\n      if (invalidModuleId == id[i])\n        continue;\n      auto j = i - 1;\n      while (j &gt;= 0 and id[j] == invalidModuleId)\n        --j;\n      if (j &lt; 0 or id[j] != id[i]) {\n        // boundary...\n        auto loc = atomicInc(moduleStart, nMaxModules);\n        moduleStart[loc + 1] = i;\n      }\n    }\n  }\n</code></pre>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/#detailed-explanation","title":"Detailed explanation","text":""},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/#0-introduction","title":"0. Introduction","text":""},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/#00-arguments","title":"0.0 Arguments","text":""},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/#uint16_t-const-__restrict__-id-input","title":"<code>uint16_t const* __restrict__ id</code> [Input]","text":"<p>This is an array (with length equal to the total number of digis), which identifies the module id that each digi corresponds to.</p> <p>This <code>id</code> is NOT the same with the <code>DetId</code>, but it's a GPU-only identifier.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/#uint32_t-__restrict__-modulestart-output","title":"<code>uint32_t* __restrict__ moduleStart</code> [Output]","text":"<p>An array of indices that ???????</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/#01-implementation-details","title":"0.1 Implementation Details","text":"<ul> <li>The <code>first</code> variable contains the global thread id within   the kernel.</li> <li><code>numElements</code> == <code>wordCounter</code> ???is this the same to the total number of digis???</li> <li>Each thread is responsible for more than one digis, i.e. if there are less blocks than    required to cover all the digis, each thread will also iterate with step equal to   number of blocks * threads per block. This is done with the <code>for</code> loop.</li> </ul>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/#1-init-for-clustering","title":"1. Init for clustering","text":"<p>We initialise the <code>clusterId</code>s for the <code>findClus</code> kernel.</p> <pre><code>for (int i = first; i &lt; numElements; i += gridDim.x * blockDim.x) {\n    clusterId[i] = i;\n</code></pre> <p>This part of the code that has nothing to do with counting the modules yet.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/#2-digi-order","title":"2. Digi order","text":"<p>Let's say we have a snippet from our <code>id</code> array.</p> <p>Instead of having numbers for the <code>id</code> we'll use letters, <code>A</code>, <code>B</code>, <code>C</code> and <code>D</code>, and mark invalid module ids with \u274c.</p> <code>id</code>AA\u274c\u274cAB\u274cBBCC\u274c\u274cD <p>Digis ordered by modules</p> <p>It is a prerequisite and we know that digis belonging to one module will appear  consecutive in our buffer. They might be separated by invalid digis/hits.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/#3-look-for-boundary-elements","title":"3. Look for boundary elements","text":"<p>Let's use our example digi array from the previous point.</p> <p>In the first row we'll show <code>id</code> and in the second column the <code>threadIdx.x</code>.</p> <code>id</code>AA\u274c\u274cAB\u274cBBCC\u274c\u274cD <code>threadIdx.x</code>012345678910111213 <p>Let's execute some of our code:</p> <pre><code>if (invalidModuleId == id[i])\n  continue;\nauto j = i - 1;\n</code></pre> <p>Note</p> <p><code>i</code> is the unique index of each digi.</p> <p><code>j</code> is the unique index of the previous (in regard to <code>i</code>) valid digi.</p> <code>id</code>AA\u274c\u274cAB\u274cBBCC\u274c\u274cD <code>threadIdx.x</code>012345678910111213 <code>i</code>012345678910111213 <code>j</code>-10\u274c\u274c34\u274c6789\u274c\u274c12 <p>Next:</p> <pre><code>while (j &gt;= 0 and id[j] == invalidModuleId)\n  --j;\n</code></pre> <p>This means that we keep going back, checking previous digis, until we stop finding digis which belong to invalid modules. In the end, <code>j</code> will hold the index of <code>i</code>th digi's closest valid backward neighbour incremented by 1:</p> <code>id</code>AA\u274c\u274cAB\u274cBBCC\u274c\u274cD <code>threadIdx.x</code>012345678910111213 <code>i</code>012345678910111213 <code>j</code> before-10\u274c\u274c34\u274c6789\u274c\u274c12 while\u2193\u2193\u2193 <code>j</code> after-10\u274c\u274c14\u274c5789\u274c\u274c10 <p>It's now time to check at which index the <code>id</code> value changes (i.e.: the data of the next module begins):</p> <pre><code>if (j &lt; 0 or id[j] != id[i]) {\n  // boundary...\n  auto loc = atomicInc(moduleStart, nMaxModules);\n  moduleStart[loc + 1] = i;\n}\n</code></pre> <p>Let's set <code>cond = (j &lt; 0 or id[j] != id[i])</code>. Check when this will be true (<code>T</code> is true, <code>F</code> is false, \u274c is not evaluated because that thread terminated early due to <code>id</code> being equal to <code>invalidModuleId</code>):</p> <code>id</code>AA\u274c\u274cAB\u274cBBCC\u274c\u274cD <code>threadIdx.x</code>012345678910111213 <code>i</code>012345678910111213 <code>j</code> after-10\u274c\u274c14\u274c5789\u274c\u274c10 <code>cond</code>TF\u274c\u274cFT\u274cFFTF\u274c\u274cT <p>Now, let's look at the <code>id</code> and <code>cond</code>, getting rid of <code>False</code> <code>cond</code> and invalid <code>id</code>s to better see what is happening:</p> <code>id</code>AA\u274c\u274cAB\u274cBBCC\u274c\u274cD <code>cond</code>TTTT <p>Wherever the <code>cond</code> is <code>T</code>, the data of a new module begins.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-countModules/#4-set-modulestart-for-each-module","title":"4. set <code>moduleStart</code> for each module","text":"<pre><code>auto loc = atomicInc(moduleStart, nMaxModules);\nmoduleStart[loc + 1] = i;\n</code></pre> <code>atomicInc</code> documentation <p><pre><code>unsigned int atomicInc(unsigned int* address,\n                   unsigned int val);\n</code></pre> reads the 32-bit word <code>old</code> located at the address <code>address</code> in global or shared memory, computes <code>((old &gt;= val) ? 0 : (old+1))</code>, and stores the result back to memory at the same address. These three operations are performed in one atomic transaction. The function returns <code>old</code>. </p> <p>After execution of the lines above, the following takes place:</p> <ul> <li>Using <code>atomicInc()</code>, the value at <code>moduleStart[0]</code> is incremented   by one, i.e: <code>moduleStart[0] + 1</code><sup>1</sup>.</li> <li><code>loc</code> will contain the value of <code>moduleStart[0]</code>, before the <code>atomicInc()</code> operation,   meaning it will be equal to <code>moduleStart[0]</code>.</li> </ul> <p>This, in effect, counts the total times that the <code>cond</code> mentioned above was evaluated to <code>True</code>, which, in effect, corresponds to the total number of modules encountered.</p> <p>Since the value returned by the <code>atomicInc()</code> (stored in <code>loc</code>)  is the <code>moduleStart[0]</code> value prior to its incrementation, it acts, indirectly, as an index to store consecutively in the <code>moduleStart</code> array the indices of the elements where <code>cond == T</code>.</p> <p>Therefore, for each <code>cond == T</code> we are:</p> <ul> <li>Incrementing the <code>moduleStart[0]</code> element by 1.</li> <li>Storing the index <code>i</code> in the next available <code>moduleStart</code> array position.</li> </ul> <p>We fill the <code>moduleStart</code> array with starting module indices. Note that we can't make sure that the first module we mark is <code>A</code> and then <code>B</code>, etc. This code is executed competitively by all of the GPU threads so the final <code>moduleStart</code> array will differ from execution to execution, even if the input data is the same:</p> pos0123456 <code>moduleStart</code> (execution i)40591300 <code>moduleStart</code> (execution ii)40913500 <code>moduleStart</code> (execution iii)41309500 <p>The last row of the table above is read as follows:</p> <ul> <li>There is a total of <code>4</code> indices stored in this array.</li> <li>One module starts at index <code>13</code> (in our example, module <code>D</code>)</li> <li>One module starts at index <code>0</code> (in our example, module <code>A</code>)   </li> <li>One module starts at index <code>9</code> (in our example, module <code>C</code>)   </li> <li>One module starts at index <code>5</code> (in our example, module <code>B</code>)</li> </ul> <p>The order that the indices are stored in <code>moduleStart</code> is determined by the order which threads reach line <code>18</code>.</p> <p>Important note 1</p> <p>In the end, <code>moduleStart[0]</code> records the total number of modules found in the array.</p> <p>Then, since we store the indices of the array elements where the module <code>id</code> changes, it means that the <code>moduleStart</code> array contains at most <code>nMaxModules + 1</code> elements (to account for <code>moduleStart[0]</code> which stores the number of indices contained in the array). </p> <p>It follows that the number of elements in <code>moduleStart</code> is less than the total number of digis.</p> <p>Important note 2</p> <p>Not all modules contain the same number of pixels, since some will be invalid.</p> Example contents of <code>moduleStart</code> (after sorting, starting from element 1) <p>In this example, the first element still represents the total number of modules. The rest are sorted indices to where the data of each module starts.</p> <p><code>0</code> does not seem to be included in this array, as it is always implied that data starts at 0.</p> <p><code>1727,8,28,49,59,63,68,81,194,198,206,233,243,270,306,401,427,436,444,453,479,512,528,589,597,612,627,631,639,777,824,857,890,912,955,980,990,1021,1034,1052,1137,1142,1182,1251,1267,1316,1324,1331,1425,1474,1656,1669,1689,1704,1754,1805,1828,1838,1848,1856,1866,2043,2067,2090,2112,2128,2131,2196,2226,2336,2367,2392,2573,2622,2676,3140,3173,3213,3219,3225,3233,3269,3290,3300,3303,3310,3358,3406,3460,3488,3512,3518,3521,3572,3597,3605,3619,3625,3665,3668,3688,3766,3773,3792,3831,3843,3857,3877,3946,4245,4271,4300,4310,4313,4325,4332,4513,4528,4537,4550,4562,4590,4612,4665,4683,4689,4692,4722,4907,4935,4969,4988,5002,5024,5029,5039,5091,5117,5138,5163,5166,5175,5225,5243,5276,5294,5315,5353,5368,5388,5414,5619,5638,5695,5709,5745,5758,5775,5849,5858,5873,5887,5905,5913,5930,5952,6011,6045,6052,6057,6090,6173,6189,6216,6224,6242,6248,6258,6265,6297,6328,6452,6524,6564,6566,6572,6578,6590,6700,6707,6720,6733,6748,6770,6800,6810,6848,6851,7055,7076,7095,7116,7138,7141,7144,7147,7157,7177,7208,7239,7263,7280,7297,7304,7336,7468,7502,7595,7641,7643,7676,7698,7825,7836,7856,7869,7879,7906,7964,8065,8098,8112,8118,8137,8162,8313,8422,8521,8531,8546,8559,8566,8574,8609,8658,8667,8711,8731,8762,8773,8795,8801,8988,9008,9048,9055,9062,9065,9068,9266,9274,9275,9280,9284,9342,9418,9482,9498,9502,9513,9528,9539,9718,9724,9738,9758,9764,9772,9779,9788,9809,9838,9900,9983,10018,10029,10085,10116,10131,10141,10156,10168,10185,10202,10236,10346,10399,10412,10425,10460,10463,10467,10598,10609,10612,10616,10622,10675,10730,10745,10756,10761,10772,10776,10784,10936,10976,11014,11024,11052,11066,11072,11086,11088,11136,11174,11333,11361,11391,11395,11406,11419,11422,11549,11557,11572,11577,11590,11665,11690,11706,11712,11725,11750,11870,11879,11906,11928,11964,11970,11983,12037,12056,12064,12079,12086,12113,12164,12172,12203,12210,12352,12381,12445,12577,12585,12608,12659,12697,12795,12828,12844,12866,12896,12923,12948,12973,12982,12993,13054,13082,13090,13097,13234,13281,13314,13316,13325,13350,13362,13526,13533,13548,13564,13574,13619,13692,13731,13754,13775,13784,13793,13802,13885,13916,13925,13936,13945,13963,13967,13984,14003,14040,14074,14089,14105,14108,14135,14139,14158,14175,14228,14246,14263,14287,14318,14359,14387,14391,14406,14417,14524,14540,14546,14571,14584,14629,14668,14711,14735,14743,14752,14776,14777,14973,15000,15045,15074,15075,15094,15096,15102,15117,15142,15260,15312,15359,15375,15393,15410,15447,15644,15717,15730,15737,15770,15826,15868,15893,15900,15914,16088,16107,16123,16139,16146,16181,16192,16197,16203,16224,16240,16266,16277,16291,16351,16368,16578,16605,16671,16683,16686,16701,16862,16867,16871,16875,16891,16939,16982,17085,17127,17143,17168,17184,17202,17343,17368,17408,17416,17458,17495,17501,17535,17564,17621,17660,17691,17720,17763,17779,17807,17817,17831,17948,17968,17994,18007,18014,18018,18027,18137,18143,18227,18230,18239,18256,18298,18327,18388,18403,18406,18420,18433,18443,18451,18475,18509,18521,18540,18574,18640,18674,18688,18701,18737,18775,18797,18815,18825,18867,18938,18966,18992,19082,19134,19161,19179,19190,19314,19318,19324,19332,19344,19370,19396,19463,19488,19506,19517,19539,19566,19785,19803,19812,19837,19840,19851,19869,19892,19931,19986,20122,20147,20187,20302,20305,20310,20318,20402,20414,20416,20422,20431,20691,20702,20709,20733,20762,20767,20777,20795,20796,20809,20853,20859,20878,20895,20911,20925,20950,21161,21239,21278,21279,21285,21290,21293,21414,21418,21419,21444,21466,21524,21553,21560,21571,21584,21592,21820,21836,21851,21871,21875,21877,21879,21882,21885,21921,21962,21991,22028,22036,22053,22060,22070,22102,22385,22406,22480,22488,22497,22512,22669,22681,22704,22707,22719,22763,22808,22870,22905,22920,22930,22939,23076,23120,23152,23170,23201,23214,23218,23231,23238,23278,23324,23354,23365,23375,23417,23440,23459,23471,23483,23498,23553,23566,23588,23756,23781,23809,23817,23825,23852,23853,24127,24136,24150,24166,24179,24201,24240,24315,24362,24378,24389,24427,24586,24618,24638,24650,24666,24671,24674,24737,24832,24903,24946,25088,25114,25125,25142,25150,25159,25351,25375,25380,25388,25401,25423,25506,25520,25537,25546,25793,25801,25822,25841,25884,25886,25896,25903,25955,25979,26003,26044,26089,26097,26113,26122,26178,26269,26292,26309,26339,26347,26393,26413,26505,26524,26533,26539,26554,26602,26615,26661,26676,26681,26692,26829,26838,26863,26877,26899,26900,26912,26914,26930,26979,27012,27048,27051,27067,27081,27092,27102,27104,27224,27229,27270,27284,27291,27307,27308,27492,27500,27501,27503,27517,27559,27596,27601,27621,27623,27643,27660,27880,27907,27918,27938,27963,27966,27971,27979,28002,28052,28100,28106,28128,28142,28147,28154,28178,28193,28212,28270,28325,28349,28393,28402,28407,28598,28613,28617,28632,28644,28667,28692,28726,28741,28744,28746,28755,28773,28937,28955,28956,29048,29054,29060,29063,29116,29136,29235,29253,29294,29306,29319,29326,29328,29481,29487,29492,29495,29498,29540,29564,29571,29584,29595,29716,29739,29744,29804,29814,29818,29853,29857,29877,29884,29911,29922,29940,29951,29966,30153,30203,30237,30252,30262,30267,30389,30401,30424,30434,30440,30460,30482,30527,30552,30559,30564,30587,30591,30841,30865,30881,30905,30919,30930,30953,31005,31036,31063,31083,31086,31094,31102,31115,31135,31269,31308,31331,31340,31345,31363,31382,31464,31470,31472,31481,31488,31530,31590,31595,31624,31631,31632,31640,31651,31790,31796,31807,31840,31845,31862,31868,31882,31895,31940,31960,31973,32015,32040,32058,32065,32077,32087,32096,32113,32155,32182,32333,32362,32380,32387,32420,32427,32433,32536,32542,32544,32557,32569,32624,32640,32684,32697,32704,32707,32718,32726,32912,32928,32943,32974,32987,33002,33022,33030,33037,33077,33102,33209,33248,33279,33283,33293,33298,33303,33513,33522,33534,33544,33551,33566,33584,33590,33600,33614,33796,33810,33844,33869,33880,33882,33890,33896,33904,33930,33931,33941,33945,33951,33973,34002,34024,34072,34120,34158,34169,34175,34181,34198,34354,34369,34379,34386,34432,34462,34491,34545,34549,34555,34561,34580,34685,34701,34737,34757,34766,34769,34773,34799,34816,34833,34854,34882,34932,34961,34980,35057,35099,35105,35232,35260,35269,35284,35325,35340,35353,35360,35433,35487,35500,35517,35547,35594,35603,35613,35678,35712,35738,35750,35784,35858,35876,35914,35952,36000,36008,36020,36060,36087,36097,36112,36139,36163,36199,36230,36245,36267,36440,36448,36464,36484,36500,36508,36512,36530,36548,36561,36565,36698,36726,36841,36868,36913,36935,36951,36980,37006,37017,37028,37046,37086,37102,37126,37141,37153,37163,37175,37219,37250,37275,37299,37322,37387,37394,37432,37499,37506,37543,37586,37635,37707,37740,37756,37817,37874,37882,37892,37923,37985,37994,38014,38028,38033,38042,38053,38083,38113,38144,38168,38201,38231,38239,38246,38276,38318,38335,38355,38402,38444,38501,38525,38552,38598,38625,38648,38663,38684,38700,38727,38741,38767,38823,38854,38880,38886,38902,38916,38940,38957,39008,39042,39077,39101,39161,39194,39214,39233,39291,39318,39349,39365,39416,39462,39473,39518,39541,39608,39624,39633,39660,39717,39731,39770,39804,39814,39823,39905,39966,39981,40008,40070,40102,40110,40137,40147,40192,40207,40222,40258,40313,40365,40374,40404,40452,40464,40474,40520,40551,40559,40579,40633,40663,40699,40721,40751,40808,40827,40859,40899,40919,40928,40949,40996,41010,41054,41068,41107,41124,41157,41162,41177,41204,41222,41244,41266,41311,41333,41348,41361,41374,41400,41445,41469,41516,41532,41544,41552,41575,41602,41608,41626,41635,41646,41672,41676,41699,41746,41795,41797,41825,41846,41876,41895,41914,41945,41977,42008,42030,42049,42151,42211,42242,42278,42318,42327,42358,42419,42431,42439,42466,42510,42522,42541,42599,42644,42650,42714,42734,42773,42790,42814,42875,42898,42905,42919,42971,43011,43079,43094,43115,43145,43152,43169,43194,43229,43236,43252,43279,43334,43357,43367,43374,43388,43400,43437,43452,43465,43478,43485,43494,43504,43546,43584,43597,43606,43638,43672,43698,43711,43761,43804,43836,43879,43904,43920,43942,43998,44008,44013,44047,44084,44094,44108,44141,44181,44231,44284,44298,44372,44384,44406,44434,44504,44514,44526,44555,44573,44644,44689,44728,44766,44827,44850,44871,44887,44938,44975,45002,45015,45062,45139,45158,45163,45211,45278,45316,45356,45448,45566,45583,45611,45648,45685,45693,45697,45774,45867,45877,45899,45951,46005,46034,46046,46054,46064,46141,46158,46160,46192,46223,46232,46264,46274,46302,46335,46352,46381,46445,46507,46527,46542,46586,46622,46648,46659,46675,46681,46705,46721,46740,46753,46768,46775,46778,46781,46790,46819,46845,46866,46876,46927,46975,46979,46993,47024,47068,47089,47107,47143,47151,47212,47281,47302,47345,47385,47389,47412,47454,47518,47554,47575,47604,47645,47653,47672,47701,47757,47766,47774,47810,47853,47869,47870,47886,47918,48010,48018,48051,48093,48111,48135,48175,48226,48228,48244,48250,48272,48286,48292,48298,48315,48334,48374,48414,48444,48499,48514,48515,48544,48604,48609,48620,48638,48669,48678,48692,48731,48760,48763,48792,48826,48835,48852,48905,48914,48935,48954,48977,49035,49053,49107,49188,49227,49269,49287,49331,49337,49348,49383,49408,49429,49453,49553,49587,49596,49653,49712,49783,49791,49809,49860,49888,49902,49909,49972,50015,50037,50087,50131,50155,50182,50223,50261,50279,50301,50340,50364,50386,50406,50459,50508,50551,50579,50595,50617,50639,50658,50671,50686,50718,50733,50747,50781,50812,50836,50882,50894,50957,51011,51032,51043,51084,51113,51157,51185,51262,51264,51266,51282,51304,51307,51311,51334,51336,51339,51361,51404,51407,51410,51471,51655,51667,51677,51727,51781,51830,51900,51949,52006,52050,52073,52091,52108,52156,52192,52203,52210,52224,52240,52252,52256,52264,52277,52283,52293,52309,52321,52331,52338,52342,52349,52357,52381,52412,52451,52459,52470,52538,52643,52645,52657,52729,52770,52781,52812,52852,52897,52912,52950,52974,52998,53008,53010,53018,53033,53073,53089,53117,53121,53143,53168,53181,53191,53266,53307,53350,53354,53387,53412,53421,53473,53503,53529,53542,53545,53562,53585,53591,53600,53621,53636,53640,53642,53677,53713,53727,53735,53781,53865,53883,53887</code></p> <pre><code>auto loc = atomicInc(moduleStart, nMaxModules);\n</code></pre> <ol> <li> <p>The value will be set to <code>0</code> if the value stored  in <code>moduleStart[0]</code> exceeds the <code>nMaxModules</code> value, i.e. <code>3892</code> for Phase 2.\u00a0\u21a9</p> </li> </ol>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-findClus/","title":"findClus","text":"<p>CUDA kernel for finding Clusters given digis extracted from raw data.</p> <p>Taking advantage of the SoA data approach, it is meant to be executed with a 1D grid of blocks, each block being a 1D grid of threads. The number of blocks and threads used to launch this kernel is equal to the number of modules present in the detector which, in turn, depends on the Phase:</p> Blocks Threads Phase 1 1856 256 Phase 2 3892 384 <p>Note</p> <p>The number of blocks is equal to the number of modules, which depend on the Phase. This number is found in the Geometry/CommonTopologies CMSSW module which contains constants related to the Pixel Detector Topology.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-findClus/#overview","title":"Overview","text":"<p>This kernel is launched with 1 block per module, meaning that each GPU block will be assigned the digis of 1 module. Then, each block's thread is assigned 1 or more digis to operate on.</p> <p>Note 1</p> <p>The case to launch this kernel with less blocks than there are modules is also accounted for, by running a first <code>for</code> loop in each thread which runs from <code>firstModule</code> (i.e. the thread's Block index) to <code>endModule</code> by increments of <code>gridDim.x</code> (i.e. total number of blocks).</p> <p>Note 2</p> <p>Since the number of threads per block will probably be  smaller than the number of digis per module, a single  thread will have to take care of doing the calculations for more than one digi. </p> <p>This is accounted for by a second <code>for</code> loop which starts at <code>first</code> (calculated by using the index stored in <code>moduleStart</code> and adding the <code>threadIdx.x</code>) and iterates up to <code>numElements</code> (i.e. total number of digis), with increments of <code>blockDim.x</code> (i.e. number of threads per block)</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-findClus/#arguments","title":"Arguments","text":""},{"location":"cmssw/pixel-local/gpu/gpuClustering-findClus/#wordcounter-input","title":"<code>wordCounter</code> [Input]","text":"<p>The total number of pixels for all data.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-findClus/#calculating-the-total-number-of-pixels-in-a-module","title":"Calculating the total number of pixels in a module","text":"<p>This is done by the code below:</p> <pre><code>for (int i = first; i &lt; numElements; i += blockDim.x) {\n  if (id[i] == invalidModuleId)  // skip invalid pixels\n    continue;\n  if (id[i] != thisModuleId) {  // find the first pixel in a different module\n    atomicMin(&amp;msize, i);\n    break;\n  }\n}\n</code></pre> <p>Shared memory is used to store the <code>msize</code> variable. This is shared among the threads in the GPU block to calculate the total number of pixels that the current module has.</p> <p>In a nutshell, all threads in a GPU block are assigned elements in the <code>id</code> array, and, after skipping invalid <code>id</code>s, they try to find the  index in the <code>id</code> array where the <code>id</code> is different to the one  that the current block is assigned. </p> <p>Let's assume that the <code>id</code> array is as follows:</p> <p></p> <p>Then, a single block is assigned to Module Y data:</p> <p></p> <p>Let's assume that we are launching the kernel with 2 threads per block. Those start iterating over the <code>id</code> array, starting from the index that the <code>moduleStart</code> array indicates for the module that has been assigned to the current block (in this example, this index is <code>15</code>):</p> <p></p> <p>They keep iterating, with step <code>blockDim.x</code><sup>1</sup>. If <code>invalidModule</code> is found in the <code>id</code> array, they skip it (see <code>t1</code> below):</p> <p></p> <p>Next iteration:</p> <p></p> <p>Next iteration. Notice that <code>t1</code> is now assigned  an index which belongs to the next module, Module Z. This satisfies the <code>if (id[i] != thisModuleId)</code> condition,  so the <code>atomicMin(&amp;msize, i)</code> instruction is executed, meaning that the thread will store in <code>msize</code> the minimum between <code>msize</code> (which has been initialized with the value of the total number of pixels, i.e. the final index of the <code>id</code> array) and the current index it's assigned (<code>i</code> which is equal to <code>22</code>). <code>msize</code> is, therefore, assigned the value <code>22</code> and <code>t1</code> breaks from the loop.</p> <p></p> <p>Now, only <code>t0</code> is left to continue iterating, which, in turn, is assigned an index which belongs to  Module Z, index <code>23</code>. The condition mentioned above is satisfied again, so <code>t0</code> executes <code>atomicMin(&amp;msize, i)</code> with <code>i=23</code> and <code>msize=22</code>, so <code>msize</code> stays <code>22</code>.</p> <p></p> <p>We now know the last non-invalid index of the current module, and this index is stored in <code>msize</code>.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-findClus/#duplicate-detection","title":"Duplicate detection","text":"Alternative 1 (PR #37359, Not merged) <p>Similarly to the total module pixels calculation, to detect duplicate digis a similar iteration logic can be applied. </p> <p>Assumming 2 threads per block, <code>t0</code> starts with the first element of the module (with index <code>15</code>) and compares the <code>x</code> and <code>y</code> values between index <code>15</code> and the next one (assumming it's not invalid), i.e. index <code>16</code>:</p> <p></p> <p>If both <code>x</code> values are the same and <code>y</code> values are the same, the digi is considered a duplicate.</p> <p>Then, <code>t0</code> compares element <code>15</code> to <code>17</code>:</p> <p></p> <p>Elements <code>18</code> and <code>19</code> are invalid, so they're skipped and the <code>20</code>th is compared:</p> <p></p> <p>This goes on until the <code>msize</code>th element.</p> <p>Then, <code>t1</code> will start from the element at position <code>15 + threadIdx.x = 16</code> and compare the <code>x</code> and <code>y</code> values with the ones at position <code>17</code> and so on.</p> <p><code>t0</code> will then iterate to the next element with step size <code>blockDim.x</code> (in our case <code>2</code>), until the 2nd element from the end is reached (in our example, the element with index <code>20</code>; there's no comparison to be made once a thread reaches index <code>21</code>).</p> <p>Alternative 2 (PR #38946, Merged)</p> <p>This alternative approach uses shared thread memory to create a <code>status</code> array  where the number of times each pixel has been encountered in a module is stored. To make it as small as possible, 2 bits are used per pixel, which, for Phase 1<sup>2</sup>, amounts to 160x416x2 bits = 16640 bytes per module<sup>3</sup>.</p> <p>A visual representation of the first array element of <code>status</code> (Pixel coordinates are in X,Y):</p> <p></p> <p>In effect, we can store the status of 16 pixels per <code>status</code> array element.</p> <p>In the <code>status</code> array, one of the following values are stored:</p> <ul> <li><code>0x00</code>: Empty</li> <li><code>0x01</code>: Found</li> <li><code>0x03</code>: Duplicate</li> </ul> <p>A <code>getIndex</code> function is implemented, which maps a pixel's <code>x</code> and <code>y</code> coordinates to an index to the <code>status</code> array.</p> <p>A <code>getShift</code> function, which, once the index in the <code>status</code> array is found, finds the exact \"sub-location\" that the pixel should be stored in, in the  given <code>status</code> array element.</p> <p>For example, pixel with coordinates <code>x=2</code> and <code>y=0</code> be stored in:</p> <pre><code>getIndex(x, y) = (uint32_t))(pixelsPerModuleX * y + x) / valuesPerWord) \n    = (uint32_t)((160 * 0 + 2) / 16 ) = 0\n</code></pre> <p>So the pixel(2,0) is stored in <code>status[0]</code>.</p> <p>To find the shift amount within <code>status[0]</code> where the pixel information will be stored in: <pre><code>getShift(x, y) = (x % valuesPerWord) * 2 = (2 % 16) * 2 = 4\n</code></pre> So, we have to shift by <code>4</code> bits within <code>status[0]</code> to store information for pixel(2,0).</p> <p>Note</p> <ul> <li>In <code>getShift</code>, the multiplication by <code>2</code> corresponds to the number of bits used to store the per-pixel information.</li> <li><code>getShift</code> will not work if the <code>pixelsPerModuleX</code> value is not a multiple of <code>valuesPerWord</code> (i.e. <code>16</code>) and would need to take <code>y</code> into account too.</li> </ul> <p>To read a specific status for a specific pixel (see <code>getStatus()</code>), once we have determined the <code>index</code> and <code>shift</code>, we use the <code>mask</code> calculated (for <code>bits=2</code>, <code>mask</code> is <code>0x03</code>) to extract the data.</p> <p></p> <p>Doing a bitwise AND (<code>&amp;</code>) of the <code>mask</code> with the <code>status[index]</code> value, after right-shifting (<code>&gt;&gt;</code>) the latter by the <code>shift</code> value calculated, we can read the 2 bits that refer to the specific pixel:</p> <p> </p> <p>To write the status to a specific pixel (see <code>promote()</code>), we typecast the status to <code>uint32_t</code>:</p> <p> </p> <p>Then left-shift it (<code>&lt;&lt;</code>) by <code>shift</code> bytes:</p> <p> </p> <p>The <code>new_word</code> is then calculated:</p> <p> </p> <p>And, finally, an <code>atomicCAS</code> (Compare And Swap) operation is done between <code>status[i]</code> and <code>new_word</code>, which stores the new value back in the <code>status</code> array, if <code>new_word</code> is different than <code>status[i]</code>.</p> <p>Note</p> <p><code>atomicOR</code> could be used directly, instead of using OR (<code>|</code>) and then <code>atomicCAS</code>, but it seems it's a bit slower. </p> <p>Documentation on <code>atomicCAS</code> can be found here.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-findClus/#histogram-filling","title":"Histogram Filling","text":"<p>Todo</p> <p>what the heckerino is histogram filling?</p> <p>The <code>hist</code> histogram is stored in shared memory, so that its access from all threads is as fast as possible.</p> <p>Code:</p> <pre><code>// fill histo\nfor (int i = first; i &lt; msize; i += blockDim.x) {\n  if (id[i] == invalidModuleId)  // skip invalid pixels\n    continue;\n  hist.count(y[i]);\n}\n__syncthreads();\nif (threadIdx.x &lt; 32)\n  ws[threadIdx.x] = 0;  // used by prefix scan...\n__syncthreads();\nhist.finalize(ws);\n__syncthreads();\nfor (int i = first; i &lt; msize; i += blockDim.x) {\n  if (id[i] == invalidModuleId)  // skip invalid pixels\n    continue;\n  hist.fill(y[i], i - firstPixel);\n}\n</code></pre> <p>Competitive filling</p> <p>We will fill our histogram <code>hist</code> with the values <code>i-firstPixel</code>. We make sure that each pixel get's in the right bin, corresponding to their column in the module map.</p> <pre><code>hist.fill(y[i], i - firstPixel);\n</code></pre> <p>Here as we know, <code>y</code> is the column value array of the digis. </p> <p>If we iterate through the histogram, pixels in the first column will be processed sooner to pixels in the next column, and so on.</p> <p>What we don't know however is what order we are going to process our pixels in one column/bin. Filling the histogram is competitive between the threads, the following image illustrates this.</p> <p></p> <p>Figure 1 - Order in histogram</p> <p>Not to misunderstand, we don't fill our histogram in this order, this is the iteration order of <code>cms::cuda::HistoContainer</code>.</p> <p>This iteration order can change, and most probably will change from reconstruction to reconstruction.</p> <p>We see the relative positions of pixels in our cluster:</p> <p></p> <p>Figure 2 - Our HistoContainer</p> <p>Our ordering will be defined as top to bottom inside bins and left to right between bins.</p> <p>At least for this example, it doesn't really matter if one imagines the order in one bin the other way around.</p> <p></p> <p>Figure 3 - What we store in the HistoContainer</p> <p>On the left, we see how we will later iterate through the histogram.</p> <p>In the middle and on the right we see what we are actually storing in the histogram.</p> <p>We are storing <code>i-firstPixel</code>. This is the relative position of our digis in the digi view/array.</p> <p>We don't need all data about digis stored there, just their position in the <code>digi array</code> or <code>digi view</code>.</p> <p>Actually, not even that. We only need their relative position and not the absolute. That is because all digis belonging to the same module are consecutive in the digi array.</p> <p>This way we can save precious space, because we would need <code>32 bits</code> to store the absolute position of a digi, however, this way we can use only <code>16 bits</code>.</p> <p>Hmm, <code>16 bits</code> means <code>2^16 = 65536</code> maximum possible relative positions. </p> <p>How do we know there are no more digis in the module?</p> <p>On one hand, it is very unlikely, since in <code>phase 1</code> our module dimensions are <code>80*2*52*8 = 66560</code>.</p> <p>Module occupancy is much lower than <code>0.98</code> in any layer, so we're good.</p> <p>Still, we're making constraints on the maximum number of digis in the mdodule. </p> <p>Currently, this is </p> <pre><code>//6000 max pixels required for HI operations with no measurable impact on pp performance\nconstexpr uint32_t maxPixInModule = 6000;\n</code></pre> <p>We actually don't use this here. We will use this for something else, namely iterating through the digis in the module. Why we will need this will be uncovered soon.</p> <p>This example only contained one cluster, but in reality we will likely have some consecutive clusters. </p> <p></p> <p>Figure 4 - Multiple clusters in one module</p> <p>Again, in reality our cluster will be more spread out, we are only drawing them this close together for the sake of the example.</p> <p>Adding multiple clusters to the game the iteration order will change.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-findClus/#nearest-neighbours-and-counting-them","title":"Nearest neighbours and counting them","text":"<p>A crucial part of the code is the following:</p> <pre><code>// allocate space for duplicate pixels: a pixel can appear more than once\n// with different charge in the same event\nconstexpr int maxNeighbours = 10;\nassert((hist.size() / blockDim.x) &lt;= maxiter);\n// nearest neighbour\nuint16_t nn[maxiter][maxNeighbours];\nuint8_t nnn[maxiter];  // number of nn\nfor (uint32_t k = 0; k &lt; maxiter; ++k)\n  nnn[k] = 0;\n\n__syncthreads();  // for hit filling!\n</code></pre> <p>nn or nearest neighbours</p> <p>We want to store the relative position of the nearest neighbours of every digi. Basically, that's why we created the histogram in the first place. With the histogram the job is half-done, we know the neighbours of every digi columnwise.</p> <p>We will create local arrays to store the neighbours.</p> <p>We consider neighbours to be 8-connected.</p> <p></p> <p>This would max out the number of possible neighbours to be, well, 8. But as the comment above (and below) explains, due to some read-out inconsistencies or whatnot we can have duplicate digis. So we allocate some extra space for them, but we'll also use some <code>assertions</code> later on to make sure we don't exceed our self-made limit.</p> <pre><code>// allocate space for duplicate pixels: a pixel can appear more than once\n// with different charge in the same event\nconstexpr int maxNeighbours = 10;\n</code></pre> <p>How many digis?</p> <p>We finally get to answer why we have an upper limit on the number of digis that we get to check rigorously in our kernel.</p> <p><pre><code>//6000 max pixels required for HI operations with no measurable impact on pp performance\nconstexpr uint32_t maxPixInModule = 6000;\n</code></pre> It is connected to this:</p> <pre><code>assert((hist.size() / blockDim.x) &lt;= maxiter);\n</code></pre> <p>We want to store nearest neighbours for every digi/pixel, but we don't know in advance how many there are. But we do need to fix the number of threads in advance, that is compile time constant.</p> <p><code>nn</code> is thread local, so what we will do, is make it two dimensional and let the threads iterate through the module digis, always increasing the <code>position</code> by <code>blockDim.x</code>, and store nearest neighbours of the next digi in the next row of the <code>nn</code> array.</p> <pre><code>// nearest neighbour\nuint16_t nn[maxiter][maxNeighbours];\nuint8_t nnn[maxiter];  // number of nn\n</code></pre> <p>For example, if <code>blockDim.x = 16</code> and we have <code>120</code> digis/hits in our event, then <code>thread 3</code> will process the following digis:</p> digi id nn place 3 -&gt; nn[0] 19 -&gt; nn[1] 35 -&gt; nn[2] 51 -&gt; nn[3] 67 -&gt; nn[4] 83 -&gt; nn[5] 99 -&gt; nn[6] 115 -&gt; nn[7] <p>We must decide  (or do we) the size of <code>nn</code> in compile time too, so that's why we have <code>maxiter</code> and this dangereous looking message:</p> <pre><code>#ifdef __CUDA_ARCH__\n// assume that we can cover the whole module with up to 16 blockDim.x-wide iterations\nconstexpr int maxiter = 16;\nif (threadIdx.x == 0 &amp;&amp; (hist.size() / blockDim.x) &gt;= maxiter)\n  printf(\"THIS IS NOT SUPPOSED TO HAPPEN too many hits in module %d: %d for block size %d\\n\",\n         thisModuleId,\n         hist.size(),\n         blockDim.x);\n#else\n    auto maxiter = hist.size();\n#endif\n</code></pre> <p>It really isn't supposed to happen. Why?</p> Why? What would happen in our code if this were true? <pre><code>threadIdx.x == 0 &amp;&amp; (hist.size() / blockDim.x) &gt;= maxiter\n</code></pre> <p>Let's say <code>hist.size() = 300</code>.</p> <p>Well, then <code>thread 3</code> would try to put the nearest neighbours of the following digis in the following non-existing places:</p> digi id nn place 259 -&gt;  nn[16]  275 -&gt;  nn[17]  291 -&gt;  nn[18]  <p>We really don't want to have out of bounds indexing errors. It could happen in theory, that's why we run simulations and try to find out are expected (max) occupancy in advance and set <code>maxiter</code> accordingly.</p> <p>nnn or number of nearest neighbours</p> <p>We will keep track of the number of nearest neighbours as well in a separate array.</p> <pre><code>uint8_t nnn[maxiter];  // number of nn\n</code></pre> <p>We could actually get rid of this and follow a different approach, can you find out how?</p> How? No, but really, think about it I'm serious <p>Ok, well. Technically, when you create <code>nn</code></p> <pre><code>// nearest neighbour\nuint16_t nn[maxiter][maxNeighbours];\n</code></pre> <p>You could do this:</p> <pre><code>// nearest neighbour\nuint16_t nn[maxiter][maxNeighbours+1];\n</code></pre> <p>And save one field at the end of each row for a special value e.g. and initialize all values to <code>numeric_limits&lt;uint16_t&gt;::max()-1</code> and later only iterate until we reach this value.</p> <p>This solution actually uses a bit more space <code>16 vs 8 bits</code> and requires us to do some initialization.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-findClus/#filling-nn","title":"Filling nn","text":"<p>Let's look at how we actually fill our nearest neighbours arrays:</p> <pre><code>// fill NN\nfor (auto j = threadIdx.x, k = 0U; j &lt; hist.size(); j += blockDim.x, ++k) {\n  assert(k &lt; maxiter);\n  auto p = hist.begin() + j;\n  auto i = *p + firstPixel;\n  assert(id[i] != invalidModuleId);\n  assert(id[i] == thisModuleId);  // same module\n  int be = Hist::bin(y[i] + 1);\n  auto e = hist.end(be);\n  ++p;\n  assert(0 == nnn[k]);\n  for (; p &lt; e; ++p) {\n    auto m = (*p) + firstPixel;\n    assert(m != i);\n    assert(int(y[m]) - int(y[i]) &gt;= 0);\n    assert(int(y[m]) - int(y[i]) &lt;= 1);\n    if (std::abs(int(x[m]) - int(x[i])) &gt; 1)\n      continue;\n    auto l = nnn[k]++;\n    assert(l &lt; maxNeighbours);\n    nn[k][l] = *p;\n  }\n}\n</code></pre> <p>Current iteration, keeping track of <code>k</code></p> <p>We will use <code>k</code> to keep track of which iteration we are currently in:</p> <pre><code>for (auto j = threadIdx.x, k = 0U; j &lt; hist.size(); j += blockDim.x, ++k) \n</code></pre> <p>It shall not overflow <code>maxiter</code> <pre><code>assert(k &lt; maxiter);\n</code></pre></p> <p>There hasn't been any nearest neighbours added to <code>nn[k]</code> so our counter of them nnn[k] should be zero, we check this here:</p> <pre><code>assert(0 == nnn[k]);\n</code></pre> <p>When we find a neighbour, we add it to <code>nn[k]</code>:</p> <pre><code>auto l = nnn[k]++;\nassert(l &lt; maxNeighbours);\nnn[k][l] = *p;\n</code></pre> <p>We also check that our index <code>l</code> is within bounds.</p> <p>Pointer to hist element <code>p</code></p> <p>We we look at the <code>j</code>th element in the histogram and set the pointer <code>p</code> to this element, <code>i</code> will be the absolute position of our digi.</p> <pre><code>auto p = hist.begin() + j;\nauto i = *p + firstPixel;\n</code></pre> <p>We made sure of these conditions when we created and filled the histogram</p> <pre><code>assert(id[i] != invalidModuleId);\nassert(id[i] == thisModuleId);  // same module\n</code></pre> <p>Let's find the pointer to the last element in the next bin, this will be <code>e</code>, probably short for <code>end</code>.</p> <p>Also, we increase <code>p</code> by one so we only start considering digis that come after <code>p</code></p> <pre><code>int be = Hist::bin(y[i] + 1);\nauto e = hist.end(be);\n++p;\n</code></pre> <p><code>m</code>, or possible neighbours</p> <p>Finally we iterate over elements from <code>p++</code> until <code>e</code></p> <pre><code>for (; p &lt; e; ++p) {\n  auto m = (*p) + firstPixel;\n  assert(m != i);\n  ...\n}\n</code></pre> <p>We know that our column is correct:</p> <pre><code>assert(int(y[m]) - int(y[i]) &gt;= 0);\nassert(int(y[m]) - int(y[i]) &lt;= 1);\n</code></pre> <p>So we only need to check whether our row value is <code>&lt;=1</code></p> <pre><code>if (std::abs(int(x[m]) - int(x[i])) &gt; 1)\n  continue;\n</code></pre> <p>If our row is within bounds, we add <code>m</code> to <code>nn</code>.</p> Example <p>In this example:</p> <p></p> <p>For <code>m</code> we consider the following values</p> i - firstPixel *(++p) <code>m</code> 4 -&gt; 1 9 13 12 5 2 16 1 -&gt; 9 13 12 5 2 26 9 -&gt; 13 12 5 2 16 13 -&gt; 12 5 2 16 <p>And for the <code>nearest neighbours</code> we get:</p> i - firstPixel nn values 4 -&gt; 9 12 5 1 -&gt; 9 9 -&gt; 12 13 -&gt; 2"},{"location":"cmssw/pixel-local/gpu/gpuClustering-findClus/#assign-same-clusterid-to-clusters","title":"Assign same <code>clusterId</code> to clusters","text":"<p>Essentially, the following piece of code assigns the same <code>clusterId</code> to all pixels/digis in a cluster.</p> <p>These <code>clusterId</code>s won't be ordered, start from <code>0</code>, but they will be the same in for neighbouring pixels. They will also be in the range <code>0</code> to <code>numElements</code>, which is the maximum number of digis for this particular event.</p> <pre><code>bool more = true;\nint nloops = 0;\nwhile (__syncthreads_or(more)) {\n    if (1 == nloops % 2) {\n    for (auto j = threadIdx.x, k = 0U; j &lt; hist.size(); j += blockDim.x, ++k) {\n        auto p = hist.begin() + j;\n        auto i = *p + firstPixel;\n        auto m = clusterId[i];\n        while (m != clusterId[m])\n        m = clusterId[m];\n        clusterId[i] = m;\n    }\n    } else {\n    more = false;\n    for (auto j = threadIdx.x, k = 0U; j &lt; hist.size(); j += blockDim.x, ++k) {\n        auto p = hist.begin() + j;\n        auto i = *p + firstPixel;\n        for (int kk = 0; kk &lt; nnn[k]; ++kk) {\n        auto l = nn[k][kk];\n        auto m = l + firstPixel;\n        assert(m != i);\n        auto old = atomicMin_block(&amp;clusterId[m], clusterId[i]);\n        // do we need memory fence?\n        if (old != clusterId[i]) {\n            // end the loop only if no changes were applied\n            more = true;\n        }\n        atomicMin_block(&amp;clusterId[i], old);\n        }  // nnloop\n    }    // pixel loop\n    }\n    ++nloops;\n}  // end while\n</code></pre> <p>We also get a bit of explanation, namely</p> <pre><code>// for each pixel, look at all the pixels until the end of the module;\n// when two valid pixels within +/- 1 in x or y are found, set their id to the minimum;\n// after the loop, all the pixel in each cluster should have the id equeal to the lowest\n// pixel in the cluster ( clus[i] == i ).\n</code></pre> <p>This is all true, but we don't see actually why though.</p> <p>We need to understand what <code>nn</code> and <code>nnn</code> and the <code>hist</code> is, and how everything is connected, and why our <code>while</code> loop is divided into two parts.</p> <p>So let's dig in.</p> <pre><code>bool more = true;\nint nloops = 0;\n</code></pre> <p><code>more</code> will be set to true every loop if we updated the <code>cluterId</code> for the current pixel, and so it will tell us to terminate our loop or not</p> <p><code>nloops</code> or number of loops</p> <pre><code>while (__syncthreads_or(more)) {\n</code></pre> <p>One can reason intuitavely what this does, or can consult the CUDA C Programming Guide.</p> <p>int __syncthreads_or(int predicate);</p> <p>is identical to __syncthreads() with the additional feature that it evaluates predicate for all threads of the block and returns non-zero if and only if predicate evaluates to non-zero for any of them.</p> <p>So <code>more</code>, our local variable is scanned for every thread in the block. We terminate the loop if we didn't update any <code>cluterId</code>s in the previous iteration.</p> <ol> <li> <p>This is equal to the number of threads per block the kernel  was launched with.\u00a0\u21a9</p> </li> <li> <p>See here \u21a9</p> </li> <li> <p>An NVIDIA T4 card has a limit of 64kB of shared memory, see here under <code>Maximum amount of shared memory per thread block</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-overview/","title":"gpuClustering overview","text":"<p>Located in: <code>RecoLocalTracker/SiPixelClusterizer/plugins/</code></p> <p>Link to file on github.</p>"},{"location":"cmssw/pixel-local/gpu/gpuClustering-overview/#purpose","title":"Purpose","text":"<p>This file's purpose is to provide a namespace (<code>gpuclustering</code>) which contains the following functionality in the form of CUDA kernels:</p> <ul> <li>Count modules (TODO???) (see here)</li> <li>Find clusters in the data passed (see here)</li> </ul>"},{"location":"cmssw/workflows/history/","title":"History","text":"<p>Following is the evolution of GPU workflows in <code>CMSSW</code> with events (<code>PR</code>s, presentations) more or less in chronological order.</p>"},{"location":"cmssw/workflows/history/#early-days","title":"Early days","text":"<p>Very early spots and historical versions can be discovered in the Patatrack fork of CMSSW.</p> <p>Some of the <code>PR</code>s concerned here are, but are not limited to (based on #31854 description):</p> <ul> <li>Add workflows for Riemann fit and GPU cms-patatrack/cmssw#20 Add workflows for Riemann fit and GPU</li> <li>Add a DQM sequence for pixel-only tracking cms-patatrack/cmssw#23 Add a DQM sequence for pixel-only tracking</li> <li>Riemann fit gpu cms-patatrack/cmssw#60 Port the Riemann fit to CUDA</li> <li>Add pixel tracking workflows for data cms-patatrack/cmssw#144 Add pixel tracking workflows for data</li> <li>Renumber GPU workflows cms-patatrack/cmssw#259 Change GPU workflow numbering: .7-&gt;.51, .8-&gt;.52, .9-&gt;.53</li> <li>Tracking developments for review and merging cms-patatrack/cmssw#338 Rework the Riemann fit and broken line fit</li> <li>Updating RelVal WF &amp; customisation on CPU cms-patatrack/cmssw#549 Update the RelVal workflows and the CPU</li> </ul>"},{"location":"cmssw/workflows/history/#towards-integration-in-cmssw","title":"Towards integration in CMSSW","text":""},{"location":"cmssw/workflows/history/#1-add-patatrack-process-modifiers-and-workflows-28522","title":"1. Add Patatrack process modifiers and workflows #28522","text":"Quote <p>fwyzard commented on Dec 2, 2019</p> <p>PR description: Add the pixelNtupleFit process modifier, that will be used to customise the Pixel-only tracks to use the ntuplet fit developed as part of the Patatrack workflows.</p> <p>Add the \"gpu\" process modifer, that can be used to enable offloading of available modules to run on GPUs.</p> <p>Add the first Patatrack workflows (currently just a placeholder).</p>"},{"location":"cmssw/workflows/history/#2-patatrack-integration-in-cmssw-a-bocci-reconstruction-meeting-20200320","title":"2. Patatrack integration in CMSSW, A. Bocci, Reconstruction meeting, 2020.03.20.","text":"<p>Relevant slide here.</p>"},{"location":"cmssw/workflows/history/#begin-integration-in-cmssw","title":"Begin integration in CMSSW","text":""},{"location":"cmssw/workflows/history/#patatrack-integration-pixel-workflows-12n-31854","title":"Patatrack integration - Pixel workflows (12/N) #31854","text":"Quote <p>** Merged silviodonato merged 54 commits into cms-sw:master from cms-patatrack:patatrack_integration_12_N_pixel_workflows on Apr 12, 2021**</p> <p>fwyzard commented on Oct 19, 2020</p> <p>PR description: Update the <code>runTheMatrix.py</code> workflows for pixel-only tracking:</p> <ul> <li><code>###.501</code>: pixel-only quadruplets on CPU</li> <li><code>###.502</code>: pixel-only quadruplets on GPU</li> <li><code>###.505</code>: pixel-only triplets on CPU</li> <li><code>###.506</code>: pixel-only triplets on GPU</li> </ul>"},{"location":"cmssw/workflows/history/#redesign","title":"Redesign","text":"<p>Once the main <code>PR</code> originating from <code>cms-patatrack/cms-sw</code> has been merged, different redesigns happened:</p>"},{"location":"cmssw/workflows/history/#1-redesign-all-gpu-workflows-to-detect-if-a-gpu-is-present-and-fall-back-to-cpu-otherwise-33428","title":"1. Redesign all GPU workflows to detect if a GPU is present, and fall back to CPU otherwise #33428","text":"Quote <p>Merged cmsbuild merged 11 commits into cms-sw:master from fwyzard:auto_gpu_workflows on May 11, 2021</p> <p>fwyzard commented on Apr 14, 2021</p> <p>PR description:</p> <p>Redesign the GPU workflows:</p> <ul> <li>the CPU (*e.g. ###.501) and GPU (###.502) workflows should now be as close as possible;</li> <li>the implementation of the CPU and GPU workflows has been simplified;</li> <li>all GPU workflows use the SwitchProducerCUDA mechanism to detect if a GPU is available and offload a module or task to the GPU; if not, they automatically fall back to the equivalent CPU modules and tasks;</li> <li>when the \"gpu\" modifier is used, the pixel local reconstruction workflow used the \"HLT\" payload type both on the CPU and on the GPU, for better consistency of the results;</li> <li>the \"Patatrack\" pixel tracks reconstruction on CPU is based on a modifier (pixelNtupletFit) instead of a customisation, in line with the other workflows;</li> <li>the HCAL-only workflows should follow more closely the implementation of the general reconstruction sequence, both for Run 2 (2018) and Run 3 scenarios.</li> </ul> <p>Some changes to the relevant EDProducers have made the definition of the workflows easier:</p> <ul> <li>the SoA-to-legacy HCAL rechit producer has been updated to make the production of the SoA and/or legacy collections optional;</li> <li>the legacy ECAL unpacker has been updated to declare only the event products it will actually produce;</li> <li>the default labels used in many modules have been updated to reflect the labels used in the configuration.</li> </ul> <p>Some other general changes and code clean up:</p> <ul> <li>remove some no-longer-used files as well as some commented-out code</li> <li>always clone() a module used in a SwitchProducerCUDA</li> <li>move the implementation of the gpuVertexFinder kernels from gpuVertexFinderImpl.h to gpuVertexFinder.cc</li> </ul> <p>The update has been presented here: https://indico.cern.ch/event/1033022/#47-gpu-workflows.</p>"},{"location":"cmssw/workflows/history/#2-updated-gpu-workflows-in-cmssw-a-bocci-reconstruction-and-analysis-tools-meeting-20210430","title":"2. updated GPU workflows in CMSSW, A. Bocci, Reconstruction and Analysis Tools meeting, 2021.04.30.","text":"<p>Relevant slides:</p> <p>slide2</p> <p>slide3</p> <p>slide4</p> <p>slide7</p>"},{"location":"cmssw/workflows/history/#3-update-gpu-workflows-35331","title":"3. Update GPU workflows #35331","text":"Quote <p>Merged cmsbuild merged 4 commits into cms-sw:master from fwyzard:update_GPU_workflows_121x on Sep 24, 2021</p> <p>fwyzard commented on Sep 18, 2021</p> <p>PR description:</p> <p>Add new GPU workflows, that run the Patatrack pixel local and pixel-only track reconstruction in addition to the full reconstruction, with the possibility of offloading to GPUs also the ECAL and HCAL local reconstruction.</p> <p>PR validation:</p> <p>Validated with</p> <pre><code>runTheMatrix.py -w upgrade -j 4 -t 8 -l 11634.591,11634.592,11634.595,11634.596\n...\nRunning up to 4 concurrent jobs, each with 8 threads per process\n...\nSSED Step2-PASSED Step3-PASSED  - time date Sat Sep 18 12:26:05 2021-date Sat Sep 18 12:21:24 2021; exit: 0 0 0 0\n Step2-PASSED Step3-PASSED  - time date Sat Sep 18 12:26:12 2021-date Sat Sep 18 12:21:25 2021; exit: 0 0 0 0\n1-PASSED Step2-PASSED Step3-PASSED  - time date Sat Sep 18 12:26:15 2021-date Sat Sep 18 12:21:25 2021; exit: 0 0 0 0\n1-PASSED Step2-PASSED Step3-PASSED  - time date Sat Sep 18 12:26:10 2021-date Sat Sep 18 12:21:26 2021; exit: 0 0 0 0\n</code></pre>"},{"location":"cmssw/workflows/history/#4-add-workflows-for-profiling-the-gpu-code-35540","title":"4. Add workflows for profiling the GPU code #35540","text":"Quote <p>Merged cmsbuild merged 2 commits into cms-sw:master from fwyzard:add_GPU_profiling_workflows on Oct 8, 2021</p> <p>fwyzard commented on Oct 5, 2021</p> <p>PR description:</p> <p>Add four workflows for profiling the GPU code:</p> <ul> <li>.504 Pixel-only local reconstruction and quadruplets</li> <li>.508 Pixel-only local reconstruction and triplets</li> <li>.514 ECAL-only local reconstruction</li> <li>.524 ECAL-only local reconstruction</li> </ul> <p>The workflows explicitly consume the GPU products, so they can only run on a GPU-equipped machine.</p> <p>The transfer to the host and the conversion to the legacy format is not run.</p> <p>PR validation:</p> <p>Used to profile the various workflows on top of CMSSW_12_1_0_pre3, runing over that release's TTbar relvals with pileup:</p> measurement CMSSW_12_1_0_pre3 I/O throughput ~ 2 kev/s <code>11634.504</code> 1071 \u00b1 3 ev/s <code>11634.508</code> 560 \u00b1 2 ev/s <code>11634.514</code> 1391 \u00b1 5 ev/s <code>11634.524</code> 1354 \u00b1 10 ev/s"},{"location":"cmssw/workflows/history/#fixes","title":"Fixes","text":""},{"location":"cmssw/workflows/history/#1-fix-the-patatrack-pixel-local-reconstruction-running-on-cpu-35915","title":"1. Fix the Patatrack pixel local reconstruction running on CPU #35915","text":"Quote <p>Merged cmsbuild merged 1 commit into cms-sw:master from fwyzard:fix_hltSiPixelRecHitSoA_121x on Nov 1, 2021</p> <p>fwyzard commented on Oct 29, 2021</p> <p>PR description:</p> <p>Use the hltSiPixelRecHitSoA producer for the pixel rechits in legacy and SoA format, instead of running the legacy producer.</p> <p>PR validation: Successfully run the GPU workflow 11634.506.</p>"},{"location":"cmssw/workflows/overview/","title":"Workflows overview","text":"<p>A single data analysis pipeline using <code>cmsRun</code> requires multiple steps, each one  requiring data fetching, analysing, configuring and lot of other details that  can take a lot of time to get familiar with in order to produce useful output.</p> <p>For this reason the concept of workflows has been introduced so that multiple steps can be executed from a single program, using a single unique  identifying number (\"Workflow number\").</p> <p>The program responsible for executing workflows is a Python script available  on LXPLUS named <code>runTheMatrix.py</code> (its source code can be found here).</p>"},{"location":"cmssw/workflows/usage/","title":"runTheMatrix Usage","text":""},{"location":"cmssw/workflows/usage/#prerequisites","title":"Prerequisites","text":"<p>You will need to run the following command to acquire a proxy to access the grid, if the workflow requires access to data on the grid:</p> <p><pre><code>voms-proxy-init -voms cms -rfc\n</code></pre> Workflows that generate their own data during <code>step1</code> should not need a proxy.</p>"},{"location":"cmssw/workflows/usage/#execution","title":"Execution","text":""},{"location":"cmssw/workflows/usage/#list-gpu-workflows","title":"List GPU workflows","text":"<pre><code>runTheMatrix.py -n --what gpu\n</code></pre> Example output <pre><code>ignoring non-requested file relval_standard\n...\n\nfound a total of  58  workflows:\n136.885502 RunHLTPhy2018D+HLTDR2_2018+RECODR2_2018reHLT_Patatrack_PixelOnlyGPU+HARVEST2018_pixelTrackingOnly\n136.885512 RunHLTPhy2018D+HLTDR2_2018+RECODR2_2018reHLT_ECALOnlyGPU+HARVEST2018_ECALOnly \n136.885522 RunHLTPhy2018D+HLTDR2_2018+RECODR2_2018reHLT_HCALOnlyGPU+HARVEST2018_HCALOnly \n136.888502 RunJetHT2018D+HLTDR2_2018+RECODR2_2018reHLT_Patatrack_PixelOnlyGPU+HARVEST2018_pixelTrackingOnly \n136.888512 RunJetHT2018D+HLTDR2_2018+RECODR2_2018reHLT_ECALOnlyGPU+HARVEST2018_ECALOnly \n136.888522 RunJetHT2018D+HLTDR2_2018+RECODR2_2018reHLT_HCALOnlyGPU+HARVEST2018_HCALOnly \n10824.502 2018_Patatrack_PixelOnlyGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.503 2018_Patatrack_PixelOnlyGPU_Validation+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.504 2018_Patatrack_PixelOnlyGPU_Profiling+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT \n10824.506 2018_Patatrack_PixelOnlyTripletsGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.507 2018_Patatrack_PixelOnlyTripletsGPU_Validation+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.508 2018_Patatrack_PixelOnlyTripletsGPU_Profiling+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT \n10824.512 2018_Patatrack_ECALOnlyGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.513 2018_Patatrack_ECALOnlyGPU_Validation+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.514 2018_Patatrack_ECALOnlyGPU_Profiling+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT \n10824.522 2018_Patatrack_HCALOnlyGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.523 2018_Patatrack_HCALOnlyGPU_Validation+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.524 2018_Patatrack_HCALOnlyGPU_Profiling+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT \n10824.582 2018_Patatrack_AllGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.583 2018_Patatrack_AllGPU_Validation+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.586 2018_Patatrack_AllTripletsGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.587 2018_Patatrack_AllTripletsGPU_Validation+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.592 2018_Patatrack_FullRecoGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.593 2018_Patatrack_FullRecoGPU_Validation+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.596 2018_Patatrack_FullRecoTripletsGPU+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10824.597 2018_Patatrack_FullRecoTripletsGPU_Validation+TTbar_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10842.502 2018_Patatrack_PixelOnlyGPU+ZMM_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10842.503 2018_Patatrack_PixelOnlyGPU_Validation+ZMM_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10842.504 2018_Patatrack_PixelOnlyGPU_Profiling+ZMM_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT \n10842.506 2018_Patatrack_PixelOnlyTripletsGPU+ZMM_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT\n10842.507 2018_Patatrack_PixelOnlyTripletsGPU_Validation+ZMM_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT+HARVESTFakeHLT \n10842.508 2018_Patatrack_PixelOnlyTripletsGPU_Profiling+ZMM_13TeV_TuneCUETP8M1_GenSim+Digi+RecoFakeHLT \n11634.502 2021_Patatrack_PixelOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.503 2021_Patatrack_PixelOnlyGPU_Validation+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.504 2021_Patatrack_PixelOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano \n11634.506 2021_Patatrack_PixelOnlyTripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.507 2021_Patatrack_PixelOnlyTripletsGPU_Validation+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.508 2021_Patatrack_PixelOnlyTripletsGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano \n11634.512 2021_Patatrack_ECALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.513 2021_Patatrack_ECALOnlyGPU_Validation+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.514 2021_Patatrack_ECALOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano \n11634.522 2021_Patatrack_HCALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.523 2021_Patatrack_HCALOnlyGPU_Validation+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.524 2021_Patatrack_HCALOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano \n11634.582 2021_Patatrack_AllGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.583 2021_Patatrack_AllGPU_Validation+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.586 2021_Patatrack_AllTripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.587 2021_Patatrack_AllTripletsGPU_Validation+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.592 2021_Patatrack_FullRecoGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.593 2021_Patatrack_FullRecoGPU_Validation+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.596 2021_Patatrack_FullRecoTripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11634.597 2021_Patatrack_FullRecoTripletsGPU_Validation+TTbar_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11650.502 2021_Patatrack_PixelOnlyGPU+ZMM_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11650.503 2021_Patatrack_PixelOnlyGPU_Validation+ZMM_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11650.504 2021_Patatrack_PixelOnlyGPU_Profiling+ZMM_14TeV_TuneCP5_GenSim+Digi+RecoNano \n11650.506 2021_Patatrack_PixelOnlyTripletsGPU+ZMM_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano \n11650.507 2021_Patatrack_PixelOnlyTripletsGPU_Validation+ZMM_14TeV_TuneCP5_GenSim+Digi+RecoNano+HARVESTNano\n11650.508 2021_Patatrack_PixelOnlyTripletsGPU_Profiling+ZMM_14TeV_TuneCP5_GenSim+Digi+RecoNano \n\n12 workflows with 3 steps\n46 workflows with 4 steps\n</code></pre> <p>Note</p> <ul> <li>Workflows starting with <code>136.885</code> or <code>136.888</code> contain actual, detector data.</li> <li>Workflows starting with <code>11634</code> are Monte Carlo (simulated) data.</li> <li>Workflows ending in <code>.504</code> are usually used for profiling, as they don't run any DQM code.</li> <li>Workflows ending in <code>.508</code> detect Triplets (tracks using only 3 hits, instead of 4) and are also used for profiling.</li> <li>More information on specific workflows here</li> </ul>"},{"location":"cmssw/workflows/usage/#list-profiling-workflows","title":"List Profiling workflows","text":"<p>Some of the workflows mentioned here, for example the profiling ones, can be found by running:</p> <pre><code>runTheMatrix.py -n --what upgrade | grep Patatrack\n</code></pre> Example output <pre><code>...\n11442.595 2018DesignPU_Patatrack_TripletsCPU+ZMM_13TeV_TuneCUETP8M1_GenSim+DigiPU+RecoFakeHLTPU+HARVESTFakeHLTPU \n11442.596 2018DesignPU_Patatrack_TripletsGPU+ZMM_13TeV_TuneCUETP8M1_GenSim+DigiPU+RecoFakeHLTPU+HARVESTFakeHLTPU \n11634.501 2021_Patatrack_PixelOnlyCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11634.502 2021_Patatrack_PixelOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11634.504 2021_Patatrack_PixelOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco \n11634.505 2021_Patatrack_PixelOnlyTripletsCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11634.506 2021_Patatrack_PixelOnlyTripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11634.508 2021_Patatrack_PixelOnlyTripletsGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco \n11634.511 2021_Patatrack_ECALOnlyCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11634.512 2021_Patatrack_ECALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11634.514 2021_Patatrack_ECALOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco \n11634.521 2021_Patatrack_HCALOnlyCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11634.522 2021_Patatrack_HCALOnlyGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11634.524 2021_Patatrack_HCALOnlyGPU_Profiling+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco \n11634.591 2021_Patatrack_CPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11634.592 2021_Patatrack_GPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11634.595 2021_Patatrack_TripletsCPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11634.596 2021_Patatrack_TripletsGPU+TTbar_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11650.501 2021_Patatrack_PixelOnlyCPU+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11650.502 2021_Patatrack_PixelOnlyGPU+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco+HARVEST \n11650.504 2021_Patatrack_PixelOnlyGPU_Profiling+ZMM_14TeV_TuneCP5_GenSim+Digi+Reco \n...\n</code></pre>"},{"location":"cmssw/workflows/usage/#running-a-workflow","title":"Running a workflow","text":"<p>Before you run a workflow, you will first need to activate a CMSSW environment (Instructions here).</p>"},{"location":"cmssw/workflows/usage/#gpu","title":"GPU","text":"<p>Login into any GPU-enabled machine and, from within an activated CMS environment run any GPU workflow, e.g:</p> <pre><code>runTheMatrix.py -l 136.885502 --what gpu\n</code></pre> <p><code>-l</code> specifies the workflow to run.</p> <p><code>--what</code> specifies, among other things, the device to run this workflow on. </p>"},{"location":"cmssw/workflows/usage/#cpu","title":"CPU","text":"<p>See notes.</p>"},{"location":"cmssw/workflows/usage/#notes","title":"Notes","text":"<ol> <li><code>--what upgrade</code> TODO </li> <li>Some workflows switche to CPU or GPU automatically, depending on the system's capabilities, e.g.:</li> </ol> <pre><code>runTheMatrix.py -w upgrade -l 11634.502\n</code></pre>"},{"location":"cmssw/workflows/usage/#expected-outputs","title":"Expected outputs","text":"<p><code>runTheMatrix.py</code> executes <code>cmsDriver</code> for each step. A directory named after the complete name of the workflow will be created on each workflow execution. In it, logs and intermediate files are created. </p> <p>To debug errors for a specific step, look into those log files.</p>"},{"location":"cmssw/workflows/usage/#possible-errors","title":"Possible Errors","text":""},{"location":"cmssw/workflows/usage/#no-root-files-are-generated-during-step-2","title":"No <code>.root</code> files are generated during Step 2","text":""},{"location":"cmssw/workflows/usage/#possible-solution","title":"Possible Solution","text":"<p>Use the <code>--ibeos</code> argument along with <code>runTheMatrix.py</code></p>"},{"location":"czangela-tutorial/","title":"Index","text":"<p>This is an effort to introduce CUDA and GPU programming in the scope of CMSSW.</p> <p>Generic CUDA resources and CMSSW examples and exercises can be found, split into parts that one can follow on their own pace.</p> <p>Start here.</p>"},{"location":"czangela-tutorial/weeks/further_reading/","title":"Further reading","text":""},{"location":"czangela-tutorial/weeks/further_reading/#1-performance-measurement","title":"1. Performance measurement","text":"<ol> <li>Memory bandwidth (sometimes also referred to as memory throughput)</li> <li>Processing throughput</li> </ol> <p>To calculate the memory bandwidth we need to understand two concepts related to it.</p> <p>Memory interface width: It is the physical bit-width of the memory bus. Every clock-cycle data is transferred along a memory bus to and from the on-card memory. The memory interface width is the physical count of the bits of how many can fit down the bus per clock cycle.</p> <p>Memory clock rate: (or memory clock cycle) Measured in the unit of Hz it is the rate of memory bus clock. For example a memory bus operating on a 1GHz rate is capable of issuing memory tranfers with the bus 10^9 times per second.</p> <p>Talking about clock rate it is important to take a detour to SDR, DDR and QDR.</p> <p></p> Example <p>A simple analogy is of course public transport, where memory interface width would be the number of seats on a bus and the memory clock rate is the frequency of buses in unit time (hour let's say). Multiplying these two numbers we can calculate how many people can be transported along one bus line in a certain time period.</p> <p></p>"},{"location":"czangela-tutorial/weeks/further_reading/#2-parallel-programming-models","title":"2. Parallel Programming models","text":"<p>Suggested reading: SIMD &lt; SIMT &lt; SMT: parallelism in NVIDIA GPUs by yosefk</p> <p>Quote</p> <p>NVIDIA call their parallel programming model SIMT - \"Single Instruction, Multiple Threads\". Two other different, but related parallel programming models are SIMD - \"Single Instruction, Multiple Data\", and SMT - \"Simultaneous Multithreading\". Each model exploits a different source of parallelism:</p> <p>In SIMD, elements of short vectors are processed in parallel. In SMT, instructions of several threads are run in parallel. SIMT is somewhere in between \u2013 an interesting hybrid between vector processing and hardware threading.</p> Other resources <p>https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html</p> <p>https://towardsdatascience.com/the-ai-illustrated-guide-why-are-gpus-so-powerful-99f4ae85a5c3</p> <p>https://streamhpc.com/blog/2016-07-19/performance-can-be-measured-as-throughput-latency-or-processor-utilisation/</p> <p>https://www.gamersnexus.net/dictionary/3-memory-interface</p>"},{"location":"czangela-tutorial/weeks/week01_exercises/","title":"Week 01 Exercises","text":""},{"location":"czangela-tutorial/weeks/week01_exercises/#exercise_01_01","title":"Exercise_01_01","text":""},{"location":"czangela-tutorial/weeks/week01_exercises/#question","title":"Question","text":"<p>01. Question: raw to digi conversion kernel</p> <ul> <li>Find the kernel that converts raw data to digis.</li> <li>Where is it launched?</li> <li>What is the execution configuration?</li> <li>How do we access individual threads in the kernel?</li> </ul> <p>To search the source code use CMSSW dxr - software cross-reference.</p>"},{"location":"czangela-tutorial/weeks/week01_exercises/#solution","title":"Solution","text":"01.a. Find the kernel that converts raw data to digis. <pre><code>// Kernel to perform Raw to Digi conversion\n__global__ void RawToDigi_kernel(const SiPixelROCsStatusAndMapping *cablingMap,\n                                const unsigned char *modToUnp,\n                                const uint32_t wordCounter,\n                                const uint32_t *word,\n                                const uint8_t *fedIds,\n                                uint16_t *xx,\n                                uint16_t *yy,\n                                uint16_t *adc,\n                                uint32_t *pdigi,\n...\n</code></pre> 01.b. Where is it launched? <p>It is launched in the <code>makeClustersAsync</code> function: <pre><code>void SiPixelRawToClusterGPUKernel::makeClustersAsync(bool isRun2,\n                                                   const SiPixelClusterThresholds clusterThresholds,\n                                                   const SiPixelROCsStatusAndMapping *cablingMap,\n                                                   const unsigned char *modToUnp,\n                                                   const SiPixelGainForHLTonGPU *gains,\n                                                   const WordFedAppender &amp;wordFed,\n                                                   SiPixelFormatterErrors &amp;&amp;errors,\n                                                   const uint32_t wordCounter,\n...       \n// Launch rawToDigi kernel\n  RawToDigi_kernel&lt;&lt;&lt;blocks, threadsPerBlock, 0, stream&gt;&gt;&gt;(\n      cablingMap,\n      modToUnp,\n      wordCounter,\n      word_d.get(),\n      fedId_d.get(),\n      digis_d.view().xx(),\n      digis_d.view().yy(),\n      digis_d.view().adc(),\n...\n</code></pre></p> 01.c. What is the execution configuration? <p>For the <code>RawToDigi_kernel</code> he execution configuration is defined as <pre><code>&lt;&lt;&lt;blocks, threadsPerBlock, 0, stream&gt;&gt;&gt;\n</code></pre></p> <p>Where  <pre><code>const int threadsPerBlock = 512;\nconst int blocks = (wordCounter + threadsPerBlock - 1) / threadsPerBlock;  // fill it all\n</code></pre></p> <p>In this case </p> <p></p> 01.d. How do we access individual threads in the kernel? <pre><code>int32_t first = threadIdx.x + blockIdx.x * blockDim.x;\n</code></pre>"},{"location":"czangela-tutorial/weeks/week01_exercises/#exercise_01_02","title":"Exercise_01_02","text":""},{"location":"czangela-tutorial/weeks/week01_exercises/#question_1","title":"Question","text":"<p>02. Question: host and device functions</p> <ul> <li> <p>Give an example of <code>global</code>, <code>device</code> and <code>host-device</code> functions in <code>CMSSW</code>.</p> </li> <li> <p>Can you find an example where <code>host</code> and <code>device</code> code diverge? How is this achieved?</p> </li> </ul>"},{"location":"czangela-tutorial/weeks/week01_exercises/#solution_1","title":"Solution","text":"02.a. Give an example of <code>global</code>, <code>device</code> and <code>host-device</code> functions in <code>CMSSW</code>. <p>For example see <code>__global__</code> kernel in previous exercise.</p> <p><code>__device__</code> function in RecoLocalTracker/SiPixelClusterizer/plugins/SiPixelRawToClusterGPUKernel.cu: <pre><code>__device__ pixelgpudetails::DetIdGPU getRawId(const SiPixelROCsStatusAndMapping *cablingMap,\n                                                uint8_t fed,\n                                                uint32_t link,\n                                                uint32_t roc) {\n    uint32_t index = fed * MAX_LINK * MAX_ROC + (link - 1) * MAX_ROC + roc;\n    pixelgpudetails::DetIdGPU detId = {\n        cablingMap-&gt;rawId[index], cablingMap-&gt;rocInDet[index], cablingMap-&gt;moduleId[index]};\n    return detId;\n}\n</code></pre></p> <p><code>__host__</code> <code>__device__</code>function in HeterogeneousCore/CUDAUtilities/interface/OneToManyAssoc.h: <pre><code>__host__ __device__ __forceinline__ void add(CountersOnly const &amp;co) {\n    for (int32_t i = 0; i &lt; totOnes(); ++i) {\n#ifdef __CUDA_ARCH__\n        atomicAdd(off.data() + i, co.off[i]);\n#else\n        auto &amp;a = (std::atomic&lt;Counter&gt; &amp;)(off[i]);\n        a += co.off[i];\n#endif\n    }\n}\n</code></pre></p> 02.b. Can you find an example where host and device code diverge? How is this achieved? <p>In the CUDA C Programming Guide we can read that:</p> <p>The <code>__device__</code> and <code>__host__</code> execution space specifiers can be used together however, in which case the function is compiled for both the host and the device.</p> <p>The <code>__CUDA_ARCH__</code> macro introduced in Application Compatibility can be used to differentiate code paths between host and device:</p> <pre><code>__host__ __device__ func()\n{\n#if __CUDA_ARCH__ &gt;= 800\n// Device code path for compute capability 8.x\n#elif __CUDA_ARCH__ &gt;= 700\n// Device code path for compute capability 7.x\n#elif __CUDA_ARCH__ &gt;= 600\n// Device code path for compute capability 6.x\n#elif __CUDA_ARCH__ &gt;= 500\n// Device code path for compute capability 5.x\n#elif __CUDA_ARCH__ &gt;= 300\n// Device code path for compute capability 3.x\n#elif !defined(__CUDA_ARCH__) \n// Host code path\n#endif\n}\n</code></pre> <p>Based on this we can see how execution diverges in the previous <code>add</code> function: <pre><code>__host__ __device__ __forceinline__ void add(CountersOnly const &amp;co) {\n    for (int32_t i = 0; i &lt; totOnes(); ++i) {\n#ifdef __CUDA_ARCH__\n        atomicAdd(off.data() + i, co.off[i]);\n#else\n        auto &amp;a = (std::atomic&lt;Counter&gt; &amp;)(off[i]);\n        a += co.off[i];\n#endif\n    }\n}\n</code></pre></p>"},{"location":"czangela-tutorial/weeks/week01_exercises/#exercise_01_03","title":"Exercise_01_03","text":""},{"location":"czangela-tutorial/weeks/week01_exercises/#exercise","title":"Exercise","text":"<p>03. Exercise: Write a kernel in which</p> <ul> <li> <p>if we're running on the <code>device</code> each thread prints which <code>block</code> and <code>thread</code> it is associated with, for example <code>block 1 thread 3</code></p> </li> <li> <p>if we're running on the <code>host</code> each thread just prints <code>host</code>.</p> </li> <li> <p>Test your program!</p> </li> </ul> How can you \"hide\" your GPU? <p>Try using <code>CUDA_VISIBLE_DEVICES</code> from the command line.</p>"},{"location":"czangela-tutorial/weeks/week01_exercises/#exercise_01_04","title":"Exercise_01_04","text":""},{"location":"czangela-tutorial/weeks/week01_exercises/#exercise_1","title":"Exercise","text":"<p>04. Exercise: Fine-grained vs coarse-grained parallelism: Give examples in the <code>MatMulKernel</code> kernel of coarse-grained and fine-grained data parallelism (as defined in CUDA abstraction model) as well as sequential execution.</p> MatMulKernelMatrix definitionGetElement and SetElementGetSubMatrix <pre><code>// Thread block size\n#define BLOCK_SIZE 16\n\n __global__ void MatMulKernel(Matrix A, Matrix B, Matrix C)\n{\n    // Block row and column\n    int blockRow = blockIdx.y;\n    int blockCol = blockIdx.x;\n\n    // Each thread block computes one sub-matrix Csub of C\n    Matrix Csub = GetSubMatrix(C, blockRow, blockCol);\n\n    // Each thread computes one element of Csub\n    // by accumulating results into Cvalue\n    float Cvalue = 0;\n\n    // Thread row and column within Csub\n    int row = threadIdx.y;\n    int col = threadIdx.x;\n\n    // Loop over all the sub-matrices of A and B that are\n    // required to compute Csub\n    // Multiply each pair of sub-matrices together\n    // and accumulate the results\n    for (int m = 0; m &lt; (A.width / BLOCK_SIZE); ++m) {\n\n        // Get sub-matrix Asub of A\n        Matrix Asub = GetSubMatrix(A, blockRow, m);\n\n        // Get sub-matrix Bsub of B\n        Matrix Bsub = GetSubMatrix(B, m, blockCol);\n\n        // Shared memory used to store Asub and Bsub respectively\n        __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n        __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n        // Load Asub and Bsub from device memory to shared memory\n        // Each thread loads one element of each sub-matrix\n        As[row][col] = GetElement(Asub, row, col);\n        Bs[row][col] = GetElement(Bsub, row, col);\n\n        // Synchronize to make sure the sub-matrices are loaded\n        // before starting the computation\n        __syncthreads();\n        // Multiply Asub and Bsub together\n        for (int e = 0; e &lt; BLOCK_SIZE; ++e)\n            Cvalue += As[row][e] * Bs[e][col];\n\n        // Synchronize to make sure that the preceding\n        // computation is done before loading two new\n        // sub-matrices of A and B in the next iteration\n        __syncthreads();\n    }\n\n    // Write Csub to device memory\n    // Each thread writes one element\n    SetElement(Csub, row, col, Cvalue);\n}\n</code></pre> <pre><code>// Matrices are stored in row-major order:\n// M(row, col) = *(M.elements + row * M.stride + col)\ntypedef struct {\n    int width;\n    int height;\n    int stride; \n    float* elements;\n} Matrix;\n</code></pre> <pre><code>// Get a matrix element\n__device__ float GetElement(const Matrix A, int row, int col)\n{\n    return A.elements[row * A.stride + col];\n}\n\n// Set a matrix element\n__device__ void SetElement(Matrix A, int row, int col,\n                        float value)\n{\n    A.elements[row * A.stride + col] = value;\n}\n</code></pre> <pre><code>// Get the BLOCK_SIZExBLOCK_SIZE sub-matrix Asub of A that is\n// located col sub-matrices to the right and row sub-matrices down\n// from the upper-left corner of A\n__device__ Matrix GetSubMatrix(Matrix A, int row, int col) \n{\n    Matrix Asub;\n    Asub.width    = BLOCK_SIZE;\n    Asub.height   = BLOCK_SIZE;\n    Asub.stride   = A.stride;\n    Asub.elements = &amp;A.elements[A.stride * BLOCK_SIZE * row\n                                        + BLOCK_SIZE * col];\n    return Asub;\n}\n</code></pre> <p></p>"},{"location":"czangela-tutorial/weeks/week01_exercises/#solution_2","title":"Solution","text":"04.a. Give examples in the <code>MatMulKernel</code> kernel of coarse-grained data parallelism. <p>Coarse-grained data parallel problems in the CUDA programming model are problems that can be solved independently in parallel by blocks of threads.</p> <p>For example in the <code>MatMulKernel</code>:</p> <pre><code> // Block row and column\nint blockRow = blockIdx.y;\nint blockCol = blockIdx.x;\n\n// Each thread block computes one sub-matrix Csub of C\nMatrix Csub = GetSubMatrix(C, blockRow, blockCol);\n</code></pre> <p>The computation of <code>Csub</code> is independent of the computation of other submatrices of <code>C</code>. The work is divided between <code>blocks</code>, no synchronization is performed between computing different <code>submatrices of C</code>.</p> 04.b. Give examples in the <code>MatMulKernel</code> kernel of fine-grained data parallelism. <p>Fine-grained data parallel problems in the CUDA programming model are finer pieces that can be solved cooperatively in parallel by all threads within the block.</p> <p>For example in the <code>MatMulKernel</code>:</p> <pre><code>// Get sub-matrix Asub of A\nMatrix Asub = GetSubMatrix(A, blockRow, m);\n\n// Get sub-matrix Bsub of B\nMatrix Bsub = GetSubMatrix(B, m, blockCol);\n\n// Shared memory used to store Asub and Bsub respectively\n__shared__ float As[BLOCK_SIZE][BLOCK_SIZE];\n__shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];\n\n// Load Asub and Bsub from device memory to shared memory\n// Each thread loads one element of each sub-matrix\nAs[row][col] = GetElement(Asub, row, col);\nBs[row][col] = GetElement(Bsub, row, col);\n\n// Synchronize to make sure the sub-matrices are loaded\n// before starting the computation\n__syncthreads();\n</code></pre> <p>Loading data into shared memory blocks of <code>matrix A and B</code> is executed parellel by all threads within the block.</p> 04.c. Give examples in the <code>MatMulKernel</code> kernel of sequential execution. <pre><code>// Multiply Asub and Bsub together\nfor (int e = 0; e &lt; BLOCK_SIZE; ++e)\n    Cvalue += As[row][e] * Bs[e][col];\n</code></pre> <p>The computation of <code>Cvalue</code> for each thread is sequential, we execute <code>BLOCK_SIZE</code> additions and multiplications.</p> <p>On the other hand the computation of <code>Cvalue</code> is also a good example of fine-grained data parallelism, since there is one value computed by each thread in the block parallel.</p> <p>To identify fine-grained parallelism one just needs to look for block-level synchronization:</p> <pre><code>for (int e = 0; e &lt; BLOCK_SIZE; ++e)\n    Cvalue += As[row][e] * Bs[e][col];\n\n// Synchronize to make sure that the preceding\n// computation is done before loading two new\n// sub-matrices of A and B in the next iteration\n__syncthreads();\n</code></pre>"},{"location":"czangela-tutorial/weeks/week01_material/","title":"Week 01","text":"Overview <ul> <li>What is the CUDA programming model?</li> <li>Hierarchy of thread groups</li> <li>Kernels and other language extensions</li> </ul> Resources <p>This material heavily borrows from the following sources:</p> <ul> <li> <p>CUDA C++ Programming Guide</p> </li> <li> <p>Introduction to GPUs by New York University</p> </li> <li> <p>Introduction to parallel programming and CUDA by Felice Pantaleo</p> </li> </ul>"},{"location":"czangela-tutorial/weeks/week01_material/#introduction","title":"Introduction","text":""},{"location":"czangela-tutorial/weeks/week01_material/#1-cuda-a-general-purpose-parallel-computing-platform-and-programming-model","title":"1. CUDA\u00ae: A General-Purpose Parallel Computing Platform and Programming Model","text":"<p>In November 2006, NVIDIA\u00ae  introduced CUDA\u00ae , which originally stood for \u201cCompute Unified Device Architecture\u201d, a general purpose parallel computing platform and programming model that leverages the parallel compute engine in NVIDIA GPUs to solve many complex computational problems in a more efficient way than on a CPU.</p> <p>CUDA comes with a software environment that allows developers to use C++ as a high-level programming language. Other languages, application programming interfaces, or directives-based approaches are supported, such as FORTRAN, DirectCompute, OpenACC.</p> <p></p>"},{"location":"czangela-tutorial/weeks/week01_material/#2-a-scalable-programming-model","title":"2. A Scalable Programming Model","text":"At the core of the  CUDA parallel programming model there are three key abstractions: <ul> <li>a hierarchy of thread groups</li> <li>shared memories</li> <li>barrier synchronization</li> </ul> <p>They are exposed to the programmer as a minimal set of language extensions.</p> <p>These abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism.</p> <p>Further reading and material</p> <p>Optional reading and exercise on this topic at abstractions: granularity.</p>"},{"location":"czangela-tutorial/weeks/week01_material/#programming-model","title":"Programming model","text":""},{"location":"czangela-tutorial/weeks/week01_material/#3-kernels","title":"3. Kernels","text":"<p>CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels, that, when called, are executed <code>N</code> times in parallel by <code>N</code> different CUDA threads, as opposed to only once like regular C++ functions.</p> <p>A kernel is defined using the <code>__global__</code> declaration specifier and the number of CUDA threads that execute that kernel for a given kernel call is specified using a new <code>&lt;&lt;&lt;...&gt;&gt;&gt;</code> execution configuration syntax.</p> <p>Each thread that executes the kernel is given a unique thread ID that is accessible within the kernel through built-in variables.</p> Kernel and execution configuration example <pre><code>// Kernel definition\n__global__ void VecAdd(float* A, float* B, float* C)\n{\n    int i = threadIdx.x;\n    C[i] = A[i] + B[i];\n}\n\nint main()\n{\n    ...\n    // Kernel invocation with N threads\n    VecAdd&lt;&lt;&lt;1, N&gt;&gt;&gt;(A, B, C);\n    ...\n}\n</code></pre> <p>01. Question: raw to digi conversion kernel</p> <ul> <li>Find the kernel that converts raw data to digis.</li> <li>Where is it launched?</li> <li>What is the execution configuration?</li> <li>How do we access individual threads in the kernel?</li> </ul> <p>Go to exercise 01_01</p>"},{"location":"czangela-tutorial/weeks/week01_material/#4-thread-hierarchy","title":"4. Thread hierarchy","text":"<p>A kernel is executed in parallel by an array of threads:</p> <ul> <li>All threads run the same code.</li> <li>Each thread has an ID that it uses to compute memory addresses and make control decisions.</li> </ul> <p></p> <p>Threads are arranged as a grid of thread blocks:</p> <ul> <li>Different kernels can have different grid/block configuration</li> <li>Threads from the same block have access to a shared memory and their execution can be synchronized</li> </ul> <p></p> <p>Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series.</p> <p>This independence requirement allows thread blocks to be scheduled in any order across any number of cores, enabling programmers to write code that scales with the number of cores.</p> <p>Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses.</p> <p>The grid of blocks and the thread blocks can be 1, 2, or 3-dimensional.</p> <p></p> <p>The CUDA architecture is built around a scalable array of multithreaded Streaming Multiprocessors (SMs) as shown below.</p> <p>Each SM has a set of execution units, a set of registers and a chunk of shared memory.</p> <p></p>"},{"location":"czangela-tutorial/weeks/week01_material/#5-language-extensions","title":"5. Language extensions","text":"<p>From CUDA Toolkit Documentation: Language Extensions:</p>"},{"location":"czangela-tutorial/weeks/week01_material/#__global__","title":"<code>__global__</code>","text":"<p>The global execution space specifier declares a function as being a kernel. Such a function is:</p> <ul> <li>Executed on the device,</li> <li>Callable from the host,</li> <li>Callable from the device for devices of compute capability 3.2 or higher (see CUDA Dynamic Parallelism for more details). A global function must have void return type, and cannot be a member of a class.</li> </ul> <p>Any call to a global function must specify its execution configuration as described in Execution.</p> <p>A call to a global function is asynchronous, meaning it returns before the device has completed its execution.</p>"},{"location":"czangela-tutorial/weeks/week01_material/#__device__","title":"<code>__device__</code>","text":"<p>The device execution space specifier declares a function that is:</p> <ul> <li>Executed on the device,</li> <li>Callable from the device only.</li> </ul> <p>The global and device execution space specifiers cannot be used together.</p>"},{"location":"czangela-tutorial/weeks/week01_material/#__host__","title":"<code>__host__</code>","text":"<p>The host execution space specifier declares a function that is:</p> <ul> <li>Executed on the host,</li> <li>Callable from the host only.</li> </ul> <p>It is equivalent to declare a function with only the host execution space specifier or to declare it without any of the host, device, or global execution space specifier; in either case the function is compiled for the host only.</p> <p>The global and host execution space specifiers cannot be used together.</p> <p>The device and host execution space specifiers can be used together however, in which case the function is compiled for both the host and the device.</p> <p>02. Question: host and device functions</p> <ul> <li> <p>Give an example of <code>global</code>, <code>device</code> and <code>host-device</code> functions in <code>CMSSW</code>.</p> </li> <li> <p>Can you find an example where <code>host</code> and <code>device</code> code diverge? How is this achieved?</p> </li> </ul> <p>Go to exercise 01_02</p> <p>03. Exercise: Write a kernel in which</p> <ul> <li> <p>if we're running on the <code>device</code> each thread prints which <code>block</code> and <code>thread</code> it is associated with, for example <code>block 1 thread 3</code></p> </li> <li> <p>if we're running on the <code>host</code> each thread just prints <code>host</code>.</p> </li> </ul> <p>Go to exercise 01_03</p>"},{"location":"czangela-tutorial/weeks/week01_material/#6-execution-configuration","title":"6. Execution Configuration","text":"<p>From CUDA Toolkit Documentation: Execution Configuration</p> <p>Any call to a global function must specify the execution configuration for that call. The execution configuration defines the dimension of the grid and blocks that will be used to execute the function on the device, as well as the associated stream (see CUDA Runtime for a description of streams).</p> <p>The execution configuration is specified by inserting an expression of the form <code>&lt;&lt;&lt; Dg, Db, Ns, S &gt;&gt;&gt;</code> between the function name and the parenthesized argument list, where:</p> <ul> <li> <p><code>Dg</code> is of type dim3 (see dim3) and specifies the dimension and size of the grid, such that <code>Dg.x * Dg.y * Dg.z</code> equals the number of blocks being launched;</p> </li> <li> <p><code>Db</code> is of type dim3 (see dim3) and specifies the dimension and size of each block, such that <code>Db.x * Db.y * Db.z</code> equals the number of threads per block;</p> </li> <li> <p><code>Ns</code> is of type <code>size_t</code> and specifies the number of bytes in shared memory that is dynamically allocated per block for this call in addition to the statically allocated memory; this dynamically allocated memory is used by any of the variables declared as an external array as mentioned in shared; Ns is an optional argument which defaults to 0;</p> </li> <li> <p><code>S</code> is of type <code>cudaStream_t</code> and specifies the associated stream; S is an optional argument which defaults to 0.</p> </li> </ul>"},{"location":"czangela-tutorial/weeks/week01_material/#abstractions-granularity","title":"Abstractions: Granularity","text":"Granularity <p>If  is the computation time and  denotes the communication time, then the Granularity G of a task can be calculated as</p> <p></p> <p>Granularity is usually measured in terms of the number of instructions executed in a particular task.</p> Fine-grained parallelism <p>Fine-grained parallelism means individual tasks are relatively small in terms of code size and execution time. The data is transferred among processors frequently in amounts of one or a few memory words.</p> Coarse-grained parallelism <p>Coarse-grained is the opposite in the sense that data is communicated infrequently, after larger amounts of computation.</p> <p></p> <p>The CUDA abstractions provide fine-grained data parallelism and thread parallelism, nested within coarse-grained data parallelism and task parallelism. They guide the programmer to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into finer pieces that can be solved cooperatively in parallel by all threads within the block.</p> <p>This decomposition preserves language expressivity by allowing threads to cooperate when solving each sub-problem, and at the same time enables automatic scalability.</p> <p>Indeed, each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a compiled CUDA program can execute on any number of multiprocessors.</p> <p></p> <p>The following exercise requires knowledge about barrier synchronization and shared memory.</p> <p>Follow-up on <code>__syncthreads()</code> and shared memory.</p> <p>04. Exercise: Fine-grained vs coarse-grained parallelism</p> <ul> <li>Give examples in the <code>MatMulKernel</code> kernel of coarse-grained and fine-grained data parallelism (as defined in CUDA abstraction model) as well as sequential execution.</li> </ul> <p>Go to exercise 01_04</p>"},{"location":"czangela-tutorial/weeks/week02_exercises/","title":"Week 02 Exercises","text":"<p>Follow exercises from Patatrack Knowledge Transfer part 1</p>"},{"location":"czangela-tutorial/weeks/week02_material/","title":"Week 02 Reading Material","text":"Resources <ul> <li> <p>CUDA C++ Programming Guide</p> </li> <li> <p>Introduction to GPUs by New York University</p> </li> <li> <p>NVidia Developer Blog: Using Shared Memory CUDA C/C++</p> </li> </ul> <p>In the previous material in A Scalable Programming Model we've been reading about the three key abstractions in the CUDA programming model:</p> <ul> <li>a hierarchy of thread groups</li> <li>shared memories</li> <li>barrier synchronization</li> </ul> <p>Thread hierarchy has been previously covered, and in this part shared memory and barrier synchronization follows.</p>"},{"location":"czangela-tutorial/weeks/week02_material/#1-shared-memory","title":"1. Shared memory","text":""},{"location":"czangela-tutorial/weeks/week02_material/#memory-hierarchy","title":"Memory hierarchy","text":"<p>CUDA threads may access data from multiple memory spaces during their execution.</p> <p>Each thread has private local memory.</p> <p>Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block.</p> <p>All threads have access to the same global memory.</p> <p></p> <p>There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces, which we won't cover in detail here. For more information continue reading here.</p> <p>The global, constant, and texture memory spaces are persistent across kernel launches by the same application.</p>"},{"location":"czangela-tutorial/weeks/week02_material/#__shared__","title":"<code>__shared__</code>","text":"<p>As detailed in Variable Memory Space Specifiers shared memory is allocated using the <code>__shared__</code>    memory space specifier.</p> <p>Shared memory is expected to be much faster than global memory.</p>"},{"location":"czangela-tutorial/weeks/week02_material/#static-and-dynamic","title":"Static and dynamic","text":"<p>Example from: https://github.com/NVIDIA-developer-blog/code-samples/blob/master/series/cuda-cpp/shared-memory/shared-memory.cu</p> <p>Usage of static and dynamic memory</p> copyrightstaticReversedynamicReversemain <pre><code>/* Copyright (c) 1993-2015, NVIDIA CORPORATION. All rights reserved.\n*\n* Redistribution and use in source and binary forms, with or without\n* modification, are permitted provided that the following conditions\n* are met:\n*  * Redistributions of source code must retain the above copyright\n*    notice, this list of conditions and the following disclaimer.\n*  * Redistributions in binary form must reproduce the above copyright\n*    notice, this list of conditions and the following disclaimer in the\n*    documentation and/or other materials provided with the distribution.\n*  * Neither the name of NVIDIA CORPORATION nor the names of its\n*    contributors may be used to endorse or promote products derived\n*    from this software without specific prior written permission.\n*\n* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n* EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n* IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n* PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n* CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n* EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n* PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n* PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n* OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n* (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n*/\n</code></pre> <pre><code>#include &lt;stdio.h&gt;\n\n__global__ void staticReverse(int *d, int n)\n{\n  __shared__ int s[64];\n  int t = threadIdx.x;\n  int tr = n-t-1;\n  s[t] = d[t];\n  __syncthreads();\n  d[t] = s[tr];\n}\n</code></pre> <pre><code>__global__ void dynamicReverse(int *d, int n)\n{\n  extern __shared__ int s[];\n  int t = threadIdx.x;\n  int tr = n-t-1;\n  s[t] = d[t];\n  __syncthreads();\n  d[t] = s[tr];\n}\n</code></pre> <pre><code>int main(void)\n{\n  const int n = 64;\n  int a[n], r[n], d[n];\n\n  for (int i = 0; i &lt; n; i++) {\n    a[i] = i;\n    r[i] = n-i-1;\n    d[i] = 0;\n  }\n\n  int *d_d;\n  cudaMalloc(&amp;d_d, n * sizeof(int)); \n\n  // run version with static shared memory\n  cudaMemcpy(d_d, a, n*sizeof(int), cudaMemcpyHostToDevice);\n  staticReverse&lt;&lt;&lt;1,n&gt;&gt;&gt;(d_d, n);\n  cudaMemcpy(d, d_d, n*sizeof(int), cudaMemcpyDeviceToHost);\n  for (int i = 0; i &lt; n; i++) \n    if (d[i] != r[i]) printf(\"Error: d[%d]!=r[%d] (%d, %d)\\n\", i, i, d[i], r[i]);\n\n  // run dynamic shared memory version\n  cudaMemcpy(d_d, a, n*sizeof(int), cudaMemcpyHostToDevice);\n  dynamicReverse&lt;&lt;&lt;1,n,n*sizeof(int)&gt;&gt;&gt;(d_d, n);\n  cudaMemcpy(d, d_d, n * sizeof(int), cudaMemcpyDeviceToHost);\n  for (int i = 0; i &lt; n; i++) \n    if (d[i] != r[i]) printf(\"Error: d[%d]!=r[%d] (%d, %d)\\n\", i, i, d[i], r[i]);\n}\n</code></pre> <p>If the shared memory array size is known at compile time, as in the staticReverse kernel, then we can explicitly declare an array of that size, as we do with the array <code>s</code>.</p> <pre><code>__global__ void staticReverse(int *d, int n)\n{\n  __shared__ int s[64];\n  int t = threadIdx.x;\n  int tr = n-t-1;\n  s[t] = d[t];\n  __syncthreads();\n  d[t] = s[tr];\n}\n</code></pre> <p>The other three kernels in this example use dynamically allocated shared memory, which can be used when the amount of shared memory is not known at compile time. In this case the shared memory allocation size per thread block must be specified (in bytes) using an optional third execution configuration parameter, as in the following excerpt.</p> <pre><code>dynamicReverse&lt;&lt;&lt;1, n, n*sizeof(int)&gt;&gt;&gt;(d_d, n);\n</code></pre> <p>The dynamic shared memory kernel, dynamicReverse(), declares the shared memory array using an unsized extern array syntax, <code>extern __shared__ int s[]</code>.</p> <pre><code>__global__ void dynamicReverse(int *d, int n)\n{\n  extern __shared__ int s[];\n  int t = threadIdx.x;\n  int tr = n-t-1;\n  s[t] = d[t];\n  __syncthreads();\n  d[t] = s[tr];\n}\n</code></pre>"},{"location":"czangela-tutorial/weeks/week02_material/#2-barrier-synchronization","title":"2. Barrier synchronization","text":""},{"location":"czangela-tutorial/weeks/week02_material/#introduction","title":"Introduction","text":"<p>When sharing data between threads, we need to be careful to avoid race conditions, because while threads in a block run logically in parallel, not all threads can execute physically at the same time.</p>"},{"location":"czangela-tutorial/weeks/week02_material/#synchronize-block-of-threads","title":"Synchronize block of threads","text":"<p>Let\u2019s say that two threads A and B each load a data element from global memory and store it to shared memory. Then, thread A wants to read B\u2019s element from shared memory, and vice versa. Let\u2019s assume that A and B are threads in two different warps. If B has not finished writing its element before A tries to read it, we have a race condition, which can lead to undefined behavior and incorrect results.</p> <p>To ensure correct results when parallel threads cooperate, we must synchronize the threads. CUDA provides a simple barrier synchronization primitive, <code>__syncthreads()</code>. A thread\u2019s execution can only proceed past a <code>__syncthreads()</code> after all threads in its block have executed the <code>__syncthreads()</code>. Thus, we can avoid the race condition described above by calling <code>__syncthreads()</code> after the store to shared memory and before any threads load from shared memory.</p>"},{"location":"czangela-tutorial/weeks/week02_material/#deadlocks","title":"Deadlocks","text":"<p>barrier synchronization</p> deadlockcorrect barrier <pre><code>int s = threadIdx.x / 2;\nif (threadIdx.x &gt; s) {\n    value[threadIdx.x] = 2*s;\n    __syncthreads();\n}\nelse {\n    value[threadIdx.x] = s;\n    __syncthreads();\n}\n</code></pre> <pre><code>int s = threadIdx.x / 2;\nif (threadIdx.x &gt; s) {\n    value[threadIdx.x] = 2*s;\n}\nelse {\n    value[threadIdx.x] = s;\n}\n__syncthreads();\n</code></pre> <p>It\u2019s important to be aware that calling <code>__syncthreads()</code> in divergent code is undefined and can lead to deadlock\u2014all threads within a thread block must call <code>__syncthreads()</code> at the same point.</p>"},{"location":"czangela-tutorial/weeks/week02_material/#3-page-locked-host-memory","title":"3. Page-Locked Host Memory","text":"<p>From the CUDA Programming Guide</p> <p>The runtime provides functions to allow the use of page-locked (also known as pinned) host memory (as opposed to regular pageable host memory allocated by <code>malloc()</code>):</p> <ul> <li><code>cudaHostAlloc()</code> and <code>cudaFreeHost()</code> allocate and free page-locked host memory;</li> <li><code>cudaHostRegister()</code> page-locks a range of memory allocated by <code>malloc()</code> (see reference manual for limitations).</li> </ul> <p>Using page-locked host memory has several benefits:</p> <ul> <li>Copies between page-locked host memory and device memory can be performed concurrently with kernel execution for some devices as mentioned in Asynchronous Concurrent Execution.</li> <li>On some devices, page-locked host memory can be mapped into the address space of the device, eliminating the need to copy it to or from device memory as detailed in Mapped Memory.</li> <li>On systems with a front-side bus, bandwidth between host memory and device memory is higher if host memory is allocated as page-locked and even higher if in addition it is allocated as write-combining as described in Write-Combining Memory.</li> </ul> <p>Explaining pinned memory:</p> <p>Setting flags for <code>cudaHostAlloc</code>:</p>"},{"location":"czangela-tutorial/weeks/week03_material/","title":"Week 3 and beyond","text":"<p>Further reading: GPU code execution performance evaluation, parallel programming models</p>"},{"location":"general/cmssw/extra-cmssw/","title":"CMSSW Resources","text":""},{"location":"general/cmssw/extra-cmssw/#doxygen-reference-manual-automated-not-really-helpful","title":"Doxygen Reference Manual (Automated, not really helpful)","text":"<p>Here.</p>"},{"location":"general/cmssw/extra-cmssw/#dxr-source-browser","title":"DXR source browser","text":"<p>Useful for browsing the CMSSW source, looking up definitions and finding callees of variables and functions.</p> <p>Here. </p>"},{"location":"general/cmssw/extra-cmssw/#tutorials","title":"Tutorials","text":""},{"location":"general/cmssw/extra-cmssw/#cms-opendata-workshop","title":"CMS OpenData Workshop","text":"<p>Note</p> <p>Mainly targeted at physicists.</p> <p>Basic CMSSW operation and concepts: here</p>"},{"location":"general/development/versioning/","title":"Code versioning","text":"<p>As of writing, Semantic Versioning is used for projects like CertHelper. </p> <p>In summary:</p> <ul> <li>Versions take the form: <code>MAJOR.MINOR.PATCH</code>.</li> <li>Small changes which fix functionality which is already in place increment the <code>PATCH</code> part.</li> <li>Changes which add new features which did not exist before increment the <code>MINOR</code> part. Incrementing it should reset <code>PATCH</code> to <code>0</code>, e.g. <code>1.0.4</code> --&gt; <code>1.1.0</code>.</li> <li>Making changes to stuff that break backwards compatibility increments the <code>MAJOR</code> part, e.g. from <code>1.0.3</code> to <code>2.0.0</code>. Incrementing the <code>MAJOR</code> version resets both <code>MINOR</code> and <code>PATCH</code>.</li> </ul>"},{"location":"general/django/overview/","title":"Django Resources","text":"<p>Django is a web framework built in python. It allows you to create web apps from scratch, with a plethora of extensions available.</p>"},{"location":"general/django/overview/#tutorials","title":"Tutorials","text":"<ol> <li>Django Girls: A basic and     straightforward tutorial for beginners, starting from the very basics    (i.e. what is a terminal, what is python etc.) You might want to skip    directly to the Django introduction.</li> <li>The official Django tutorial.</li> </ol>"},{"location":"general/django/database/exporting_importing/","title":"Exporting/Importing data","text":"<p>Using the built-in Django management commands, one can dump or load data directly from JSON or YAML files.</p>"},{"location":"general/django/database/exporting_importing/#exporting","title":"Exporting","text":"<p>To dump data in JSON format:</p> <pre><code>python3 manage.py dumpdata &lt;specific table or app name&gt; &gt; filename.json\n</code></pre> <p>For example, for dumping all the <code>remotescripts</code> configuration of CertHelper:</p> <pre><code>python3 manage.py dumpdata remotescripts &gt; remotescripts.json\n</code></pre>"},{"location":"general/django/database/exporting_importing/#importing-data","title":"Importing data","text":"<p>To import data from a JSON file:</p> <pre><code>python3 manage.py loaddata &lt;json file&gt;\n</code></pre> <p>e.g.:</p> <pre><code>python3 manage.py loaddata remotescripts.json\n</code></pre>"},{"location":"general/django/setup/overview/","title":"Setting up your Django project locally","text":"<p>Warning</p> <p>This guide is intended for a Linux-based OS</p>"},{"location":"general/django/setup/overview/#procedure","title":"Procedure","text":"<ol> <li><code>git clone</code> your django project locally.</li> <li>Verify that Python3 is installed on your system (ideally version&gt;=3.8).</li> <li>Create a database:<ul> <li>If using an SQlite3 database (Django's default), it will be automatically created for you by Django in the following steps.</li> <li>If using PostgreSQL:<ul> <li>Install PostgreSQL natively (e.g. for Ubuntu, instructions here).</li> <li>Add a password for the <code>postgres</code> user as per the instructions above.</li> <li>Create the database:<ul> <li>Connect: <code>psql -U postgres</code></li> <li><code>CREATE DATABASE &lt;database name&gt;;</code> where <code>&lt;database name&gt;</code> is the database name specified in Django's settings, or <code>.env</code> file, e.g. <code>mlplayground_development</code>.</li> </ul> </li> <li>[Optional] Install pgadmin4 for administering it.</li> </ul> </li> </ul> </li> <li>A virtual environment. Example setup:<ol> <li><code>cd</code> to your project's root.</li> <li><code>python3 -m venv venv</code></li> </ol> </li> <li>Activate the virtual environment:<ul> <li><code>source venv/bin/activate</code> </li> </ul> </li> <li>Install requirements:<ul> <li><code>python -m pip install -r requirements.txt</code> (once the venv is activated you don't need to specifically run <code>python3</code>, just <code>python</code>).</li> </ul> </li> <li>Make sure you have an <code>.env</code> file with all the required project variables in your project's root.</li> <li>Create the database structure:<ul> <li><code>python manage.py migrate --run-syncdb</code></li> </ul> </li> <li>Create a superuser:<ul> <li><code>python manage.py createsuperuser</code></li> </ul> </li> <li>Run the developement server: <code>python3 manage.py runserver</code>. By default, you should be able to access http://localhost:8000.<ul> <li>If you want to run with HTTPS:<ul> <li><code>pip install Werkzeug</code></li> <li><code>python manage.py runserver_plus --cert cert</code></li> </ul> </li> </ul> </li> </ol> <p>Why PostgreSQL?</p> <p>PostgreSQL supports features that are unavailable to other databases, such as <code>ArrayFields</code>.</p> <p>Logging in</p> <p>When running the project locally, CERN SSO will not be functioning by default. One way to login is to use the superuser you created to login to the app. If you need to test the SSO, make sure that:</p> <ul> <li>You have added the correct Redirect URI in the SSO registrion in the Application portal (i.e. <code>https://localhost:8000/accounts/cern/login/callback/</code>)  </li> <li>Run the local server with HTTPS just in case.</li> </ul>"},{"location":"general/django/setup/overview/#usual-workflow","title":"Usual workflow","text":"<p>The setup procedure needs to be followed only once, usually. For usual developement, you will only need to run steps 5 and 10.</p>"},{"location":"general/gpu/cern-resources/","title":"CERN GPU Resources","text":"<p>See the CERN cloud insfrastructure resources guide on how to request GPU resources.</p>"},{"location":"general/gpu/extra-gpu/","title":"CUDA/GPU Programming","text":"<p>External documentation, tutorials and courses related to GPU developing with CUDA.</p>"},{"location":"general/gpu/extra-gpu/#patatrack","title":"Patatrack","text":"<p>Patatrack Wiki.</p> <p>Patatrack Website.</p>"},{"location":"general/gpu/extra-gpu/#courses","title":"Courses","text":""},{"location":"general/gpu/extra-gpu/#caltech-gpu-programming-cs-179","title":"Caltech - GPU Programming [CS 179]","text":"<p>Caltech course</p>"},{"location":"general/gpu/extra-gpu/#presentations-and-introductory-material","title":"Presentations and introductory material","text":"<ol> <li>Introduction to GPUs by NYU</li> <li>Previous Patatrack Knowledge transfer</li> <li>Direct link to Welcome and Introduction to Parallel Programming</li> <li>Direck link to Introduction to GPU programming using CUDA</li> <li>Direct link to SoA model for Pixel Reconstruction (and beyond?)</li> <li>Direct link to A fully Heterogeneous Pixel Reconstruction</li> </ol>"},{"location":"general/gpu/extra-gpu/#hands-on-training","title":"Hands-on training","text":""},{"location":"general/gpu/extra-gpu/#nvidia-cuda-workshop","title":"NVIDIA CUDA Workshop","text":"<p>An official NVIDIA introductory workshop.</p> <p>Warning</p> <p>This requires that you have access to a NVIDIA GPU-capable computer with <code>nvcc</code> and <code>ncu</code> installed.</p> <p>Tip</p> <p><code>ncu</code> is only supported by a limited GPUs as of writing (2024/08). You can replace all <code>ncu</code> calls you see in the notebooks with <code>nvprof</code> if <code>ncu</code> does not work for your card.</p> <ul> <li>Download and extract this file.</li> <li>Create a <code>python3</code> virtual environment and activate it.</li> <li>Run <code>pip install jupyterlab</code>.</li> <li>Run <code>jupyter lab</code>.</li> </ul>"},{"location":"general/gpu/extra-gpu/#angelas-gpu-programming-primer","title":"Angela's GPU programming primer","text":"<p>Follow the material found here.</p>"},{"location":"general/gpu/extra-gpu/#patatrack_1","title":"Patatrack","text":"<p>Solve all exercises here https://patatrack.web.cern.ch/patatrack/wiki/cuda_training_dpg_12_2019/.</p> <p>Part 1</p> <p>Part 2</p> <p>Part 3</p>"},{"location":"general/gpu/extra-gpu/#cuda-algorithms-in-cmssw-documentation","title":"CUDA algorithms in CMSSW documentation","text":"<p>CUDA algorithms in CMSSW documentation image</p>"},{"location":"general/gpu/extra-gpu/#cuda-programming-guide","title":"CUDA Programming Guide","text":"<p>nvidia CUDA Documentation</p>"},{"location":"general/openshift/resources/","title":"Application Resources Limits","text":"<p>Each OpenShift application has a limited amount of resources it is given access to. Exceeding these leads to spawned pods being killed (You might see something along the lines of <code>OOMKilled</code>).</p> <p>In such a case, you can edit the application's resource limits. The PaaS guide for tweaking those limits can be found here.</p> <p>Warning</p> <p>By default, the limits are...limited. See here for details. </p> <p>You may request an increase by submitting a ticket  here.</p>"},{"location":"general/openshift/routes/","title":"Openshift Routes","text":""},{"location":"general/openshift/routes/#configuring-the-gateway-timeout","title":"Configuring the gateway timeout","text":"<p>Long-running functions that return a response from a web-app to a client are generally not recommended<sup>1</sup>. However, in some cases this might prove necessary.</p> <p>To do so in Openshift, locate the app's Routes (e.g. for CertHelper those are here). To set the <code>haproxy</code>'s (which handles the routing) timeout, add an  annotation to the Route:</p> <ol> <li> <p>Click on Edit annotations:</p> <p></p> </li> <li> <p>Add a new key named <code>haproxy.router.openshift.io/timeout</code> with value the required timeout in either seconds or minutes, e.g. <code>5m</code>:</p> <p></p> </li> </ol> <ol> <li> <p>If a server takes too long to respond, this usually raises a <code>504</code> HTTP error.\u00a0\u21a9</p> </li> </ol>"},{"location":"general/ssh/mounting_remote/","title":"Mounting remote locations via SSH","text":"<p>Source<sup>1</sup></p>"},{"location":"general/ssh/mounting_remote/#installation","title":"Installation","text":"<ul> <li><code>sudo apt install sshfs</code></li> </ul>"},{"location":"general/ssh/mounting_remote/#mounting","title":"Mounting","text":"<ul> <li>Make a mountpoint for the remote filesystem:   <pre><code>mkdir ~/lxplus\n</code></pre></li> <li>Mount it:   <pre><code>sshfs &lt;username&gt;@lxplus.cern.ch:/afs/cern.ch/user/&lt;username's first letter&gt;/&lt;username/ ~/lxplus\n</code></pre></li> </ul> <ol> <li> <p>Thx \u54c8\u62c9 \u21a9</p> </li> </ol>"},{"location":"general/ssh/tunnel/","title":"Creating a Tunnel","text":"<p>When not in CERN, to access resources which are unavailable outside CERN, you will need to tunnel your computer's network through SSH. </p> <p>Sournce.</p>"},{"location":"general/ssh/tunnel/#linux","title":"Linux","text":"<pre><code>python3 -m pip install sshuttle\nsshuttle --dns -vr &lt;username&gt;@lxtunnel.cern.ch 137.138.0.0/16 128.141.0.0/16 128.142.0.0/16 188.184.0.0/15 --python=python3\n</code></pre>"},{"location":"mlplayground/config/","title":"Project configuration","text":"<p>Parameters which can be configured for this project.</p>"},{"location":"mlplayground/config/#environmental-variables","title":"Environmental variables","text":""},{"location":"mlplayground/config/#django_secret_key","title":"<code>DJANGO_SECRET_KEY</code>","text":""},{"location":"mlplayground/config/#dir_path_dqmio_storage","title":"<code>DIR_PATH_DQMIO_STORAGE</code>","text":"<p>Path (usually to EOS) where DQM files are to be found.  This directory is scanned for valid DQM files which are then available to parse histograms from.</p> <p>If running locally, this can be changed to whichever directory you have sample DQM files stored.</p> <p>See here.</p>"},{"location":"mlplayground/config/#django_database_name","title":"<code>DJANGO_DATABASE_NAME</code>","text":""},{"location":"mlplayground/config/#django_database_user","title":"<code>DJANGO_DATABASE_USER</code>","text":""},{"location":"mlplayground/config/#django_database_password","title":"<code>DJANGO_DATABASE_PASSWORD</code>","text":""},{"location":"mlplayground/config/#django_database_host","title":"<code>DJANGO_DATABASE_HOST</code>","text":""},{"location":"mlplayground/config/#django_database_port","title":"<code>DJANGO_DATABASE_PORT</code>","text":""},{"location":"mlplayground/config/#django_allowed_hosts","title":"<code>DJANGO_ALLOWED_HOSTS</code>","text":""},{"location":"mlplayground/config/#site_id","title":"<code>SITE_ID</code>","text":""},{"location":"mlplayground/config/#cern_sso_registration_client_id","title":"<code>CERN_SSO_REGISTRATION_CLIENT_ID</code>","text":"<p>You can get this by registering the application in the <code>Application Portal</code> or reusing </p>"},{"location":"mlplayground/config/#cern_sso_registration_client_secret","title":"<code>CERN_SSO_REGISTRATION_CLIENT_SECRET</code>","text":"<p>Same with <code>CERN_SSO_REGISTRATION_CLIENT_ID</code>.</p>"},{"location":"mlplayground/connectivity/","title":"Connectivity","text":"<pre><code>flowchart LR\n    de[Data Engineering] &lt;-- Socket --&gt; ds[Data Science]\n    db[(Database)] &lt;--&gt; de</code></pre>"},{"location":"mlplayground/connectivity/#kedro-pipeline-trigger","title":"Kedro pipeline trigger","text":"<p>The <code>dqm-playground-ds</code> deployment runs using a wrapper script (<code>entrypoint.py</code>) which listens on port 8888 for incoming socket clients.</p> <p>A connected client can send commands to this server, which can trigger kedro pipelines.</p> <p>This server can be accessed (from within the PaaS cluster only) using the hostname provided by PaaS' Services.</p> <p>For example, to test the interconnection between deployments on PaaS, one can execute from a <code>ml-playground</code> pod terminal:</p> <pre><code>curl dqm-playground-ds:8888\n</code></pre> <p>To verify that a response is received and the port is open.</p>"},{"location":"mlplayground/docs/","title":"Available Documentation","text":""},{"location":"mlplayground/docs/#user-documentation","title":"User documentation","text":"<p>Not available yet </p>"},{"location":"mlplayground/docs/#dev-documentation","title":"Dev documentation","text":"<p>You're looking at it!</p>"},{"location":"mlplayground/faq/","title":"FAQ","text":""},{"location":"mlplayground/faq/#database-management","title":"Database management","text":""},{"location":"mlplayground/faq/#how-do-i-dump-and-recreate-a-list-of-dqm-files","title":"How do I dump and recreate a list of DQM files?","text":"<p>The easiest way is to just delete all <code>HistogramDataFiles</code> instances. To do so,  you will first need to delete all data associated with these <code>HistogramDataFiles</code>. For example, for <code>LumisectionHistogram2D</code>:</p> <ul> <li>Connect to the database via <code>psql</code> or pgadmin.</li> <li><code>DELETE FROM public.histograms_lumisectionhistogram2d;</code> will delete   all 2D Lumisection histogram data.</li> <li><code>DELETE FROM public.historgam_file_manager where dimensionality=2</code> will   delete all DQM file entries which contain 2D Lumisection histogram data.</li> <li>Exit <code>psql</code>.</li> <li>Run <code>python manage.py discover_dqm_files</code> which will recreate entries for the deleted DQM files.</li> </ul>"},{"location":"mlplayground/faq/#deployment","title":"Deployment","text":""},{"location":"mlplayground/faq/#i-started-a-new-build-on-paas-but-it-is-stuck-at-0-pods-scaling-to-1-whats-wrong","title":"I started a new build on PaaS but it is stuck at <code>0 pods scaling to 1</code>. What's wrong?","text":"<p>This usually happens if the resources set for the deployment are higher than the maximum available. This should not happen, if the limits are kept under the max ones for the MLPlayground app. If, however, this happens, you could either:</p> <ul> <li>Wait it out for a couple of minutes.</li> <li>[MOST PROBABLE SOLUTION] Try scaling the pods down to 0 and up to 1 again.</li> <li>Set the limits to the default ones (e.g. <code>cpu: '1'</code> and <code>memory: '2Gi'</code>), and re-raise them.</li> </ul>"},{"location":"mlplayground/introduction/","title":"Introduction","text":"<p>Todo</p> <p>TODO</p> <p></p>"},{"location":"mlplayground/introduction/#purpose","title":"Purpose","text":"<p>This is a Django project aimed at:</p> <ul> <li>Managing the storage of CMS data:<ul> <li>Run and Lumisection Information.</li> <li>Run and Lumisection Histograms.</li> <li>Run and Lumisection Certification.</li> </ul> </li> <li>Providing an API for serving this data to any user that requires it.<ul> <li>Storing datasets used by users to train algorithms.</li> <li>Storing parameters and results of said algorithms.</li> </ul> </li> </ul>"},{"location":"mlplayground/introduction/#deployment-on-openshift","title":"Deployment on Openshift","text":"<p>The app is deployed here. For more information on its deployment, go here.</p>"},{"location":"mlplayground/introduction/#deploying-locally","title":"Deploying locally","text":"<p>Basic instructions for Django projects can be found here.</p>"},{"location":"mlplayground/introduction/#composition","title":"Composition","text":"<p>The project is comprised of the Data Engineering part (DE) and the Data Science part (DS). Each one has its own repository:</p> <ul> <li>Data Engineering repository</li> <li>Data Science repository</li> </ul>"},{"location":"mlplayground/apps/histogram_file_manager/api/","title":"Histogram File Manager API","text":"<p>Documentation on the <code>api</code> folder in the <code>histogram_file_manager</code> app. </p>"},{"location":"mlplayground/apps/histogram_file_manager/api/#viewsets","title":"Viewsets","text":""},{"location":"mlplayground/apps/histogram_file_manager/api/#histogramdatafileviewset","title":"<code>HistogramDataFileViewSet</code>","text":"<p>This ViewSet provides the Views of the API to the <code>HistogramDataFile</code> model, i.e. the endpoints which list the available instances of the aforementioned model.</p> <p>Info</p> <p>To reduce the load exerted on the DB each time a full list of <code>HistogramDataFiles</code> are requested, the responses are cached for 60 seconds (see <code>histogram_file_manager/api/viewsets.py</code>). Any delays in getting fresh data on the available files is due to this caching.</p>"},{"location":"mlplayground/apps/histogram_file_manager/api/#start_parsing","title":"<code>start_parsing</code>","text":"<p>A custom endpoint action which, given the <code>id</code> of the <code>HistogramDataFile</code> and the appropriate information, calls the appropriate parsing function to extract the contents of the file.</p> <p>The function makes use of the defined <code>HISTOGRAM_PARSING_FUNCTIONS_MAP</code> dictionary which is a dictionary  mapping each combination of filetype, dimensionality and granularity to the appropriate parsing function, located in the respective model instances that the method will create.</p> <p>Example</p> <p>For filetype <code>csv</code>, dimensionality <code>2</code> (2D data) and granularity <code>lum</code> (Lumisection), the appropriate function is: <code>histograms.models.LumisectionHistogram2D.from_csv</code>.</p> <p>Adding methods for new file types or products</p> <p>Expanding the <code>HISTOGRAM_PARSING_FUNCTIONS_MAP</code> should be straightforward  once the parsing functions are implemented. They just need to be placed as values under the appropriate keys of the dictionary.</p> <p>E.g. For a <code>test</code> file type, with data dimensionality <code>2</code> and granularity <code>lum</code>, the dictionary should be expanded as follows: <pre><code>HISTOGRAM_PARSING_FUNCTIONS_MAP = {\n    HistogramDataFile.FILETYPE_CSV: {\n        HistogramDataFile.DIMENSIONALITY_1D: {\n            HistogramDataFile.GRANULARITY_LUMISECTION: LumisectionHistogram1D.from_csv\n        },\n        HistogramDataFile.DIMENSIONALITY_2D: {\n            HistogramDataFile.GRANULARITY_LUMISECTION: LumisectionHistogram2D.from_csv\n        },\n    },\n    # New filetype defined below\n    HistogramDataFile.FILETYPE_TEST: {\n        HistogramDataFile.DIMENSIONALITY_2D: {\n            HistogramDataFile.GRANULARITY_LUMISECTION: LumisectionHistogram2D.from_test\n        },\n    }       \n}   \n</code></pre> In this example, you should also define <code>FILETYPE_TEST</code> in the <code>HistogramDataFile</code> model.</p> <p>An depiction of the flow described above can be found below:</p> <pre><code>flowchart LR\n    api[&lt;strong&gt;API Call&lt;/strong&gt;\\n- File ID\\n- File extension\\n- Granularity\\n- Dimensions] -- &lt;tt&gt;GET&lt;/tt&gt; --&gt; hfm[\"&lt;strong&gt;histogram_file_manager/api/viewsets.py&lt;/strong&gt;\\n&lt;tt&gt;start_parsing()&lt;/tt&gt;\"]\n    hfm -- &lt;tt&gt;HISTOGRAM_PARSING_FUNCTIONS_MAP&lt;/tt&gt; --&gt; hm[\"&lt;strong&gt;histograms/models.py&lt;/strong&gt;\\n&lt;tt&gt;from_csv()&lt;/tt&gt;\\nfrom_nanodqm()&lt;/tt&gt;\"]\n\n    style api fill:#B74AA545,stroke:#333,stroke-width:4px   </code></pre>"},{"location":"mlplayground/apps/histogram_file_manager/management/","title":"Management commands","text":"<p>Note</p> <p>Run these commands on a PaaS terminal.</p>"},{"location":"mlplayground/apps/histogram_file_manager/management/#discover_dqm_files","title":"<code>discover_dqm_files</code>","text":"<p>Location: <code>histogram_data_files/management/commands/disover_dqm_files.py</code></p> <p>A django management command which is responsible for searching through the directory pointed to by the <code>DIR_PATH_DQMIO_STORAGE</code> environment variable, keeping only those with a valid file extension.</p> <p>For each valid file found, a new entry in the database is created. </p> <p>Note</p> <p>This procedure does not parse the files' contents, it just creates an entry for the files themselves in the database. </p> <p>The parsing is handled by the appropriate model methods found in the <code>histograms/models.py</code> file and is invoked via the API.</p>"},{"location":"mlplayground/apps/histogram_file_manager/models/","title":"Models","text":"<p>This app only provides the <code>HistogramDataFile</code> model.</p>"},{"location":"mlplayground/apps/histogram_file_manager/models/#histogramdatafile","title":"<code>HistogramDataFile</code>","text":"<p>This is a model which describes some generic attributes of the histogram data files, such as:</p> <ul> <li>The <code>filepath</code> where they're located.</li> <li>Their <code>filesize</code>.</li> <li>Their <code>contents</code>, which is a <code>ManyToMany</code> field to the <code>HistogramDataFileContents</code> table. A single DQM file may contain mutltiple types of histograms (e.g. <code>LumisectionHistograms1D</code> and <code>LumisectionHistograms2D</code>).</li> </ul> <p>The <code>DATAFILE_FORMAT_CHOICES</code> class attribute is also defined, which is used by the <code>discover_dqm_files</code> management command. This management command is, currently, the only way to create <code>HistogramDataFile</code> entries in the database.</p> <p>Currently, the choices for available files provided are only refreshed every time the <code>discover_dqm_files</code> management command is run. To run it, login to PaaS, select the <code>ml4dqm-playground</code> project, go to <code>Administrator</code>-&gt;<code>Pods</code>, select the currently running pod, go to <code>Terminal</code> and run <code>python manage.py discover_dqm_files</code></p> <p>Warning</p> <p>The <code>HistogramDataFile</code> entries can be deleted without affecting the Histograms loaded from the deleted files. However, re-reading the file will NOT update the existing Histogram entries to point to the newly read file</p> <p>This is due to the <code>HistogramBase</code> model having the <code>on_delete=SET_NULL</code> on the <code>ForeignKey</code> to the <code>HistogramDataFile</code>. </p> <p>This was chosen so that the large number of extracted lumisections would be preserved in the DB after deleting an entry to a <code>HistogramDataFile</code>.</p>"},{"location":"mlplayground/apps/histogram_file_manager/models/#histogramdatafilecontents","title":"<code>HistogramDataFileContents</code>","text":"<ul> <li>The <code>granularity</code> of the data they contain (e.g. Run or Lumisection data)</li> <li>The <code>data_dimensionality</code> of the data contained (1D or 2D).</li> </ul>"},{"location":"mlplayground/apps/histogram_file_manager/overview/","title":"histogram_file_manager","text":"<p>App responsible for managing Histogram Data files. Those usually contain histogram data for:</p> <ul> <li>Runs</li> <li>Lumisections:<ul> <li>1D</li> <li>2D</li> </ul> </li> </ul> <p>They can be of format:</p> <ul> <li><code>.csv</code></li> <li><code>.nanodqm</code> (Not yet supported)</li> </ul>"},{"location":"mlplayground/apps/histograms/models/","title":"Models","text":""},{"location":"mlplayground/apps/histograms/models/#histogrambase","title":"<code>HistogramBase</code>","text":""},{"location":"mlplayground/apps/histograms/models/#runhistogram","title":"<code>RunHistogram</code>","text":""},{"location":"mlplayground/apps/histograms/models/#lumisectionhistogrambase","title":"<code>LumisectionHistogramBase</code>","text":""},{"location":"mlplayground/apps/histograms/models/#lumisectionhistogram1d","title":"<code>LumisectionHistogram1D</code>","text":""},{"location":"mlplayground/apps/histograms/models/#lumiesectionhistogram2d","title":"<code>LumiesectionHistogram2D</code>","text":""},{"location":"mlplayground/apps/histograms/overview/","title":"histograms app","text":"<p>This application is responsible for managing the actual Histograms contained in histogram data files.</p> <p>It can currently handle:</p> <ul> <li>Run histograms (parsing function not implemented yet)</li> <li>Lumisection histograms:<ul> <li>1D histograms</li> <li>2D histograms</li> </ul> </li> </ul>"},{"location":"mlplayground/deploying/deployments/","title":"Deployment","text":"<p>The project is deployed on CERN's PaaS platform as two separate deployments:</p> <ul> <li><code>mlplayground</code> using a modified s2i deployment,</li> <li><code>dqm-playground-ds</code> from a Docker image.</li> </ul>"},{"location":"mlplayground/deploying/deployments/#mlplayground","title":"<code>mlplayground</code>","text":"<p>Due to the <code>root</code> dependency that opening <code>nanoDQM</code> files introduces,  a custom s2i base image has been created using the procedure followed here.</p> <p>See the <code>Dockerfile</code> for the extra packages added to the default <code>RHEL UBI8</code> image.</p>"},{"location":"mlplayground/deploying/deployments/#running-management-commands-on-openshift","title":"Running management commands on OpenShift","text":"<p>Due to the project's dependency on CERN's ROOT, in order to run any management command on OpenShift (i.e. <code>discover_dqm_files</code>) you must run <code>source /opt/app-root/src/root/bin/thisroot.sh</code> to add ROOT to <code>PATH</code>. </p>"},{"location":"mlplayground/deploying/deployments/#dqm-playground-ds","title":"<code>dqm-playground-ds</code>","text":"<p>This project is automatically built into a Docker image using GitHub Actions and published on Docker Hub.</p> <p>On each new image build, an update is rolled out and the deployment is updated.</p> <p>This was configured using these instructions.</p>"},{"location":"mlplayground/deploying/deployments/#paas-resource-limits","title":"PaaS Resource Limits","text":"<p>Histogram parsing from <code>.csv</code> files can be pretty CPU-intensive, which the default PaaS limits cannot handle.</p> <p>Follow the guide to increase the limits to:</p> <pre><code>resources:\n    limits:\n        cpu: '4'\n        memory: 8Gi\n    requests:\n        cpu: '2'\n        memory: 4Gi\n</code></pre>"},{"location":"mlplayground/deploying/guide/","title":"Deployment Guide","text":"<p>The following steps will guide you through the deployment procedure of the app on OpenShift. An overview of the steps is:</p> <ol> <li>Create a new PaaS project</li> <li>Setup the base image &amp; the repository that will be used</li> <li>Setup a Database</li> <li>Setup Environmental Variables</li> <li>Setup a Django superuser</li> <li>Mount EOS Storage</li> <li>Single Sign-On</li> <li>Deploy</li> <li>Expose the app</li> </ol> <p>The procedure can be done completely via the web UI provided by PaaS.</p>"},{"location":"mlplayground/deploying/guide/#prerequisites","title":"Prerequisites","text":""},{"location":"mlplayground/deploying/guide/#s2i-directory-inside-the-root-of-your-repository","title":"<code>.s2i</code> directory inside the root of your repository","text":"<p>We will be using the Software To Image (s2i) approach to deploy on PaaS, namely the Python flavor. This means that a Docker image is created from our repository on each deployment.</p> <p>There should be a <code>.s2i</code> directory inside your repository, with the <code>environment</code> file in it.</p>"},{"location":"mlplayground/deploying/guide/#environment-contents","title":"<code>environment</code> contents","text":"<p>These are environmental variables used by Openshift when creating the Docker image. The value of <code>APP_SCRIPT</code> will be the entrypoint of the created image. It should point to <code>openshift-start-up-script.sh</code>.</p>"},{"location":"mlplayground/deploying/guide/#requesting-a-website","title":"Requesting a website","text":"<p>Create a new PaaS project by clicking here. Then, fill out the fields as shown below:</p> <p></p> <p>When creating a website, different site types can be chosen. In order to use the OpenShift software, the <code>PaaS Web Application</code> option has to be selected.</p>"},{"location":"mlplayground/deploying/guide/#setup-procedure","title":"Setup Procedure","text":"<p>Once the website is successfully requested, the (new, empty) project should be available in OpenShift.</p>"},{"location":"mlplayground/deploying/guide/#create-the-base-image","title":"Create the base image","text":"<p>MLPlayground needs <code>ROOT</code> in order to read data from <code>.root</code> files.  To do so, a <code>ROOT</code> installation is needed in the base image that we're going to build MLPlayground in. Usually, we use <code>python3.8-ubi8</code>, but after lots of failed experimenting with pre-built <code>ROOT</code> packages, it appears that it's \"simpler\" to build it from source.</p> <p>For this reason, we are using a <code>python3.8-ubi8</code> image as base, on top of which we use a <code>Dockerfile</code> which:</p> <ul> <li>Installs all <code>ROOT</code> dependencies,</li> <li>Builds <code>ROOT</code> from source.</li> </ul> <p>Note</p> <p>These instructions are adapted from the tutorial here.</p> <ol> <li> <p>Open PaaS and select your project.    There should be no resources there:</p> <p></p> </li> <li> <p>Click on the \"+\" sign, on the top-right:</p> <p></p> </li> <li> <p>Add the following in the YAML editor that appears:    <pre><code>apiVersion: image.openshift.io/v1\nkind: ImageStream\nmetadata:\n  name: python-with-root\n</code></pre>    And click <code>Create</code>. The new base image we will be creating    will be called <code>python-with-root</code>, as specified in the YAML above.</p> </li> <li> <p>Click the \"+\" sign again, to create a new YAML.</p> </li> <li> <p>Paste the following contents in the new editor:    <pre><code>apiVersion: build.openshift.io/v1\nkind: BuildConfig\nmetadata:\n  name: python-with-root\nspec:\n  resources:\n    limits:\n      cpu: '2'\n      memory: 4Gi\n    requests:\n      cpu: '2'\n      memory: 4Gi\n  output:\n    to:\n      kind: ImageStreamTag\n      name: python-with-root:latest\n  strategy:\n    dockerStrategy:\n      from:\n        kind: ImageStreamTag\n        name: python:3.8-ubi8\n        namespace: openshift\n    type: Docker\n  source:\n    type: Git\n    git:\n      uri: 'https://github.com/CMSTrackerDPG/MLplayground.git'\n  triggers:\n    - type: GitHub\n      github:\n        secret: &lt;??&gt;\n    - type: Generic\n      generic:\n        secret: &lt;??&gt;\n    - type: ConfigChange\n    - type: ImageChange\n      imageChange: {}\n</code></pre>    This YAML specifies that we are using <code>python:3.8-ubi8</code> as the base image,    with the extra steps configured in the <code>source</code> parameter. The result will    be stored under the name <code>python-with-root:latest</code>.    Note that we are using the custom <code>Dockerfile</code> found here, as specified under <code>source</code>--&gt;<code>git</code>.    This <code>Dockerfile</code> builds a Python image with <code>ROOT</code> installed.</p> </li> <li> <p>Go to <code>Builds</code> and click the <code>python-with-root</code> <code>BuildConfig</code>.</p> </li> <li>Under the <code>Environment</code> tab, add a <code>ROOT_TAG_NAME</code> variable, with the value <code>v6-24-08</code>. This is    the <code>ROOT</code> version to be installed:     <code>v6-24-08</code> seems to work with the <code>python3.8</code> image.</li> <li> <p>Under <code>Actions</code>, on the top-right, click <code>Start build</code>:</p> <p></p> </li> <li> <p>After several hours, the build of the base image should be complete.</p> </li> </ol>"},{"location":"mlplayground/deploying/guide/#create-the-s2i-build","title":"Create the s2i build","text":"<p>Now, once we have created the base image and it has finished building, we are ready to deploy our application on top of the base image.</p> <p>Note</p> <p>Adapted from here.</p> <p>The following steps need to be done in order to configure the web application with the GitHub repository:</p> <ol> <li>Go to PaaS.</li> <li> <p>Select the project you created</p> <p></p> </li> <li> <p>Click on \"Add\" on the left</p> <p></p> </li> <li> <p>choose <code>Git Repository</code></p> <p></p> </li> <li> <p>Paste the repository URL in the field provided.</p> </li> <li></li> <li>Under <code>Advanced Git options</code>, you may select a specific branch, if needed. E.g. for the  development mlplayground instance, the <code>develop</code> branch must be selected.</li> <li> <p>Under <code>Advanced Git options</code>, you will be warned that <code>Multiple import strategies detected</code>.     Click on <code>Edit Import Strategy</code> and select <code>Builder Image</code>. Next, select <code>Python</code>, and    the proper <code>Builder Image version</code>:     </p> <p>Info</p> <p>As of writing, we select <code>3.8-ubi8</code>.</p> </li> <li> <p>Under General, change the Application name and Name appropriately. </p> </li> <li> <p>Under Resources, select Deployment</p> <p></p> </li> <li> <p>[Optional] Add GitHub credentials at \"Source Secret\" if the repository is     private</p> </li> <li> <p>Make sure that Create a route to the Application is ticked.</p> </li> <li>Under Show advanced Routing options:     a. Paste the Hostname you want (will be automatically registered),     b. Make sure Secure Route is ticked,     c. Under TLS termination, select <code>Edge</code>,     d. Under Insecure Traffic, select <code>Redirect</code>.</li> <li> <p>Click on Create. The application has been configured!</p> <p>Note</p> <p>Under Topology, you will see your project trying to run for the first time. This will fail, since most environmental variables are missing. Click on the main app:</p> <p></p> <p>You should be getting the following error:</p> <p></p> </li> <li> <p>We now also need to change the base image used to build our app.     Go to <code>Builds</code>, select the <code>BuildConfig</code> you just created (e.g. <code>mlplayground-develop</code>),     and click the <code>YAML</code> tab. Navigate to <code>spec</code>&gt;<code>strategy</code>&gt;<code>sourceStrategy</code>&gt;<code>from</code> and change     the <code>namespace</code> to the name of the project you created (e.g. <code>ml4dqm-playground-dev</code>) and     the <code>name</code> to <code>python-with-root:latest</code>, where <code>python-with-root</code> is the base image you      created in the previous section. </p> <p></p> <p>Warning</p> <p>The base image must have finished building before you can build your application.</p> </li> </ol>"},{"location":"mlplayground/deploying/guide/#setup-a-database","title":"Setup a Database","text":"<p>The database was requested from the CERN DB on demand service. To request one,  follow the instructions here.</p> <p>A PostgreSQL database is used.</p> <p>After the database has been requested it can be used straight away. Django takes care of creating the necessary tables and only requires the credentials.</p>"},{"location":"mlplayground/deploying/guide/#actions-to-take-once-the-database-is-ready","title":"Actions to take once the database is ready","text":"<p>Warning</p> <p>This is only needed if you just requested a new Database instance from the DBoD website.</p>"},{"location":"mlplayground/deploying/guide/#change-default-password","title":"Change default password","text":"<p>Given the username that was sent to you via the DBoD Service, (possibly <code>admin</code>) connect to the database:</p> <pre><code>psql -h dbod-birdup.cern.ch -U admin -p 6601\n</code></pre> <p>And run:</p> <pre><code>ALTER ROLE admin WITH PASSWORD 'new_password';\n</code></pre>"},{"location":"mlplayground/deploying/guide/#ssl-configuration","title":"SSL Configuration","text":"<p>Error</p> <pre><code>django.db.utils.OperationalError: connection to server at \n\"&lt;host&gt;\" (&lt;ip&gt;), port 6601 failed: FATAL:  no pg_hba.conf entry for host \"&lt;ip&gt;\"\n</code></pre> <p>Follow the instructions here to edit the required configuration files using the file editor.</p>"},{"location":"mlplayground/deploying/guide/#create-the-database","title":"Create the database","text":"<p>Assuming that the database name you are going to use is <code>certhelperdb</code> (i.e. <code>DJANGO_DATABASE_NAME</code> is <code>certhelperdb</code>), you will need to  create it manually first.</p> <p>To do so, you will have to first connect to it using <code>psql</code><sup>1</sup>:</p> <pre><code>psql -h &lt;Database hostname&gt; -p &lt;Database port&gt; -U &lt;Database user&gt;\n</code></pre> <p>Then, in the SQL prompt, run the following to create the database:</p> <pre><code>CREATE DATABASE certhelperdb;\n</code></pre> <p>and enter your password once prompted.      </p>"},{"location":"mlplayground/deploying/guide/#setup-environmental-variables","title":"Setup Environmental Variables","text":"<p>This procedure takes place on PaaS, via the <code>Developer</code> view.</p> <ol> <li>Under <code>Secrets</code>, click <code>Create</code>&gt;<code>Key/value secret</code>:<ul> <li><code>Secret name</code>: <code>postgres-user</code></li> <li><code>Key</code>: <code>user</code></li> <li><code>Value</code>: Your PostgreSQL database's username.</li> <li>Click <code>Add key/value</code>.</li> <li><code>Key</code>: <code>password</code></li> <li><code>Value</code>: Your PostgreSQL database's password.</li> <li>Click <code>Create</code>.</li> </ul> </li> <li>Under <code>Secrets</code>, click <code>Create</code>&gt;<code>Key/value secret</code>:<ul> <li><code>Secret name</code>: <code>cern-sso-login-registration-credentials</code></li> <li><code>Key</code>: <code>CERN_SSO_REGISTRATION_CLIENT_ID</code></li> <li><code>Value</code>: Your SSO registration's <code>Client ID</code>.</li> <li>Click <code>Add key/value</code>.</li> <li><code>Key</code>: <code>CERN_SSO_REGISTRATION_CLIENT_SECRET</code></li> <li><code>Value</code>: Your SSO registration's <code>Client Secret</code>.</li> <li>Click <code>Create</code>.</li> </ul> </li> <li> <p>Under <code>Builds --&gt; Your project name --&gt; Environment</code> use the    <code>Add from ConfigMap or Secret</code> button to add the variables:</p> <p><code>DJANGO_DATABASE_USER       &lt;postgres-user/user&gt; DJANGO_DATABASE_PASSWORD   &lt;postgres-user/password&gt;   CERN_SSO_REGISTRATION_CLIENT_ID &lt;cern-sso-login-registration-credentials/CERN_SSO_REGISTRATION_CLIENT_ID&gt;   CERN_SSO_REGISTRATION_CLIENT_SECRET &lt;cern-sso-login-registration-credentials/CERN_SSO_REGISTRATION_CLIENT_SECRET&gt;</code></p> </li> <li> <p>Non-secret variables:</p> <ol> <li><code>DJANGO_SECRET_KEY</code>: Generate a django secret key.</li> <li><code>DJANGO_ALLOWED_HOSTS</code>: The app's URL without \"https://\",  e.g <code>ml4dqm-playground-develop.web.cern.ch</code>.</li> <li><code>DJANGO_DATABASE_ENGINE</code>: <code>django.db.backends.postgresql</code></li> <li><code>DJANGO_DATABASE_NAME</code>: The name of the database that is going to     be used in your PostgreSQL instance. Note: You will have to     create the database yourself, via <code>psql</code> or <code>pgadmin</code>.</li> <li><code>DJANGO_DATABASE_HOST</code>: <code>dbod-mlplaygrounddb.cern.ch</code></li> <li><code>DJANGO_DATABASE_PORT</code>: The port that the DBoD service gave to you.</li> <li><code>DIR_PATH_DQMIO_STORAGE</code>: The colon-separated paths pointing to the directories     that are going to be searched for DQM files.</li> <li><code>DJANGO_DEBUG</code>: <code>False</code>, or <code>True</code> if you need to temporarily do some testing.</li> <li><code>CSRF_TRUSTED_ORIGINS</code>: Your full app's URL with \"https://\", e.g. <code>https://ml4dqm-playground-develop.web.cern.ch</code>.</li> <li><code>SITE_ID</code>: Empty, until Single Sign-On is configured.</li> </ol> </li> <li> <p>Save the variables and rebuild the project:     </p> </li> </ol> <p>Note</p> <p>The procedure above should only be followed once. Once the app is fully configured, you should not have to alter anything, unless a change occurs (e.g. Database host/password).</p> <p>Note</p> <p>More information on the environmental variables can be found here.</p>"},{"location":"mlplayground/deploying/guide/#mount-eos-storage","title":"Mount EOS Storage","text":"<p>The project has 1 TB of storage associated in the EOS. To mount it to OpenShift follow these instructions.</p> <p>Detailed instructions can be found on the PaaS docs.</p>"},{"location":"mlplayground/deploying/guide/#create-a-superuser","title":"Create a superuser","text":""},{"location":"mlplayground/deploying/guide/#open-a-terminal-in-the-running-pod","title":"Open a terminal in the running pod","text":"<ol> <li> <p>Go to PaaS and, in the <code>Developer</code> view, click <code>Topology</code>:</p> <p></p> </li> <li> <p>Click on the application shown there:</p> <p></p> </li> <li> <p>On the right, in the <code>Pods</code> section, click <code>View logs</code>:</p> <p></p> </li> <li> <p>Click the <code>Terminal</code> tab:</p> <p></p> </li> <li> <p>Run <code>source root/bin/thisroot.sh</code>. This is needed to <code>import ROOT</code> in <code>python</code> later.</p> </li> </ol>"},{"location":"mlplayground/deploying/guide/#run-the-django-management-command","title":"Run the Django management command","text":"<p>In the terminal you opened in the previous section, run <code>python manage.py createsuperuser</code> to create the superuser. You will be asked for a <code>username</code>, a <code>password</code> and an <code>e-mail</code> (the latter is optional).</p>"},{"location":"mlplayground/deploying/guide/#single-sign-on","title":"Single Sign-On","text":""},{"location":"mlplayground/deploying/guide/#cern-setup","title":"CERN Setup","text":"<ol> <li>Visit the application portal.</li> <li>Create a new application, giving it a meaningful <code>Application identifier</code> (e.g. <code>ml4dqm-playground</code>).</li> <li>Under the <code>SSO Registration</code> tab, create a new one:<ol> <li>Under <code>Redirect URI(s)</code> add <code>https://ml4dqm-playground.web.cern.ch/accounts/cern/login/callback/</code>.</li> <li>Under <code>Base URL</code> add <code>https://ml4dqm-playground.web.cern.ch/</code></li> </ol> </li> <li>Click <code>Submit</code> and note the <code>Client ID</code> and <code>Client Secret</code> that have been generated.</li> </ol> <p>Note</p> <p>Do not use the <code>Identifier</code> which is automatically generated by  PaaS (found in the application portal and looks like <code>webframeworks-paas-*</code>). It is not meant to be used for the SSO registration of the application, so you will need to generate a new one.</p>"},{"location":"mlplayground/deploying/guide/#integration","title":"Integration","text":"<p>The single sign-on integration is very easy when using the <code>django-allauth</code> python package, which has built-in CERN support.</p> <p>Note</p> <p>Based on  the installation procedure here.</p> <ol> <li>If you have not yet created a Django <code>superuser</code> for your database yet, create one.</li> <li>Log into the <code>admin</code> site of your application using the <code>superuser</code> account.</li> <li>On the left, go to <code>Sites</code> and add a new entry, using the full URL of your app:    </li> <li>Then, go to <code>Social applications</code> and add a new entry, selecting <code>CERN</code> as a <code>Provider</code> and    using the <code>client_id</code> you noted in the previous section:    </li> <li>Verify the <code>SITE_ID</code> value by checking the database itself. E.g. it might ge <code>1</code> or <code>2</code>. To do that,    you will first have to open a terminal on a running pod.    Run <code>python manage.py shell</code>. In the prompt that appears, run the following:    <pre><code>from django.contrib.sites.models import Site\n\nprint(Site.objects.all()[0].id)\n</code></pre>    The number printed on the terminal is your <code>SITE_ID</code>. Use it to update    the environmental variable</li> <li>Make sure you have set the following environment variables in your Build environment on PaaS:<ol> <li><code>CERN_SSO_REGISTRATION_CLIENT_ID</code></li> <li><code>CERN_SSO_REGISTRATION_CLIENT_SECRET</code></li> </ol> </li> </ol>"},{"location":"mlplayground/deploying/guide/#deploying-a-new-build","title":"Deploying a new build","text":"<p>If you want to rebuild the deployment, you can do so manually by triggering a build on PaaS.</p> <p>This can be done by visiting paas.cern.ch, selecting the <code>Developer</code> view, selecting the project and then visiting <code>Builds</code>. Click the three dots on the right of your <code>BuildConfig</code> and press the <code>build</code> button. The new deployment process should be started. In the meantime, the logs of the build process can be viewed by clicking on <code>View Log</code>.</p>"},{"location":"mlplayground/deploying/guide/#exposing-the-app","title":"Exposing the app","text":"<p>See the PaaS docs on how to make the app visible from outside the CERN GPN.</p> <ol> <li> <p>You will either have to do that through LXPLUS, or your computer must be inside CERN. You can always use an SSH tunnel for that.\u00a0\u21a9</p> </li> </ol>"}]}